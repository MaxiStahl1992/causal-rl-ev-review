{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25326f13",
   "metadata": {},
   "source": [
    "# 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98eafa0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Iterable, Dict, Any, List\n",
    "\n",
    "import pandas as pd\n",
    "from neo4j import GraphDatabase, basic_auth\n",
    "from neo4j.exceptions import ServiceUnavailable, TransientError\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd99cb7",
   "metadata": {},
   "source": [
    "# 2. Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "68e200bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded .env file from: /Users/stahlma/Desktop/01_Studium/10_Seminar/causal-rl-ev-review/.env\n",
      "✅ Connection to Neo4j successful!\n"
     ]
    }
   ],
   "source": [
    "project_dir = Path().cwd()\n",
    "dotenv_path = project_dir / '.env'\n",
    "\n",
    "# Checks if the .env file exists at the constructed path before loading.\n",
    "if dotenv_path.exists():\n",
    "    # Loads the environment variables from the found .env file.\n",
    "    load_dotenv(dotenv_path=dotenv_path)\n",
    "    print(f\"Loaded .env file from: {dotenv_path}\")\n",
    "else:\n",
    "    print(f\".env file not found at: {dotenv_path}\")\n",
    "\n",
    "load_dotenv(dotenv_path=dotenv_path)\n",
    "URI  = os.getenv(\"NEO4J_URI\") \n",
    "URI = os.getenv(\"NEO4J_URI\")\n",
    "AUTH = (os.getenv(\"NEO4J_USERNAME\"), os.getenv(\"NEO4J_PASSWORD\"))\n",
    "\n",
    "# Function to execute a query\n",
    "def run_query(driver, query, **params):\n",
    "    \"\"\"Executes a Cypher query and returns the result.\"\"\"\n",
    "    with driver.session() as session:\n",
    "        result = session.run(query, **params)\n",
    "        return [record for record in result]\n",
    "\n",
    "# Establish the connection to the database\n",
    "try:\n",
    "    driver = GraphDatabase.driver(URI, auth=AUTH)\n",
    "    driver.verify_connectivity()\n",
    "    print(\"✅ Connection to Neo4j successful!\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Failed to connect to Neo4j: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3193b033",
   "metadata": {},
   "source": [
    "# 3. Uniqueness Constraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9104de59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating constraint for Paper nodes...\n",
      "Creating constraint for Author nodes...\n",
      "Creating constraint for Query nodes...\n",
      "Creating constraint for FieldOfStudy nodes...\n",
      "Creating constraint for Venue nodes...\n",
      "Constraints created successfully.\n"
     ]
    }
   ],
   "source": [
    "# Create a uniqueness constraint on Paper nodes\n",
    "print(\"Creating constraint for Paper nodes...\")\n",
    "paper_constraint_query = \"CREATE CONSTRAINT paper_id_unique IF NOT EXISTS FOR (p:Paper) REQUIRE p.paperId IS UNIQUE;\"\n",
    "run_query(driver, paper_constraint_query)\n",
    "\n",
    "# Create a uniqueness constraint on Author nodes\n",
    "print(\"Creating constraint for Author nodes...\")\n",
    "author_constraint_query = \"CREATE CONSTRAINT author_id_unique IF NOT EXISTS FOR (a:Author) REQUIRE a.authorId IS UNIQUE;\"\n",
    "run_query(driver, author_constraint_query)\n",
    "\n",
    "# Constraint for Query nodes\n",
    "print(\"Creating constraint for Query nodes...\")\n",
    "query_constraint = \"CREATE CONSTRAINT query_id_unique IF NOT EXISTS FOR (q:Query) REQUIRE q.queryId IS UNIQUE;\"\n",
    "run_query(driver, query_constraint)\n",
    "\n",
    "# Constraint for FieldOfStudy nodes\n",
    "print(\"Creating constraint for FieldOfStudy nodes...\")\n",
    "fos_constraint = \"CREATE CONSTRAINT concept_id_unique IF NOT EXISTS FOR (f:FieldOfStudy) REQUIRE f.conceptId IS UNIQUE;\"\n",
    "run_query(driver, fos_constraint)\n",
    "\n",
    "# Constraint for Venue nodes\n",
    "print(\"Creating constraint for Venue nodes...\")\n",
    "venue_constraint = \"CREATE CONSTRAINT venue_id_unique IF NOT EXISTS FOR (v:Venue) REQUIRE v.venueId IS UNIQUE;\"\n",
    "run_query(driver, venue_constraint)\n",
    "\n",
    "print(\"Constraints created successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5053e6",
   "metadata": {},
   "source": [
    "# 4. Load Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0135a657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 2055 Paper nodes...\n",
      "✅ Paper nodes loaded.\n",
      "\n",
      "Loading 7004 Author nodes...\n",
      "✅ Author nodes loaded.\n",
      "\n",
      "Loading 8 Query nodes...\n",
      "✅ Query nodes loaded.\n",
      "\n",
      "Loading 14 FieldOfStudy nodes...\n",
      "✅ FieldOfStudy nodes loaded.\n",
      "\n",
      "Loading 932 Venue nodes...\n",
      "✅ Venue nodes loaded.\n"
     ]
    }
   ],
   "source": [
    "# Load the papers CSV\n",
    "papers_df = pd.read_csv('./data/processed/Papers.csv')\n",
    "# Convert to a list of dictionaries for the query\n",
    "papers_records = papers_df.to_dict('records')\n",
    "\n",
    "# Cypher query to create Paper nodes from a list of records\n",
    "create_papers_query = \"\"\"\n",
    "UNWIND $records AS row\n",
    "MERGE (p:Paper {paperId: row.paper_id})\n",
    "SET\n",
    "    p.title = row.title,\n",
    "    p.abstract = row.abstract,\n",
    "    p.year = toInteger(row.year),\n",
    "    p.publicationDate = row.publication_date,\n",
    "    p.doi = row.doi,\n",
    "    p.s2Url = row.s2_url\n",
    "\"\"\"\n",
    "\n",
    "# Send the data in batches of 5000\n",
    "print(f\"Loading {len(papers_records)} Paper nodes...\")\n",
    "batch_size = 5000\n",
    "for i in range(0, len(papers_records), batch_size):\n",
    "    batch = papers_records[i:i + batch_size]\n",
    "    run_query(driver, create_papers_query, records=batch)\n",
    "print(\"✅ Paper nodes loaded.\")\n",
    "\n",
    "# --- Load Author Nodes ---\n",
    "authors_df = pd.read_csv('./data/processed/Authors.csv')\n",
    "authors_records = authors_df.to_dict('records')\n",
    "\n",
    "create_authors_query = \"\"\"\n",
    "UNWIND $records AS row\n",
    "MERGE (a:Author {authorId: row.author_id})\n",
    "SET a.name = row.name\n",
    "\"\"\"\n",
    "\n",
    "print(f\"\\nLoading {len(authors_records)} Author nodes...\")\n",
    "for i in range(0, len(authors_records), batch_size):\n",
    "    batch = authors_records[i:i + batch_size]\n",
    "    run_query(driver, create_authors_query, records=batch)\n",
    "print(\"✅ Author nodes loaded.\")\n",
    "\n",
    "# --- Load Query Nodes ---\n",
    "queries_df = pd.read_csv('./data/processed/Queries.csv')\n",
    "queries_records = queries_df.to_dict('records')\n",
    "\n",
    "create_queries_query = \"\"\"\n",
    "UNWIND $records AS row\n",
    "MERGE (q:Query {queryId: row.query_id})\n",
    "SET q.name = row.name\n",
    "\"\"\"\n",
    "\n",
    "print(f\"\\nLoading {len(queries_records)} Query nodes...\")\n",
    "run_query(driver, create_queries_query, records=queries_records)\n",
    "print(\"✅ Query nodes loaded.\")\n",
    "\n",
    "\n",
    "# --- Load FieldOfStudy Nodes ---\n",
    "fos_df = pd.read_csv('./data/processed/FieldsOfStudy.csv')\n",
    "fos_records = fos_df.to_dict('records')\n",
    "\n",
    "create_fos_query = \"\"\"\n",
    "UNWIND $records AS row\n",
    "MERGE (f:FieldOfStudy {conceptId: row.concept_id})\n",
    "SET f.name = row.name\n",
    "\"\"\"\n",
    "\n",
    "print(f\"\\nLoading {len(fos_records)} FieldOfStudy nodes...\")\n",
    "run_query(driver, create_fos_query, records=fos_records)\n",
    "print(\"✅ FieldOfStudy nodes loaded.\")\n",
    "\n",
    "\n",
    "# --- Load Venue Nodes ---\n",
    "venues_df = pd.read_csv('./data/processed/Venues.csv')\n",
    "venues_records = venues_df.to_dict('records')\n",
    "\n",
    "create_venues_query = \"\"\"\n",
    "UNWIND $records AS row\n",
    "MERGE (v:Venue {venueId: row.venue_id})\n",
    "SET v.name = row.name, v.type = row.type, v.url = row.url\n",
    "\"\"\"\n",
    "\n",
    "print(f\"\\nLoading {len(venues_records)} Venue nodes...\")\n",
    "run_query(driver, create_venues_query, records=venues_records)\n",
    "print(\"✅ Venue nodes loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71a35b6",
   "metadata": {},
   "source": [
    "# 5. Load Relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e1302e86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading 7845 AUTHORED relationships...\n",
      "✅ AUTHORED relationships loaded.\n",
      "\n",
      "Loading 805 CITES relationships...\n",
      "✅ CITES relationships loaded.\n",
      "\n",
      "Loading 1947 PUBLISHED_IN relationships...\n",
      "✅ PUBLISHED_IN relationships loaded.\n",
      "\n",
      "Loading 2055 FOUND_BY relationships...\n",
      "✅ FOUND_BY relationships loaded.\n",
      "\n",
      "Loading 1585 HAS_FIELD relationships...\n",
      "✅ HAS_FIELD relationships loaded.\n",
      "\n",
      "Loading 1600 SIMILAR_TO relationships...\n",
      "✅ SIMILAR_TO relationships loaded.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 5000\n",
    "\n",
    "# --- Load Authorship Relationships ---\n",
    "authorship_df = pd.read_csv('./data/processed/Authorship.csv')\n",
    "authorship_records = authorship_df.to_dict('records')\n",
    "\n",
    "create_authorship_query = \"\"\"\n",
    "UNWIND $records AS row\n",
    "MATCH (a:Author {authorId: row.author_id})\n",
    "MATCH (p:Paper {paperId: row.paper_id})\n",
    "MERGE (a)-[r:AUTHORED {position: toInteger(row.position)}]->(p)\n",
    "\"\"\"\n",
    "\n",
    "print(f\"\\nLoading {len(authorship_records)} AUTHORED relationships...\")\n",
    "for i in range(0, len(authorship_records), batch_size):\n",
    "    batch = authorship_records[i:i + batch_size]\n",
    "    run_query(driver, create_authorship_query, records=batch)\n",
    "print(\"✅ AUTHORED relationships loaded.\")\n",
    "\n",
    "\n",
    "# --- Load Cites Relationships ---\n",
    "cites_df = pd.read_csv('./data/processed/Cites.csv')\n",
    "# Rename columns to avoid ambiguity in the query\n",
    "cites_df.rename(columns={'src_paper_id': 'sourceId', 'dst_paper_id': 'destinationId'}, inplace=True)\n",
    "cites_records = cites_df.to_dict('records')\n",
    "\n",
    "create_cites_query = \"\"\"\n",
    "UNWIND $records AS row\n",
    "MATCH (source:Paper {paperId: row.sourceId})\n",
    "MATCH (destination:Paper {paperId: row.destinationId})\n",
    "MERGE (source)-[:CITES]->(destination)\n",
    "\"\"\"\n",
    "\n",
    "print(f\"\\nLoading {len(cites_records)} CITES relationships...\")\n",
    "for i in range(0, len(cites_records), batch_size):\n",
    "    batch = cites_records[i:i + batch_size]\n",
    "    run_query(driver, create_cites_query, records=batch)\n",
    "print(\"✅ CITES relationships loaded.\")\n",
    "\n",
    "# --- Load PUBLISHED_IN Relationships ---\n",
    "published_in_df = pd.read_csv('./data/processed/PublishedIn.csv')\n",
    "published_in_records = published_in_df.to_dict('records')\n",
    "\n",
    "create_published_in_query = \"\"\"\n",
    "UNWIND $records AS row\n",
    "MATCH (p:Paper {paperId: row.paper_id})\n",
    "MATCH (v:Venue {venueId: row.venue_id})\n",
    "MERGE (p)-[:PUBLISHED_IN]->(v)\n",
    "\"\"\"\n",
    "print(f\"\\nLoading {len(published_in_records)} PUBLISHED_IN relationships...\")\n",
    "for i in range(0, len(published_in_records), batch_size):\n",
    "    batch = published_in_records[i:i + batch_size]\n",
    "    run_query(driver, create_published_in_query, records=batch)\n",
    "print(\"✅ PUBLISHED_IN relationships loaded.\")\n",
    "\n",
    "\n",
    "# --- Load FOUND_BY Relationships (FromQuery.csv) ---\n",
    "from_query_df = pd.read_csv('./data/processed/FromQuery.csv')\n",
    "from_query_records = from_query_df.to_dict('records')\n",
    "\n",
    "create_from_query_query = \"\"\"\n",
    "UNWIND $records AS row\n",
    "MATCH (p:Paper {paperId: row.paper_id})\n",
    "MATCH (q:Query {queryId: row.query_id})\n",
    "MERGE (p)-[:FOUND_BY]->(q)\n",
    "\"\"\"\n",
    "print(f\"\\nLoading {len(from_query_records)} FOUND_BY relationships...\")\n",
    "for i in range(0, len(from_query_records), batch_size):\n",
    "    batch = from_query_records[i:i + batch_size]\n",
    "    run_query(driver, create_from_query_query, records=batch)\n",
    "print(\"✅ FOUND_BY relationships loaded.\")\n",
    "\n",
    "\n",
    "# --- Load HAS_FIELD Relationships ---\n",
    "has_field_df = pd.read_csv('./data/processed/HasField.csv')\n",
    "has_field_records = has_field_df.to_dict('records')\n",
    "\n",
    "create_has_field_query = \"\"\"\n",
    "UNWIND $records AS row\n",
    "MATCH (p:Paper {paperId: row.paper_id})\n",
    "// The conceptId was created with a prefix and formatted name, so we recreate it here to match\n",
    "MATCH (f:FieldOfStudy {conceptId: \"F:\" + toLower(replace(row.field_of_study, \" \", \"-\"))})\n",
    "MERGE (p)-[:HAS_FIELD]->(f)\n",
    "\"\"\"\n",
    "print(f\"\\nLoading {len(has_field_records)} HAS_FIELD relationships...\")\n",
    "for i in range(0, len(has_field_records), batch_size):\n",
    "    batch = has_field_records[i:i + batch_size]\n",
    "    run_query(driver, create_has_field_query, records=batch)\n",
    "print(\"✅ HAS_FIELD relationships loaded.\")\n",
    "\n",
    "\n",
    "# --- Load SIMILAR_TO Relationships ---\n",
    "similarity_df = pd.read_csv('./data/processed/Similarity.csv')\n",
    "similarity_records = similarity_df.to_dict('records')\n",
    "\n",
    "create_similarity_query = \"\"\"\n",
    "UNWIND $records AS row\n",
    "MATCH (p1:Paper {paperId: row.src_paper_id})\n",
    "MATCH (p2:Paper {paperId: row.dst_paper_id})\n",
    "// We use an undirected relationship here because similarity is mutual\n",
    "MERGE (p1)-[r:SIMILAR_TO]-(p2)\n",
    "SET r.score = toFloat(row.score), r.method = row.method\n",
    "\"\"\"\n",
    "print(f\"\\nLoading {len(similarity_records)} SIMILAR_TO relationships...\")\n",
    "for i in range(0, len(similarity_records), batch_size):\n",
    "    batch = similarity_records[i:i + batch_size]\n",
    "    run_query(driver, create_similarity_query, records=batch)\n",
    "print(\"✅ SIMILAR_TO relationships loaded.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "causal-rl-ev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
