{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25326f13",
   "metadata": {},
   "source": [
    "# 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98eafa0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Iterable, Dict, Any, List\n",
    "\n",
    "import pandas as pd\n",
    "from neo4j import GraphDatabase, basic_auth\n",
    "from neo4j.exceptions import ServiceUnavailable, TransientError\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd99cb7",
   "metadata": {},
   "source": [
    "# 2. Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "68e200bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded .env file from: /Users/stahlma/Desktop/01_Studium/10_Seminar/causal-rl-ev-review/.env\n",
      "✅ Connection to Neo4j successful!\n"
     ]
    }
   ],
   "source": [
    "project_dir = Path().cwd()\n",
    "dotenv_path = project_dir / '.env'\n",
    "\n",
    "# Checks if the .env file exists at the constructed path before loading.\n",
    "if dotenv_path.exists():\n",
    "    # Loads the environment variables from the found .env file.\n",
    "    load_dotenv(dotenv_path=dotenv_path)\n",
    "    print(f\"Loaded .env file from: {dotenv_path}\")\n",
    "else:\n",
    "    print(f\".env file not found at: {dotenv_path}\")\n",
    "\n",
    "load_dotenv(dotenv_path=dotenv_path)\n",
    "URI  = os.getenv(\"NEO4J_URI\") \n",
    "URI = os.getenv(\"NEO4J_URI\")\n",
    "AUTH = (os.getenv(\"NEO4J_USERNAME\"), os.getenv(\"NEO4J_PASSWORD\"))\n",
    "\n",
    "# Function to execute a query\n",
    "def run_query(driver, query, **params):\n",
    "    \"\"\"Executes a Cypher query and returns the result.\"\"\"\n",
    "    with driver.session() as session:\n",
    "        result = session.run(query, **params)\n",
    "        return [record for record in result]\n",
    "\n",
    "# Establish the connection to the database\n",
    "try:\n",
    "    driver = GraphDatabase.driver(URI, auth=AUTH)\n",
    "    driver.verify_connectivity()\n",
    "    print(\"✅ Connection to Neo4j successful!\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Failed to connect to Neo4j: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3193b033",
   "metadata": {},
   "source": [
    "# 3. Uniqueness Constraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9104de59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating constraint for Paper nodes...\n",
      "Creating constraint for Author nodes...\n",
      "Creating constraint for Query nodes...\n",
      "Creating constraint for FieldOfStudy nodes...\n",
      "Creating constraint for Venue nodes...\n",
      "Constraints created successfully.\n"
     ]
    }
   ],
   "source": [
    "# Create a uniqueness constraint on Paper nodes\n",
    "print(\"Creating constraint for Paper nodes...\")\n",
    "paper_constraint_query = \"CREATE CONSTRAINT paper_id_unique IF NOT EXISTS FOR (p:Paper) REQUIRE p.paperId IS UNIQUE;\"\n",
    "run_query(driver, paper_constraint_query)\n",
    "\n",
    "# Create a uniqueness constraint on Author nodes\n",
    "print(\"Creating constraint for Author nodes...\")\n",
    "author_constraint_query = \"CREATE CONSTRAINT author_id_unique IF NOT EXISTS FOR (a:Author) REQUIRE a.authorId IS UNIQUE;\"\n",
    "run_query(driver, author_constraint_query)\n",
    "\n",
    "# Constraint for Query nodes\n",
    "print(\"Creating constraint for Query nodes...\")\n",
    "query_constraint = \"CREATE CONSTRAINT query_id_unique IF NOT EXISTS FOR (q:Query) REQUIRE q.queryId IS UNIQUE;\"\n",
    "run_query(driver, query_constraint)\n",
    "\n",
    "# Constraint for FieldOfStudy nodes\n",
    "print(\"Creating constraint for FieldOfStudy nodes...\")\n",
    "fos_constraint = \"CREATE CONSTRAINT concept_id_unique IF NOT EXISTS FOR (f:FieldOfStudy) REQUIRE f.conceptId IS UNIQUE;\"\n",
    "run_query(driver, fos_constraint)\n",
    "\n",
    "# Constraint for Venue nodes\n",
    "print(\"Creating constraint for Venue nodes...\")\n",
    "venue_constraint = \"CREATE CONSTRAINT venue_id_unique IF NOT EXISTS FOR (v:Venue) REQUIRE v.venueId IS UNIQUE;\"\n",
    "run_query(driver, venue_constraint)\n",
    "\n",
    "print(\"Constraints created successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5053e6",
   "metadata": {},
   "source": [
    "# 4. Load Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0135a657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 2055 Paper nodes...\n"
     ]
    },
    {
     "ename": "CypherSyntaxError",
     "evalue": "{code: Neo.ClientError.Statement.SyntaxError} {message: Invalid input 'p': expected an expression, ',', 'ORDER BY', 'CALL', 'CREATE', 'LOAD CSV', 'DELETE', 'DETACH', 'FINISH', 'FOREACH', 'INSERT', 'LIMIT', 'MATCH', 'MERGE', 'NODETACH', 'OFFSET', 'OPTIONAL', 'REMOVE', 'RETURN', 'SET', 'SKIP', 'UNION', 'UNWIND', 'USE', 'WITH' or <EOF> (line 11, column 5 (offset: 254))\n\"    p.citationCount = toInteger(row.citation_count)\"\n     ^}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mGqlError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[31mGqlError\u001b[39m: {gql_status: 42I06} {gql_status_description: error: syntax error or access rule violation - invalid input. Invalid input 'p', expected: an expression, ',', 'ORDER BY', 'CALL', 'CREATE', 'LOAD CSV', 'DELETE', 'DETACH', 'FINISH', 'FOREACH', 'INSERT', 'LIMIT', 'MATCH', 'MERGE', 'NODETACH', 'OFFSET', 'OPTIONAL', 'REMOVE', 'RETURN', 'SET', 'SKIP', 'UNION', 'UNWIND', 'USE', 'WITH' or <EOF>.} {message: 42I06: Invalid input 'p', expected: an expression, ',', 'ORDER BY', 'CALL', 'CREATE', 'LOAD CSV', 'DELETE', 'DETACH', 'FINISH', 'FOREACH', 'INSERT', 'LIMIT', 'MATCH', 'MERGE', 'NODETACH', 'OFFSET', 'OPTIONAL', 'REMOVE', 'RETURN', 'SET', 'SKIP', 'UNION', 'UNWIND', 'USE', 'WITH' or <EOF>.} {diagnostic_record: {'_classification': 'CLIENT_ERROR', '_position': {'offset': 254, 'line': 11, 'column': 5}, 'OPERATION': '', 'OPERATION_CODE': '0', 'CURRENT_SCHEMA': '/'}} {raw_classification: CLIENT_ERROR}",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mCypherSyntaxError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 25\u001b[39m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(papers_records), batch_size):\n\u001b[32m     24\u001b[39m     batch = papers_records[i:i + batch_size]\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m     \u001b[43mrun_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdriver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_papers_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecords\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m✅ Paper nodes loaded.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# --- Load Author Nodes ---\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 21\u001b[39m, in \u001b[36mrun_query\u001b[39m\u001b[34m(driver, query, **params)\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Executes a Cypher query and returns the result.\"\"\"\u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m driver.session() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m     result = \u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [record \u001b[38;5;28;01mfor\u001b[39;00m record \u001b[38;5;129;01min\u001b[39;00m result]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/causal-rl-ev/lib/python3.11/site-packages/neo4j/_sync/work/session.py:330\u001b[39m, in \u001b[36mSession.run\u001b[39m\u001b[34m(self, query, parameters, **kwargs)\u001b[39m\n\u001b[32m    328\u001b[39m bookmarks = \u001b[38;5;28mself\u001b[39m._get_bookmarks()\n\u001b[32m    329\u001b[39m parameters = \u001b[38;5;28mdict\u001b[39m(parameters \u001b[38;5;129;01mor\u001b[39;00m {}, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m330\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_auto_result\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    331\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    332\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    333\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdatabase\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    334\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimpersonated_user\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    335\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdefault_access_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    336\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbookmarks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    337\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnotifications_min_severity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    338\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnotifications_disabled_classifications\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    339\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    341\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._auto_result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/causal-rl-ev/lib/python3.11/site-packages/neo4j/_sync/work/result.py:236\u001b[39m, in \u001b[36mResult._run\u001b[39m\u001b[34m(self, query, parameters, db, imp_user, access_mode, bookmarks, notifications_min_severity, notifications_disabled_classifications)\u001b[39m\n\u001b[32m    234\u001b[39m \u001b[38;5;28mself\u001b[39m._pull()\n\u001b[32m    235\u001b[39m \u001b[38;5;28mself\u001b[39m._connection.send_all()\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_attach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/causal-rl-ev/lib/python3.11/site-packages/neo4j/_sync/work/result.py:430\u001b[39m, in \u001b[36mResult._attach\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    428\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exhausted \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[32m    429\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m._attached \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m430\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_connection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/causal-rl-ev/lib/python3.11/site-packages/neo4j/_sync/io/_common.py:193\u001b[39m, in \u001b[36mConnectionErrorHandler.__getattr__.<locals>.outer.<locals>.inner\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    191\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minner\u001b[39m(*args, **kwargs):\n\u001b[32m    192\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m193\u001b[39m         \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    194\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m (Neo4jError, ServiceUnavailable, SessionExpired) \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    195\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m asyncio.iscoroutinefunction(\u001b[38;5;28mself\u001b[39m.__on_error)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/causal-rl-ev/lib/python3.11/site-packages/neo4j/_sync/io/_bolt.py:863\u001b[39m, in \u001b[36mBolt.fetch_message\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    859\u001b[39m \u001b[38;5;66;03m# Receive exactly one message\u001b[39;00m\n\u001b[32m    860\u001b[39m tag, fields = \u001b[38;5;28mself\u001b[39m.inbox.pop(\n\u001b[32m    861\u001b[39m     hydration_hooks=\u001b[38;5;28mself\u001b[39m.responses[\u001b[32m0\u001b[39m].hydration_hooks\n\u001b[32m    862\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m863\u001b[39m res = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_process_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtag\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfields\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    864\u001b[39m \u001b[38;5;28mself\u001b[39m.idle_since = monotonic()\n\u001b[32m    865\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/causal-rl-ev/lib/python3.11/site-packages/neo4j/_sync/io/_bolt5.py:1208\u001b[39m, in \u001b[36mBolt5x7._process_message\u001b[39m\u001b[34m(self, tag, fields)\u001b[39m\n\u001b[32m   1206\u001b[39m \u001b[38;5;28mself\u001b[39m._enrich_error_diagnostic_record(summary_metadata)\n\u001b[32m   1207\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1208\u001b[39m     \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mon_failure\u001b[49m\u001b[43m(\u001b[49m\u001b[43msummary_metadata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1209\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (ServiceUnavailable, DatabaseUnavailable):\n\u001b[32m   1210\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.pool:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/causal-rl-ev/lib/python3.11/site-packages/neo4j/_sync/io/_common.py:263\u001b[39m, in \u001b[36mResponse.on_failure\u001b[39m\u001b[34m(self, metadata)\u001b[39m\n\u001b[32m    261\u001b[39m handler = \u001b[38;5;28mself\u001b[39m.handlers.get(\u001b[33m\"\u001b[39m\u001b[33mon_summary\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    262\u001b[39m Util.callback(handler)\n\u001b[32m--> \u001b[39m\u001b[32m263\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._hydrate_error(metadata)\n",
      "\u001b[31mCypherSyntaxError\u001b[39m: {code: Neo.ClientError.Statement.SyntaxError} {message: Invalid input 'p': expected an expression, ',', 'ORDER BY', 'CALL', 'CREATE', 'LOAD CSV', 'DELETE', 'DETACH', 'FINISH', 'FOREACH', 'INSERT', 'LIMIT', 'MATCH', 'MERGE', 'NODETACH', 'OFFSET', 'OPTIONAL', 'REMOVE', 'RETURN', 'SET', 'SKIP', 'UNION', 'UNWIND', 'USE', 'WITH' or <EOF> (line 11, column 5 (offset: 254))\n\"    p.citationCount = toInteger(row.citation_count)\"\n     ^}"
     ]
    }
   ],
   "source": [
    "# Load the papers CSV\n",
    "papers_df = pd.read_csv('./data/processed/Papers.csv')\n",
    "# Convert to a list of dictionaries for the query\n",
    "papers_records = papers_df.to_dict('records')\n",
    "\n",
    "# Cypher query to create Paper nodes from a list of records\n",
    "create_papers_query = \"\"\"\n",
    "UNWIND $records AS row\n",
    "MERGE (p:Paper {paperId: row.paper_id})\n",
    "SET\n",
    "    p.title = row.title,\n",
    "    p.abstract = row.abstract,\n",
    "    p.year = toInteger(row.year),\n",
    "    p.publicationDate = row.publication_date,\n",
    "    p.doi = row.doi,\n",
    "    p.s2Url = row.s2_url\n",
    "\"\"\"\n",
    "\n",
    "# Send the data in batches of 5000\n",
    "print(f\"Loading {len(papers_records)} Paper nodes...\")\n",
    "batch_size = 5000\n",
    "for i in range(0, len(papers_records), batch_size):\n",
    "    batch = papers_records[i:i + batch_size]\n",
    "    run_query(driver, create_papers_query, records=batch)\n",
    "print(\"✅ Paper nodes loaded.\")\n",
    "\n",
    "# --- Load Author Nodes ---\n",
    "authors_df = pd.read_csv('./data/processed/Authors.csv')\n",
    "authors_records = authors_df.to_dict('records')\n",
    "\n",
    "create_authors_query = \"\"\"\n",
    "UNWIND $records AS row\n",
    "MERGE (a:Author {authorId: row.author_id})\n",
    "SET a.name = row.name\n",
    "\"\"\"\n",
    "\n",
    "print(f\"\\nLoading {len(authors_records)} Author nodes...\")\n",
    "for i in range(0, len(authors_records), batch_size):\n",
    "    batch = authors_records[i:i + batch_size]\n",
    "    run_query(driver, create_authors_query, records=batch)\n",
    "print(\"✅ Author nodes loaded.\")\n",
    "\n",
    "# --- Load Query Nodes ---\n",
    "queries_df = pd.read_csv('./data/processed/Queries.csv')\n",
    "queries_records = queries_df.to_dict('records')\n",
    "\n",
    "create_queries_query = \"\"\"\n",
    "UNWIND $records AS row\n",
    "MERGE (q:Query {queryId: row.query_id})\n",
    "SET q.name = row.name\n",
    "\"\"\"\n",
    "\n",
    "print(f\"\\nLoading {len(queries_records)} Query nodes...\")\n",
    "run_query(driver, create_queries_query, records=queries_records)\n",
    "print(\"✅ Query nodes loaded.\")\n",
    "\n",
    "\n",
    "# --- Load FieldOfStudy Nodes ---\n",
    "fos_df = pd.read_csv('./data/processed/FieldsOfStudy.csv')\n",
    "fos_records = fos_df.to_dict('records')\n",
    "\n",
    "create_fos_query = \"\"\"\n",
    "UNWIND $records AS row\n",
    "MERGE (f:FieldOfStudy {conceptId: row.concept_id})\n",
    "SET f.name = row.name\n",
    "\"\"\"\n",
    "\n",
    "print(f\"\\nLoading {len(fos_records)} FieldOfStudy nodes...\")\n",
    "run_query(driver, create_fos_query, records=fos_records)\n",
    "print(\"✅ FieldOfStudy nodes loaded.\")\n",
    "\n",
    "\n",
    "# --- Load Venue Nodes ---\n",
    "venues_df = pd.read_csv('./data/processed/Venues.csv')\n",
    "venues_records = venues_df.to_dict('records')\n",
    "\n",
    "create_venues_query = \"\"\"\n",
    "UNWIND $records AS row\n",
    "MERGE (v:Venue {venueId: row.venue_id})\n",
    "SET v.name = row.name, v.type = row.type, v.url = row.url\n",
    "\"\"\"\n",
    "\n",
    "print(f\"\\nLoading {len(venues_records)} Venue nodes...\")\n",
    "run_query(driver, create_venues_query, records=venues_records)\n",
    "print(\"✅ Venue nodes loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a5465538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Enriching graph with citation count data...\n",
      "✅ Paper nodes updated with citation counts.\n"
     ]
    }
   ],
   "source": [
    "# --- Data Enrichment: Add Citation Counts to Paper Nodes ---\n",
    "# This code should be run in the 04_... import notebook\n",
    "\n",
    "print(\"\\nEnriching graph with citation count data...\")\n",
    "# Assuming your file is named 'normalized_papers.csv' and is in the same folder as the others\n",
    "citations_update_df = pd.read_csv('./data/processed/normalized_papers.csv')\n",
    "# Keep only the necessary columns and drop rows with missing counts\n",
    "citations_update_df = citations_update_df[['id', 'citation_count']].dropna()\n",
    "citations_update_records = citations_update_df.to_dict('records')\n",
    "\n",
    "update_citations_query = \"\"\"\n",
    "UNWIND $records AS row\n",
    "MATCH (p:Paper {paperId: row.id})\n",
    "SET p.citation_count = toInteger(row.citation_count)\n",
    "\"\"\"\n",
    "batch_size = 5000\n",
    "for i in range(0, len(citations_update_records), batch_size):\n",
    "    batch = citations_update_records[i:i + batch_size]\n",
    "    run_query(driver, update_citations_query, records=batch)\n",
    "print(\"✅ Paper nodes updated with citation counts.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71a35b6",
   "metadata": {},
   "source": [
    "# 5. Load Relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e1302e86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading 7845 AUTHORED relationships...\n",
      "✅ AUTHORED relationships loaded.\n",
      "\n",
      "Loading 805 CITES relationships...\n",
      "✅ CITES relationships loaded.\n",
      "\n",
      "Loading 1947 PUBLISHED_IN relationships...\n",
      "✅ PUBLISHED_IN relationships loaded.\n",
      "\n",
      "Loading 2055 FOUND_BY relationships...\n",
      "✅ FOUND_BY relationships loaded.\n",
      "\n",
      "Loading 1585 HAS_FIELD relationships...\n",
      "✅ HAS_FIELD relationships loaded.\n",
      "\n",
      "Loading 1600 SIMILAR_TO relationships...\n",
      "✅ SIMILAR_TO relationships loaded.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 5000\n",
    "\n",
    "# --- Load Authorship Relationships ---\n",
    "authorship_df = pd.read_csv('./data/processed/Authorship.csv')\n",
    "authorship_records = authorship_df.to_dict('records')\n",
    "\n",
    "create_authorship_query = \"\"\"\n",
    "UNWIND $records AS row\n",
    "MATCH (a:Author {authorId: row.author_id})\n",
    "MATCH (p:Paper {paperId: row.paper_id})\n",
    "MERGE (a)-[r:AUTHORED {position: toInteger(row.position)}]->(p)\n",
    "\"\"\"\n",
    "\n",
    "print(f\"\\nLoading {len(authorship_records)} AUTHORED relationships...\")\n",
    "for i in range(0, len(authorship_records), batch_size):\n",
    "    batch = authorship_records[i:i + batch_size]\n",
    "    run_query(driver, create_authorship_query, records=batch)\n",
    "print(\"✅ AUTHORED relationships loaded.\")\n",
    "\n",
    "\n",
    "# --- Load Cites Relationships ---\n",
    "cites_df = pd.read_csv('./data/processed/Cites.csv')\n",
    "# Rename columns to avoid ambiguity in the query\n",
    "cites_df.rename(columns={'src_paper_id': 'sourceId', 'dst_paper_id': 'destinationId'}, inplace=True)\n",
    "cites_records = cites_df.to_dict('records')\n",
    "\n",
    "create_cites_query = \"\"\"\n",
    "UNWIND $records AS row\n",
    "MATCH (source:Paper {paperId: row.sourceId})\n",
    "MATCH (destination:Paper {paperId: row.destinationId})\n",
    "MERGE (source)-[:CITES]->(destination)\n",
    "\"\"\"\n",
    "\n",
    "print(f\"\\nLoading {len(cites_records)} CITES relationships...\")\n",
    "for i in range(0, len(cites_records), batch_size):\n",
    "    batch = cites_records[i:i + batch_size]\n",
    "    run_query(driver, create_cites_query, records=batch)\n",
    "print(\"✅ CITES relationships loaded.\")\n",
    "\n",
    "# --- Load PUBLISHED_IN Relationships ---\n",
    "published_in_df = pd.read_csv('./data/processed/PublishedIn.csv')\n",
    "published_in_records = published_in_df.to_dict('records')\n",
    "\n",
    "create_published_in_query = \"\"\"\n",
    "UNWIND $records AS row\n",
    "MATCH (p:Paper {paperId: row.paper_id})\n",
    "MATCH (v:Venue {venueId: row.venue_id})\n",
    "MERGE (p)-[:PUBLISHED_IN]->(v)\n",
    "\"\"\"\n",
    "print(f\"\\nLoading {len(published_in_records)} PUBLISHED_IN relationships...\")\n",
    "for i in range(0, len(published_in_records), batch_size):\n",
    "    batch = published_in_records[i:i + batch_size]\n",
    "    run_query(driver, create_published_in_query, records=batch)\n",
    "print(\"✅ PUBLISHED_IN relationships loaded.\")\n",
    "\n",
    "\n",
    "# --- Load FOUND_BY Relationships (FromQuery.csv) ---\n",
    "from_query_df = pd.read_csv('./data/processed/FromQuery.csv')\n",
    "from_query_records = from_query_df.to_dict('records')\n",
    "\n",
    "create_from_query_query = \"\"\"\n",
    "UNWIND $records AS row\n",
    "MATCH (p:Paper {paperId: row.paper_id})\n",
    "MATCH (q:Query {queryId: row.query_id})\n",
    "MERGE (p)-[:FOUND_BY]->(q)\n",
    "\"\"\"\n",
    "print(f\"\\nLoading {len(from_query_records)} FOUND_BY relationships...\")\n",
    "for i in range(0, len(from_query_records), batch_size):\n",
    "    batch = from_query_records[i:i + batch_size]\n",
    "    run_query(driver, create_from_query_query, records=batch)\n",
    "print(\"✅ FOUND_BY relationships loaded.\")\n",
    "\n",
    "\n",
    "# --- Load HAS_FIELD Relationships ---\n",
    "has_field_df = pd.read_csv('./data/processed/HasField.csv')\n",
    "has_field_records = has_field_df.to_dict('records')\n",
    "\n",
    "create_has_field_query = \"\"\"\n",
    "UNWIND $records AS row\n",
    "MATCH (p:Paper {paperId: row.paper_id})\n",
    "// The conceptId was created with a prefix and formatted name, so we recreate it here to match\n",
    "MATCH (f:FieldOfStudy {conceptId: \"F:\" + toLower(replace(row.field_of_study, \" \", \"-\"))})\n",
    "MERGE (p)-[:HAS_FIELD]->(f)\n",
    "\"\"\"\n",
    "print(f\"\\nLoading {len(has_field_records)} HAS_FIELD relationships...\")\n",
    "for i in range(0, len(has_field_records), batch_size):\n",
    "    batch = has_field_records[i:i + batch_size]\n",
    "    run_query(driver, create_has_field_query, records=batch)\n",
    "print(\"✅ HAS_FIELD relationships loaded.\")\n",
    "\n",
    "\n",
    "# --- Load SIMILAR_TO Relationships ---\n",
    "similarity_df = pd.read_csv('./data/processed/Similarity.csv')\n",
    "similarity_records = similarity_df.to_dict('records')\n",
    "\n",
    "create_similarity_query = \"\"\"\n",
    "UNWIND $records AS row\n",
    "MATCH (p1:Paper {paperId: row.src_paper_id})\n",
    "MATCH (p2:Paper {paperId: row.dst_paper_id})\n",
    "// We use an undirected relationship here because similarity is mutual\n",
    "MERGE (p1)-[r:SIMILAR_TO]-(p2)\n",
    "SET r.score = toFloat(row.score), r.method = row.method\n",
    "\"\"\"\n",
    "print(f\"\\nLoading {len(similarity_records)} SIMILAR_TO relationships...\")\n",
    "for i in range(0, len(similarity_records), batch_size):\n",
    "    batch = similarity_records[i:i + batch_size]\n",
    "    run_query(driver, create_similarity_query, records=batch)\n",
    "print(\"✅ SIMILAR_TO relationships loaded.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "causal-rl-ev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
