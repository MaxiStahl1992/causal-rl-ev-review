{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "799de44b",
   "metadata": {},
   "source": [
    "# 1. Imports, paths and logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c1fec18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: data/processed/normalized_papers.csv\n",
      "Output dir: data/processed\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s | %(levelname)s | %(message)s\")\n",
    "logger = logging.getLogger(\"create_node_csvs\")\n",
    "\n",
    "# Input and output locations\n",
    "IN_PATH = Path(\"./data/processed/normalized_papers.csv\")\n",
    "OUT_DIR = Path(\"./data/processed\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Node CSV outputs\n",
    "PAPERS_CSV         = OUT_DIR / \"Papers.csv\"\n",
    "QUERIES_CSV        = OUT_DIR / \"Queries.csv\"\n",
    "FIELDS_CSV         = OUT_DIR / \"FieldsOfStudy.csv\"\n",
    "AUTHORS_CSV        = OUT_DIR / \"Authors.csv\"\n",
    "VENUES_CSV         = OUT_DIR / \"Venues.csv\"\n",
    "\n",
    "ADDED_AT = datetime.utcnow().replace(microsecond=0).isoformat() + \"Z\"\n",
    "print(\"Input:\", IN_PATH)\n",
    "print(\"Output dir:\", OUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3c40f7",
   "metadata": {},
   "source": [
    "# 2. Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39bbd69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_list_field(val) -> List[str]:\n",
    "    \"\"\"\n",
    "    Parse a list-like field that may be:\n",
    "    - a true Python list,\n",
    "    - a JSON-encoded list of strings or dicts,\n",
    "    - a delimited string (| ; ,),\n",
    "    - a bare scalar.\n",
    "    Returns a list of stripped strings (empty if nothing usable).\n",
    "    \"\"\"\n",
    "    if val is None:\n",
    "        return []\n",
    "    if isinstance(val, list):\n",
    "        out = []\n",
    "        for x in val:\n",
    "            if isinstance(x, dict):\n",
    "                # Author/references dicts may occur elsewhere; here we just stringify non-empty values.\n",
    "                out.append(json.dumps(x, ensure_ascii=False))\n",
    "            else:\n",
    "                s = str(x).strip()\n",
    "                if s:\n",
    "                    out.append(s)\n",
    "        return out\n",
    "    s = str(val).strip()\n",
    "    if not s:\n",
    "        return []\n",
    "    # Try JSON list first\n",
    "    try:\n",
    "        obj = json.loads(s)\n",
    "        if isinstance(obj, list):\n",
    "            out = []\n",
    "            for x in obj:\n",
    "                if isinstance(x, dict):\n",
    "                    out.append(json.dumps(x, ensure_ascii=False))\n",
    "                else:\n",
    "                    xs = str(x).strip()\n",
    "                    if xs:\n",
    "                        out.append(xs)\n",
    "            return out\n",
    "    except Exception:\n",
    "        pass\n",
    "    # Fallback delimiters\n",
    "    for sep in (\"|\", \"; \", \";\", \", \", \",\"):\n",
    "        if sep in s:\n",
    "            return [t.strip() for t in s.split(sep) if t.strip()]\n",
    "    return [s]\n",
    "\n",
    "def slugify(text: str) -> str:\n",
    "    \"\"\"Make a deterministic slug: lowercase, spaces→hyphens, keep [a-z0-9-].\"\"\"\n",
    "    if text is None:\n",
    "        return \"\"\n",
    "    s = str(text).strip().lower()\n",
    "    s = re.sub(r\"\\s+\", \"-\", s)\n",
    "    s = re.sub(r\"[^a-z0-9\\-]+\", \"\", s)\n",
    "    s = re.sub(r\"-{2,}\", \"-\", s).strip(\"-\")\n",
    "    return s\n",
    "\n",
    "def norm_doi(doi: Optional[str]) -> str:\n",
    "    \"\"\"\n",
    "    Normalize DOI to lowercase without URL prefixes. Returns empty string if missing.\n",
    "    Assumes upstream normalization mostly done; this adds extra safety.\n",
    "    \"\"\"\n",
    "    if doi is None:\n",
    "        return \"\"\n",
    "    s = str(doi).strip().lower()\n",
    "    if not s:\n",
    "        return \"\"\n",
    "    s = re.sub(r\"^https?://(dx\\.)?doi\\.org/\", \"\", s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2749f76",
   "metadata": {},
   "source": [
    "# 3. Load Normalized Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "636f1cde",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-29 14:57:13,405 | INFO | Rows: 2055 | Unique paper ids: 2055\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns: ['id', 'title', 'abstract', 'year', 'publicationDate', 'doi', 'venue', 'venue_type', 'venue_url', 'fields_of_study', 's2_fields_of_study', 'citation_count', 'influential_citation_count', 'reference_count', 'references', 'author_id', 'authors', 's2_url', 'open_access_pdf', 'query', 'doi_from_openalex', 'venue_from_openalex', 'abstract_from_openalex', 'year_from_openalex', 'openalex_id']\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "title",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "abstract",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "year",
         "rawType": "Int64",
         "type": "integer"
        },
        {
         "name": "publicationDate",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "doi",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "venue",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "venue_type",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "venue_url",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "fields_of_study",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "s2_fields_of_study",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "citation_count",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "influential_citation_count",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "reference_count",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "references",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "author_id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "authors",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "s2_url",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "open_access_pdf",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "query",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "doi_from_openalex",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "venue_from_openalex",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "abstract_from_openalex",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "year_from_openalex",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "openalex_id",
         "rawType": "object",
         "type": "unknown"
        }
       ],
       "ref": "5b7b471d-2838-4bb7-91da-0faf95d415f3",
       "rows": [
        [
         "0",
         "00b75f61f8bd3246fff75f84d852ba3e80d5338e",
         "Applications of information Nonanticipative Rate Distortion Function",
         "The objective of this paper is to further investigate various applications of information Nonanticipative Rate Distortion Function (NRDF) by discussing two working examples, the Binary Symmetric Markov Source with parameter p (BSMS(p)) with Hamming distance distortion, and the multidimensional partially observed Gaussian-Markov source. For the BSMS(p), we give the solution to the NRDF, and we use it to compute the Rate Loss (RL) of causal codes with respect to noncausal codes. For the multidimensional Gaussian-Markov source, we give the solution to the NRDF, we show its operational meaning via joint source-channel matching over a vector of parallel Gaussian channels, and we compute the RL of causal and zero-delay codes with respect to noncausal codes.",
         "2014",
         "2014-01-22",
         "10.1109/isit.2014.6875397",
         "2014 IEEE International Symposium on Information Theory",
         null,
         null,
         "['Mathematics', 'Computer Science']",
         "['Mathematics', 'Computer Science']",
         "4",
         "0",
         "18",
         "['5761c61368f672c4516a6914674a39e6aa5983b8', '6c12e3176a59b0cf9dbf6b155571e5c1d1e35206', '198dcf518298e0afe39d005da5320cfe840480c1', '83ebce3943f7120d63ce82a58c5be0837362c1b9', '1957b2e9ae9c2876d7b4282adbbf71a0cb676f6a', '4889f5908adb0f58ee3b39f63ec3ee4360ab55e4', '9211e266fc2a0b83d49abc8baf8d30ad680bdef1', '406166145d0a322c16aa91c2dba0c11a772f066d', 'a962569b7decff456f08f2d1e7c7af408a629409', '6ce3c8e18021c811d0cff944b9cddd5d5cef1c9c', 'c677637c2f8b936b0dc1cba8a2fed0008620f2a9', 'd384cd823c2ff4b217cb6540636f0e891e23dcdd', None, 'f03e5bd609acc71d9f2614639df062e06e039d23', 'd789b834bcb9734199b52fb589d9d4ad614a0b45', '3ee2431657c854bd1ba873a51f6ff292cc0e89b6', None, '75b52f1746ed1b293a9396cc8981bed744115189']",
         "['145657810', '2081852', '1745427']",
         "['Photios A. Stavrou', 'C. Kourtellaris', 'C. Charalambous']",
         "https://www.semanticscholar.org/paper/00b75f61f8bd3246fff75f84d852ba3e80d5338e",
         "http://arxiv.org/pdf/1401.5828",
         "Causal Reinforcement Learning | Causal RL",
         "False",
         "False",
         "False",
         "False",
         null
        ],
        [
         "1",
         "01befcd360d36d520f595b34d5d26e37e0ac16f3",
         "Explainable Agency in Reinforcement Learning Agents",
         "This thesis explores how reinforcement learning (RL) agents can provide explanations for their actions and behaviours. As humans, we build causal models to encode cause-effect relations of events and use these to explain why events happen. Taking inspiration from cognitive psychology and social science literature, I build causal explanation models and explanation dialogue models for RL agents. By mimicking human-like explanation models, these agents can provide explanations that are natural and intuitive to humans.",
         "2020",
         "2020-04-03",
         "10.1609/aaai.v34i10.7134",
         "AAAI Conference on Artificial Intelligence",
         "conference",
         "http://www.aaai.org/",
         "['Computer Science']",
         "['Computer Science']",
         "1",
         "0",
         "14",
         "['cfb68baa23048e3e0f8845c099fa013797bd623f', '74df8f66eb0f99616c828ae5a5152a7025626ab3', '1d5304850eb1f35446d4c78a9dc6207d97e66a28', 'be711f681580d3a02c8bc4c4dab0c7a043f4e1d2', 'e89dfa306723e8ef031765e9c44e5f6f94fd8fda', 'd022f6d2c2ed802f114ed05d3c791555911c1952', '5c9d2e0d7c54bb44c60bb9c11898b41c18bd0e9a', '87272aaa0008372a3a1485993c7a6a8ef252110a', '37c6ef567e4fa84ac645863d7b55b32f0f45640a', '88290499724ce49f0f44bdb3c2a1733ec356696e', '141960241e01e4420b8b5e7ff6e6332e3e80ed22', '22da5f6f70be46c8fbf233c51c9571f5985b69ab', '6128702ddff4e1d51afbb22844ff30378d1fc3a4', '50934979694fb48e55d0cf38888f67b84ad6601b']",
         "['9303604']",
         "['Prashan Madumal']",
         "https://www.semanticscholar.org/paper/01befcd360d36d520f595b34d5d26e37e0ac16f3",
         "https://doi.org/10.1609/aaai.v34i10.7134",
         "Causal Reinforcement Learning | Causal RL",
         "False",
         "False",
         "False",
         "False",
         null
        ]
       ],
       "shape": {
        "columns": 25,
        "rows": 2
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>year</th>\n",
       "      <th>publicationDate</th>\n",
       "      <th>doi</th>\n",
       "      <th>venue</th>\n",
       "      <th>venue_type</th>\n",
       "      <th>venue_url</th>\n",
       "      <th>fields_of_study</th>\n",
       "      <th>...</th>\n",
       "      <th>author_id</th>\n",
       "      <th>authors</th>\n",
       "      <th>s2_url</th>\n",
       "      <th>open_access_pdf</th>\n",
       "      <th>query</th>\n",
       "      <th>doi_from_openalex</th>\n",
       "      <th>venue_from_openalex</th>\n",
       "      <th>abstract_from_openalex</th>\n",
       "      <th>year_from_openalex</th>\n",
       "      <th>openalex_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00b75f61f8bd3246fff75f84d852ba3e80d5338e</td>\n",
       "      <td>Applications of information Nonanticipative Ra...</td>\n",
       "      <td>The objective of this paper is to further inve...</td>\n",
       "      <td>2014</td>\n",
       "      <td>2014-01-22</td>\n",
       "      <td>10.1109/isit.2014.6875397</td>\n",
       "      <td>2014 IEEE International Symposium on Informati...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['Mathematics', 'Computer Science']</td>\n",
       "      <td>...</td>\n",
       "      <td>['145657810', '2081852', '1745427']</td>\n",
       "      <td>['Photios A. Stavrou', 'C. Kourtellaris', 'C. ...</td>\n",
       "      <td>https://www.semanticscholar.org/paper/00b75f61...</td>\n",
       "      <td>http://arxiv.org/pdf/1401.5828</td>\n",
       "      <td>Causal Reinforcement Learning | Causal RL</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01befcd360d36d520f595b34d5d26e37e0ac16f3</td>\n",
       "      <td>Explainable Agency in Reinforcement Learning A...</td>\n",
       "      <td>This thesis explores how reinforcement learnin...</td>\n",
       "      <td>2020</td>\n",
       "      <td>2020-04-03</td>\n",
       "      <td>10.1609/aaai.v34i10.7134</td>\n",
       "      <td>AAAI Conference on Artificial Intelligence</td>\n",
       "      <td>conference</td>\n",
       "      <td>http://www.aaai.org/</td>\n",
       "      <td>['Computer Science']</td>\n",
       "      <td>...</td>\n",
       "      <td>['9303604']</td>\n",
       "      <td>['Prashan Madumal']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/01befcd3...</td>\n",
       "      <td>https://doi.org/10.1609/aaai.v34i10.7134</td>\n",
       "      <td>Causal Reinforcement Learning | Causal RL</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         id  \\\n",
       "0  00b75f61f8bd3246fff75f84d852ba3e80d5338e   \n",
       "1  01befcd360d36d520f595b34d5d26e37e0ac16f3   \n",
       "\n",
       "                                               title  \\\n",
       "0  Applications of information Nonanticipative Ra...   \n",
       "1  Explainable Agency in Reinforcement Learning A...   \n",
       "\n",
       "                                            abstract  year publicationDate  \\\n",
       "0  The objective of this paper is to further inve...  2014      2014-01-22   \n",
       "1  This thesis explores how reinforcement learnin...  2020      2020-04-03   \n",
       "\n",
       "                         doi  \\\n",
       "0  10.1109/isit.2014.6875397   \n",
       "1   10.1609/aaai.v34i10.7134   \n",
       "\n",
       "                                               venue  venue_type  \\\n",
       "0  2014 IEEE International Symposium on Informati...         NaN   \n",
       "1         AAAI Conference on Artificial Intelligence  conference   \n",
       "\n",
       "              venue_url                      fields_of_study  ...  \\\n",
       "0                   NaN  ['Mathematics', 'Computer Science']  ...   \n",
       "1  http://www.aaai.org/                 ['Computer Science']  ...   \n",
       "\n",
       "                             author_id  \\\n",
       "0  ['145657810', '2081852', '1745427']   \n",
       "1                          ['9303604']   \n",
       "\n",
       "                                             authors  \\\n",
       "0  ['Photios A. Stavrou', 'C. Kourtellaris', 'C. ...   \n",
       "1                                ['Prashan Madumal']   \n",
       "\n",
       "                                              s2_url  \\\n",
       "0  https://www.semanticscholar.org/paper/00b75f61...   \n",
       "1  https://www.semanticscholar.org/paper/01befcd3...   \n",
       "\n",
       "                            open_access_pdf  \\\n",
       "0            http://arxiv.org/pdf/1401.5828   \n",
       "1  https://doi.org/10.1609/aaai.v34i10.7134   \n",
       "\n",
       "                                       query doi_from_openalex  \\\n",
       "0  Causal Reinforcement Learning | Causal RL             False   \n",
       "1  Causal Reinforcement Learning | Causal RL             False   \n",
       "\n",
       "  venue_from_openalex abstract_from_openalex year_from_openalex openalex_id  \n",
       "0               False                  False              False         NaN  \n",
       "1               False                  False              False         NaN  \n",
       "\n",
       "[2 rows x 25 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the user's normalized CSV and standardize minimal types without re-normalizing content.\n",
    "df = pd.read_csv(IN_PATH)\n",
    "\n",
    "# Critical identifiers as strings; year as nullable Int64\n",
    "df[\"id\"] = df[\"id\"].astype(str)\n",
    "if \"year\" in df.columns:\n",
    "    df[\"year\"] = pd.to_numeric(df[\"year\"], errors=\"coerce\").astype(\"Int64\")\n",
    "if \"doi\" in df.columns:\n",
    "    df[\"doi\"] = df[\"doi\"].apply(norm_doi)\n",
    "\n",
    "# Quick schema check print\n",
    "print(\"Columns:\", df.columns.tolist())\n",
    "logger.info(\"Rows: %d | Unique paper ids: %d\", len(df), df[\"id\"].nunique())\n",
    "\n",
    "# Ensure expected columns exist; fill missing non-critical columns with empty strings if absent\n",
    "for col in [\"title\",\"abstract\",\"publicationDate\",\"venue\",\"venue_type\",\"venue_url\",\"fields_of_study\",\n",
    "            \"s2_fields_of_study\",\"citation_count\",\"influential_citation_count\",\"reference_count\",\n",
    "            \"s2_url\",\"open_access_pdf\",\"query\",\"openalex_id\"]:\n",
    "    if col not in df.columns:\n",
    "        df[col] = \"\"\n",
    "\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99926584",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "title",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "abstract",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "year",
         "rawType": "Int64",
         "type": "integer"
        },
        {
         "name": "publicationDate",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "doi",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "venue",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "venue_type",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "venue_url",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "fields_of_study",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "s2_fields_of_study",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "citation_count",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "influential_citation_count",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "reference_count",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "references",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "author_id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "authors",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "s2_url",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "open_access_pdf",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "query",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "doi_from_openalex",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "venue_from_openalex",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "abstract_from_openalex",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "year_from_openalex",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "openalex_id",
         "rawType": "object",
         "type": "unknown"
        }
       ],
       "ref": "57e97487-05f8-4a01-80ca-0e85666c15df",
       "rows": [
        [
         "0",
         "00b75f61f8bd3246fff75f84d852ba3e80d5338e",
         "Applications of information Nonanticipative Rate Distortion Function",
         "The objective of this paper is to further investigate various applications of information Nonanticipative Rate Distortion Function (NRDF) by discussing two working examples, the Binary Symmetric Markov Source with parameter p (BSMS(p)) with Hamming distance distortion, and the multidimensional partially observed Gaussian-Markov source. For the BSMS(p), we give the solution to the NRDF, and we use it to compute the Rate Loss (RL) of causal codes with respect to noncausal codes. For the multidimensional Gaussian-Markov source, we give the solution to the NRDF, we show its operational meaning via joint source-channel matching over a vector of parallel Gaussian channels, and we compute the RL of causal and zero-delay codes with respect to noncausal codes.",
         "2014",
         "2014-01-22",
         "10.1109/isit.2014.6875397",
         "2014 IEEE International Symposium on Information Theory",
         null,
         null,
         "['Mathematics', 'Computer Science']",
         "['Mathematics', 'Computer Science']",
         "4",
         "0",
         "18",
         "['5761c61368f672c4516a6914674a39e6aa5983b8', '6c12e3176a59b0cf9dbf6b155571e5c1d1e35206', '198dcf518298e0afe39d005da5320cfe840480c1', '83ebce3943f7120d63ce82a58c5be0837362c1b9', '1957b2e9ae9c2876d7b4282adbbf71a0cb676f6a', '4889f5908adb0f58ee3b39f63ec3ee4360ab55e4', '9211e266fc2a0b83d49abc8baf8d30ad680bdef1', '406166145d0a322c16aa91c2dba0c11a772f066d', 'a962569b7decff456f08f2d1e7c7af408a629409', '6ce3c8e18021c811d0cff944b9cddd5d5cef1c9c', 'c677637c2f8b936b0dc1cba8a2fed0008620f2a9', 'd384cd823c2ff4b217cb6540636f0e891e23dcdd', None, 'f03e5bd609acc71d9f2614639df062e06e039d23', 'd789b834bcb9734199b52fb589d9d4ad614a0b45', '3ee2431657c854bd1ba873a51f6ff292cc0e89b6', None, '75b52f1746ed1b293a9396cc8981bed744115189']",
         "['145657810', '2081852', '1745427']",
         "['Photios A. Stavrou', 'C. Kourtellaris', 'C. Charalambous']",
         "https://www.semanticscholar.org/paper/00b75f61f8bd3246fff75f84d852ba3e80d5338e",
         "http://arxiv.org/pdf/1401.5828",
         "Causal Reinforcement Learning | Causal RL",
         "False",
         "False",
         "False",
         "False",
         null
        ],
        [
         "1",
         "01befcd360d36d520f595b34d5d26e37e0ac16f3",
         "Explainable Agency in Reinforcement Learning Agents",
         "This thesis explores how reinforcement learning (RL) agents can provide explanations for their actions and behaviours. As humans, we build causal models to encode cause-effect relations of events and use these to explain why events happen. Taking inspiration from cognitive psychology and social science literature, I build causal explanation models and explanation dialogue models for RL agents. By mimicking human-like explanation models, these agents can provide explanations that are natural and intuitive to humans.",
         "2020",
         "2020-04-03",
         "10.1609/aaai.v34i10.7134",
         "AAAI Conference on Artificial Intelligence",
         "conference",
         "http://www.aaai.org/",
         "['Computer Science']",
         "['Computer Science']",
         "1",
         "0",
         "14",
         "['cfb68baa23048e3e0f8845c099fa013797bd623f', '74df8f66eb0f99616c828ae5a5152a7025626ab3', '1d5304850eb1f35446d4c78a9dc6207d97e66a28', 'be711f681580d3a02c8bc4c4dab0c7a043f4e1d2', 'e89dfa306723e8ef031765e9c44e5f6f94fd8fda', 'd022f6d2c2ed802f114ed05d3c791555911c1952', '5c9d2e0d7c54bb44c60bb9c11898b41c18bd0e9a', '87272aaa0008372a3a1485993c7a6a8ef252110a', '37c6ef567e4fa84ac645863d7b55b32f0f45640a', '88290499724ce49f0f44bdb3c2a1733ec356696e', '141960241e01e4420b8b5e7ff6e6332e3e80ed22', '22da5f6f70be46c8fbf233c51c9571f5985b69ab', '6128702ddff4e1d51afbb22844ff30378d1fc3a4', '50934979694fb48e55d0cf38888f67b84ad6601b']",
         "['9303604']",
         "['Prashan Madumal']",
         "https://www.semanticscholar.org/paper/01befcd360d36d520f595b34d5d26e37e0ac16f3",
         "https://doi.org/10.1609/aaai.v34i10.7134",
         "Causal Reinforcement Learning | Causal RL",
         "False",
         "False",
         "False",
         "False",
         null
        ],
        [
         "2",
         "01e9241dbb9eaca99b86468bb079f4b631b71671",
         "Causal prompting model-based offline reinforcement learning",
         "Model-based offline Reinforcement Learning (RL) allows agents to fully utilise pre-collected datasets without requiring additional or unethical explorations. However, applying model-based offline RL to online systems presents challenges, primarily due to the highly suboptimal (noise-filled) and diverse nature of datasets generated by online systems. To tackle these issues, we introduce the Causal Prompting Reinforcement Learning (CPRL) framework, designed for highly suboptimal and resource-constrained online scenarios. The initial phase of CPRL involves the introduction of the Hidden-Parameter Block Causal Prompting Dynamic (Hip-BCPD) to model environmental dynamics. This approach utilises invariant causal prompts and aligns hidden parameters to generalise to new and diverse online users. In the subsequent phase, a single policy is trained to address multiple tasks through the amalgamation of reusable skills, circumventing the need for training from scratch. Experiments conducted across datasets with varying levels of noise, including simulation-based and real-world offline datasets from the Dnurse APP, demonstrate that our proposed method can make robust decisions in out-of-distribution and noisy environments, outperforming contemporary algorithms. Additionally, we separately verify the contributions of Hip-BCPDs and the skill-reuse strategy to the robustness of performance. We further analyse the visualised structure of Hip-BCPD and the interpretability of sub-skills. We released our source code and the first ever real-world medical dataset for precise medical decision-making tasks.",
         "2024",
         "2024-06-03",
         "10.48550/arxiv.2406.01065",
         "arXiv.org",
         null,
         "https://arxiv.org",
         "['Computer Science']",
         "['Computer Science']",
         "0",
         "0",
         "0",
         "[]",
         "['2116329956', '2153979217', '2303466987', '2280033365', '2304715506', '2259229']",
         "['Xuehui Yu', 'Yi Guan', 'Rujia Shen', 'Xin Li', 'Chen Tang', 'Jingchi Jiang']",
         "https://www.semanticscholar.org/paper/01e9241dbb9eaca99b86468bb079f4b631b71671",
         null,
         "Causal Reinforcement Learning | Causal RL",
         "False",
         "False",
         "False",
         "False",
         null
        ],
        [
         "3",
         "026dc8d3cbb360bdd12d19c924bc633221c9b423",
         "Learning Causal Overhypotheses through Exploration in Children and Computational Models",
         "Despite recent progress in reinforcement learning (RL), RL algorithms for exploration still remain an active area of research. Existing methods often focus on state-based metrics, which do not consider the underlying causal structures of the environment, and while recent research has begun to explore RL environments for causal learning, these environments primarily leverage causal information through causal inference or induction rather than exploration. In contrast, human children - some of the most proficient explorers - have been shown to use causal information to great benefit. In this work, we introduce a novel RL environment designed with a controllable causal structure, which allows us to evaluate exploration strategies used by both agents and children in a unified environment. In addition, through experimentation on both computation models and children, we demonstrate that there are significant differences between information-gain optimal RL exploration in causal environments and the exploration of children in the same environments. We conclude with a discussion of how these findings may inspire new directions of research into efficient exploration and disambiguation of causal structures for RL algorithms.",
         "2022",
         "2022-02-21",
         "10.48550/arxiv.2202.10430",
         "CLEaR",
         "conference",
         "http://www.jolace.com/publications/clear/",
         "['Computer Science']",
         "['Computer Science']",
         "9",
         "1",
         "50",
         "['3be84e24b144541a8cd9030526ef2b8ef2cbfe54', 'b2c70c4d23c98dd4e77234fe0720595d3d565a12', '640ed82c67c3f9fe56997ecc14e350581019984b', '316c5d697f7caf92b419213e929a6063afaf253c', '05bf1ca3a8a1366970bef3cd9275859e7430ef56', '2342b32e245989103dbc56d6f07f1400f4fd2e06', '2bb5873a1a96205fb86cee12bf137f48ef13f675', '9d55314573ec254569df35ecc4cc8d464431f2cc', '0bc855f84668b35cb65618d996d09f6e434d28c9', 'a2fdfda785b3a2a0178d174daa515377c531f222', '8d99d02296be69e6615f4ba9bcdb95e1b75b3fbe', '8d2175d9418a63eb95c1a20c8747ddbef69ceec4', '836bfba6faf0d72f5202e002ef01c7b50c718fde', 'c441a7e9b1c7ecf54f583cd61e896faa10b60358', '87a40a2d506fec06d67dc8f14ce2c708a789a2b4', '8f06c78435b9712aa77094796075b8e6b3f50c05', 'ef2bc452812d6005ab0a66af6c3f97b6b0ba837e', '4cb3fd057949624aa4f0bbe7a6dcc8777ff04758', '1b19f433a3e8497e9d9bd67efb108521d16b5b85', 'caea502325b6a82b1b437c62585992609b5aa542', '9f67b3edc67a35c884bd532a5e73fa3a7f3660d8', '4e6680d350dde09b05794f0521253af63690486e', 'b72e488b48e464c7633418873f1cf9fbf3453ff2', '3b290ffa1f4f8226e326f00984acecdfbe9e28bf', 'd465b7687a75fd5f909bce8ad736718c32b943e7', '0f810eb4777fd05317951ebaa7a3f5835ee84cf4', '225ab689f41cef1dc18237ef5dab059a49950abf', '12f67fb182bc934fc95ce97acff553d83e2ca72e', '0fcb2034e31a2bc2f12a2b1363d0d77baf445fdf', '6e90fd78e8a3b98af3954aae5209703aa966603e', '2b10281297ee001a9f3f4ea1aa9bea6b638c27df', '4b63e34276aa98d5345efa7fe09bb06d8a9d8f52', '18583db96535b07332e0e6c0193e3ac72409f765', '622e1bfecc11c019ecd752471e9444673ab1c067', 'a5c602afb195c2203b1fe1138c5c6f6184f61da6', '089827b0895bdbea2d15bfed0c400f9245e8a157', '8012f6bdb3cccd5b7dc2cb577c072de5563fe7c6', '87cce3752a730403975cb9f6c0e9521d58494d39', '4dce5692fac1440f0d6d52782a2943cff3620ad8', '94fd1c0ec926f6558c92911579040c439c99146b', '95745f70f8bb68db0d73c09c29fe964ee48b5460', '0d21adbba3a679c95d52544a05cfc40a510da129', 'da559bd90f2490f58ad91f7d43bab26823239f2c', '94db34f4b68189bfcba22beab33ee3b54f10b876', '77c9b9288a29d56676ba9cecf462c950971537d9', '2af61b978d2097fb11b3b7d6376386676a532205', 'b7bfe204e2c06527474088a159792c87c22536d5', '97707bfcb63c528e490243e3e1df06e275fee996', '792695c436fd0148a71e7f2830ea5bac7938b014', '9832bfbc03b85d64afb0ebc59a8217d600144315']",
         "['8519553', '2150491107', '39229748', '152502885', '2158860', '145604319', '2064588', '2155486466', '1729041', '2222423']",
         "['Eliza Kosoy', 'Adrian Liu', 'Jasmine Collins', 'David Chan', 'Jessica B. Hamrick', 'Nan Rosemary Ke', 'Sandy H. Huang', 'Bryanna Kaufmann', 'J. Canny', 'A. Gopnik']",
         "https://www.semanticscholar.org/paper/026dc8d3cbb360bdd12d19c924bc633221c9b423",
         null,
         "Causal Reinforcement Learning | Causal RL",
         "True",
         "False",
         "False",
         "False",
         "https://openalex.org/W4221153082"
        ],
        [
         "4",
         "0348b36927f740b82f51afcd1c35cae8386bc336",
         "Segmented Encoding for Sim2Real of RL-based End-to-End Autonomous Driving",
         "Among the challenges in the recent research of end-to-end (E2E) driving, interpretability and distribution shift in the simulation-to-real (Sim2Real) have drawn considerable attention. Because of low interpretability, we cannot clearly explain the causal relationship between the input image and the control actions by the network. Moreover, the distribution shift problem in Sim2Real degrades the driving performance of the policy in the realworld deployment. In this paper, we propose a segmentation-based classwise disentangled latent encoding algorithm to cope with the two challenges. In the proposed algorithm, multi-class segmentation transfers RGB images in both simulation and real environments to the same domain, while preserving the necessary information of objects of primary classes, such as pedestrian, road, and cars, for driving decisions. Besides, in the class-wise disentangled latent encoding, segmented images are encoded to a latent vector, which improves the interpretability significantly, since the state input has a structured format. The interpretability improvement is testified by the t-stochastic neighbor embedding, image reconstruction and the causal relationship between the real images and the control actions. We deploy the driving policy trained in the simulation directly to an autonomous vehicle platform and show, to the best of our knowledge, the first demonstration of the RL-based E2E autonomous in various real environments.",
         "2022",
         "2022-06-05",
         "10.1109/iv51971.2022.9827374",
         "2022 IEEE Intelligent Vehicles Symposium (IV)",
         null,
         null,
         "['Computer Science']",
         "['Computer Science']",
         "3",
         "0",
         "20",
         "[]",
         "['47238664', '39530824', '2179287901', '67345281']",
         "['Seung H. Chung', 'S. Kong', 'S. Cho', 'I. M. A. Nahrendra']",
         "https://www.semanticscholar.org/paper/0348b36927f740b82f51afcd1c35cae8386bc336",
         null,
         "Causal Reinforcement Learning | Causal RL",
         "False",
         "False",
         "False",
         "False",
         null
        ],
        [
         "5",
         "03899c6a748ac99f656b79299187e1c7ee7317e0",
         "Predictor-Corrector(PC) Temporal Difference(TD) Learning (PCTD)",
         "Using insight from numerical approximation of ODEs and the problem formulation and solution methodology of TD learning through a Galerkin relaxation, I propose a new class of TD learning algorithms. After applying the improved numerical methods, the parameter being approximated has a guaranteed order of magnitude reduction in the Taylor Series error of the solution to the ODE for the parameter $\\theta(t)$ that is used in constructing the linearly parameterized value function. Predictor-Corrector Temporal Difference (PCTD) is what I call the translated discrete time Reinforcement Learning(RL) algorithm from the continuous time ODE using the theory of Stochastic Approximation(SA). Both causal and non-causal implementations of the algorithm are provided, and simulation results are listed for an infinite horizon task to compare the original TD(0) algorithm against both versions of PCTD(0).",
         "2021",
         "2021-04-15",
         "nan",
         "arXiv.org",
         null,
         "https://arxiv.org",
         "['Computer Science']",
         "['Computer Science']",
         "1",
         "0",
         "9",
         "['45602b4aa5d3d67dac367c5a73f857ea7ab787bc', '1f1255e3f47f18acb3fd31b465737fe024181584', '8237b2311d0c41eb4562b3c70eeb1626c64334b6', '6024d2494e6b918bfa9977c9b98283688a66740e', '6896019fb53f868a72693669e76470c26d7fa4b0', '0474ee77870f38fde91fce08a287d30aa9af4811', '9813c4933a6a3f5d0c9a95cdff43258522979fef', '97efafdb4a3942ab3efba53ded7413199f79c054', 'b23b75e82a74ef4967b07573e8475bcc6a47a559']",
         "['1471441763']",
         "['C. Bowyer']",
         "https://www.semanticscholar.org/paper/03899c6a748ac99f656b79299187e1c7ee7317e0",
         null,
         "Causal Reinforcement Learning | Causal RL",
         "False",
         "False",
         "False",
         "False",
         "https://openalex.org/W3153049565"
        ],
        [
         "6",
         "03e19dbf435d39d729d8e6d44cb36e422f66b2be",
         "Projected State-action Balancing Weights for Offline Reinforcement Learning",
         "Offline policy evaluation (OPE) is considered a fundamental and challenging problem in reinforcement learning (RL). This paper focuses on the value estimation of a target policy based on pre-collected data generated from a possibly different policy, under the framework of infinite-horizon Markov decision processes. Motivated by the recently developed marginal importance sampling method in RL and the covariate balancing idea in causal inference, we propose a novel estimator with approximately projected state-action balancing weights for the policy value estimation. We obtain the convergence rate of these weights and show that the proposed value estimator is semi-parametric efficient under technical conditions. In terms of asymptotics, our results scale with both the number of trajectories and the number of decision points at each trajectory. As such, consistency can still be achieved with a limited number of subjects when the number of decision points diverges. In addition, we develop a necessary and sufficient condition for establishing the well-posedness of the Bellman operator in the off-policy setting, which characterizes the difficulty of OPE and may be of independent interest. Numerical experiments demonstrate the promising performance of our proposed estimator.",
         "2021",
         "2021-09-10",
         "10.1214/23-aos2302",
         "Annals of Statistics",
         "journal",
         "https://www.jstor.org/journal/annalsstatistics",
         "['Computer Science', 'Mathematics']",
         "['Computer Science', 'Mathematics']",
         "20",
         "0",
         "79",
         "['9ad4777a95bad31a4119b1b062573e38fc069fc1', '3b152cd6c33a0d2565a91a7caf9e94126ff84465', '1791f0c80dacdb7eb7bdbcea74ffa3f0acfc8ebe', 'ea6c459c2ec5ba478b3c8782bc02f27846771fe1', '82da3373cdf63393c004cf8b87e5af86398bccd3', '5e7bc93622416f14e6948a500278bfbe58cd3890', '2b0e79ed1340a79344e37b6f57191b76d810962f', '33612f0f6386dcc34e44454c5073bc1d7d68451d', '8ccda7750efbbb35cbc13590aafcbac8dca022a7', 'bcc378385c8d5a1d4cd5b78bce17879b1405f89c', '014fa07e99a5de181c217d5bc6acbe60ee071ab0', '4f29f21b9711a83376be3072ac3af42d1943eba9', '7b5773650b4e52568d4e9e4c6384e9034d40fd66', 'c2c5d4650beb72a0e00a4c384064f4ab2ddef1dc', 'f49cfdbc1052fd012242743c2da27ee8bb4f9277', '25a7b8c2e110a1bcd5c42ab5de55a0c08b0b8846', '9001698e033524864d4d45f051a5ba362d4afd9e', '875280d96b2f138902061ae6409249ee4ded0da3', '3203be64597368d356b3ebca54a15699a3714e24', 'a4e07c37d9ec0cffdd2052b3f4d7022441c4f826', '6da39d0d4df877b8068b3d1ca02d9634baed2740', 'e81ea45d8bec329fdb11fd84990852f620895d6f', 'cc07fc48ce2a381e7f39235cef5fd10b939182c4', '29dd1f2daaa7929e597a25b9e73ba7233262fe30', '56136aa0b2c347cbcf3d50821f310c4253155026', '52de8d633c479507a3838bbe60f1e23c9c645331', '634a2c1b21c07a16d533aacab76d4190cd83df76', '6fe4e3772b98aaf5cae0ad3b1081a50550ac51e0', '766133888c58c949de2e9bf086fb49a2cb3d295f', 'c78e33553f33e4987e87ba334ec7c50856ae0f49', '3a218693acc16fa30fadb76b44b57d3443951892', '09b4a22b518eaf5bcfeae6af9851e153b6c61add', '3f0b4d879cb448b7e8870b68b944f749a8cb438d', '1dfdd662abbe67ccba6cd885c16a710c61dd04ff', 'f1a626bee8bb1d0cb9f9647bb15143911cf5d166', 'c88763faeebfc910a4720f339d8e07782fd396dd', '3504565c8c3cbae18a50247604f07a7e01cde801', '5887ce8b4ae719af24188a42e426718989083cde', '933c6a9ca58600d4ab080fff832490b0a1c5c54b', '4629c0b55bb92313d01c1be4541d37e6fee309cb', '244db45b7cd61e0717be7032e78c6a612ef3b49a', '14b4b81303cc6e746a27eefbc4989e200be1b10e', 'af977d2e3fcecbec48bef0e79e9a5ec8e2fb8b2c', 'c75d5d62dc53efa7fa873f24d6dbec8945e3cda7', 'f667ac48121db41d62c4e27cdd37b917a3904aaa', 'fd855edabd177f903df92a82514bfaa68335ba0b', 'ae3b05bd3185c72c2dd0517dd44bdb47d1a8b590', 'd2265b96291f15d8b663da97fbd751f9d92bd7cb', '59183f99d570e2888447a2496fa27ace3e7e6ce4', 'a4720303031cf063637c1605f21c296e485c24ad', '872d01b512def14d5a7d9dedecb4f8b3a67d979b', '616474134107bb0e91c3635f315e730ec3986952', '126a25352db3bd310b7e48df9604e26b2b8d3e2a', '78020db7e3d968f6e6cc26d18e31e5b668ca7fee', 'c6d971aa7630ef28f79fc57351e5abac0b8e2ddd', '6c8ee290be214ea31c52dc6f9bd9c104ab6734e8', '46c56845fbb9e9452a318d736356949bd24fa012', 'a9cd8efe9184dddb1bedbbec3a356c4dfb22fe63', '439febbb243eede1e3e315eba770008261efdba8', 'af10ebcab9c0c55a018052ca7e4df001c6a54314', 'e786caa59202d923ccaae00ae6a4682eec92699b', '41ebbfd30d7fd44c47814b73ba36f0eae6415629', '2f0be7ef09295368bb1b1f01945ff337e0d44b39', 'f7ad50732cdc13e33f62336fa56c80ae8ac90ced', '335076e3d689cd5122f8cb262d7651c3e8ffd9b4', '531aab3000f5373d2c99c2c171b7185f83f3a167', '186ebb48a0583d237dc5dfb950f4774ea767b2b0', '692534d0d47974ba2d4d230a80a7efe763b33ad0', '589949ff835be2da6b607f55e0619fcf321ee751', 'e4f70641d07f88b628ffeeb9f309dc6d88d33b69', '6507b811893de0a1a12ed9457e8ec658cf5645f8', '04ecebe35187aac6b8e05ae82d139fce8b764951', 'c3ce78766a58ece15bc2b1ee2a8d33c3b8549589', None, 'e9e495ff046b0da2948b1de10c9ea008804f40a5', '97efafdb4a3942ab3efba53ded7413199f79c054', 'b80e2143bbe4999e83c708293e37066406534a61', '7c268394c54e26f1488a23640291d71267d6a97c', None]",
         "['2110272295', '3146500', '144717679']",
         "['Jiayi Wang', 'Zhengling Qi', 'Raymond K. W. Wong']",
         "https://www.semanticscholar.org/paper/03e19dbf435d39d729d8e6d44cb36e422f66b2be",
         "https://arxiv.org/pdf/2109.04640",
         "Causal Reinforcement Learning | Causal RL",
         "False",
         "False",
         "False",
         "False",
         null
        ],
        [
         "7",
         "03fd18ef5127ab491db4a1921cd8da29f6935018",
         "Towards Generalizable Reinforcement Learning via Causality-Guided Self-Adaptive Representations",
         "General intelligence requires quick adaption across tasks. While existing reinforcement learning (RL) methods have made progress in generalization, they typically assume only distribution changes between source and target domains. In this paper, we explore a wider range of scenarios where not only the distribution but also the environment spaces may change. For example, in the CoinRun environment, we train agents from easy levels and generalize them to difficulty levels where there could be new enemies that have never occurred before. To address this challenging setting, we introduce a causality-guided self-adaptive representation-based approach, called CSR, that equips the agent to generalize effectively across tasks with evolving dynamics. Specifically, we employ causal representation learning to characterize the latent causal variables within the RL system. Such compact causal representations uncover the structural relationships among variables, enabling the agent to autonomously determine whether changes in the environment stem from distribution shifts or variations in space, and to precisely locate these changes. We then devise a three-step strategy to fine-tune the causal model under different scenarios accordingly. Empirical experiments show that CSR efficiently adapts to the target domains with only a few samples and outperforms state-of-the-art baselines on a wide range of scenarios, including our simulated environments, CartPole, CoinRun and Atari games.",
         "2024",
         "2024-07-30",
         "10.48550/arxiv.2407.20651",
         "International Conference on Learning Representations",
         "conference",
         "https://iclr.cc/",
         "['Computer Science']",
         "['Computer Science']",
         "1",
         "0",
         "66",
         "[]",
         "['2313354156', '2313614198', '2293287494', '2313740191', '2267039846', '2109329726']",
         "['Yupei Yang', 'Biwei Huang', 'Fan Feng', 'Xinyue Wang', 'Shikui Tu', 'Lei Xu']",
         "https://www.semanticscholar.org/paper/03fd18ef5127ab491db4a1921cd8da29f6935018",
         null,
         "Causal Reinforcement Learning | Causal RL",
         "False",
         "False",
         "False",
         "False",
         null
        ],
        [
         "8",
         "0425c47e19b5f1fcc680967ebd6c6e7cebc0b768",
         "Aligning as Debiasing: Causality-Aware Alignment via Reinforcement Learning with Interventional Feedback",
         "Large language models (LLMs) often generate biased outputs containing offensive, toxic, or stereotypical text. Existing LLM alignment methods such as reinforcement learning from human feedback (RLHF) alleviate biases primarily based on reward signals from current model outputs without considering the source of biases. In this work, to explore how biases are formed, we revisit LLMs’ text generation from a causal perspective. We identify pretraining data and input prompts, which contain semantic correlations of textual phrases, as two confounders between LLMs and model outputs causing biases. Inspired by our causal view, we leverage the reward model in RL alignment as an instrumental variable to perform causal intervention on LLMs. Utilizing the reward difference between an initial LLM and intervened LLM as interventional feedback to guide RL finetuning, we propose Causality-Aware Alignment (CAA) for LLM debiasing. Experiments on two text generation tasks with three different alignment objectives demonstrate the advantages of our method in aligning LLMs to generate less biased and safer outputs.",
         "2024",
         null,
         "10.18653/v1/2024.naacl-long.262",
         "North American Chapter of the Association for Computational Linguistics",
         "conference",
         "https://www.aclweb.org/portal/naacl",
         "['Computer Science']",
         "['Computer Science']",
         "11",
         "0",
         "60",
         "[]",
         "['2282516443', '2281159998', '51002202', '2273802124', '2284680226', '2294834537']",
         "['Yu Xia', 'Tong Yu', 'Zhankui He', 'Handong Zhao', 'Julian J. McAuley', 'Shuai Li']",
         "https://www.semanticscholar.org/paper/0425c47e19b5f1fcc680967ebd6c6e7cebc0b768",
         null,
         "Causal Reinforcement Learning | Causal RL",
         "False",
         "False",
         "False",
         "False",
         null
        ],
        [
         "9",
         "045c71ae19740cbd93fb7a2e94bc9096ae7345e1",
         "Emergent α\\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\alpha $$\\end{document}-like fermionic vacuum structure and",
         "We report a non-trivial feature of the vacuum structure of free massive or massless Dirac fields in the hyperbolic de Sitter spacetime. Here we have two causally disconnected regions, say R and L separated by another region, C. We are interested in the field theory in R∪L\\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$R\\cup L$$\\end{document} to understand the long range quantum correlations between R and L. There are local modes of the Dirac field having supports individually either in R or L, as well as global modes found via analytically continuing the R modes to L and vice versa. However, we show that unlike the case of a scalar field, the analytic continuation does not preserve the orthogonality of the resulting global modes. Accordingly, we need to orthonormalise them following the Gram–Schmidt prescription, prior to the field quantisation in order to preserve the canonical anti-commutation relations. We observe that this prescription naturally incorporates a spacetime independent continuous parameter, θRL\\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\theta _{\\mathrm{RL}}$$\\end{document}, into the picture. Thus interestingly, we obtain a naturally emerging one-parameter family of α\\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\alpha $$\\end{document}-like de Sitter vacua. The values of θRL\\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\theta _{\\mathrm{RL}}$$\\end{document} yielding the usual thermal spectra of massless created particles are pointed out. Next, using these vacua, we investigate both entanglement and Rényi entropies of either of the regions and demonstrate their dependence on θRL\\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\theta _{\\mathrm{RL}}$$\\end{document}.",
         "2019",
         "2019-09-01",
         "10.1140/epjc/s10052-019-7319-x",
         "The European Physical Journal C",
         null,
         null,
         "[]",
         "[]",
         "0",
         "0",
         "0",
         "[]",
         "['46508728', '103192109', '1382445838']",
         "['Sourav Bhattacharya', 'S. Chakrabortty', 'Shivang Goyal']",
         "https://www.semanticscholar.org/paper/045c71ae19740cbd93fb7a2e94bc9096ae7345e1",
         "https://link.springer.com/content/pdf/10.1140/epjc/s10052-019-7319-x.pdf",
         "Causal Reinforcement Learning | Causal RL",
         "False",
         "False",
         "False",
         "False",
         null
        ],
        [
         "10",
         "049cb2a029702540399f06fe2889dd1d1961a186",
         "Inherently Explainable Reinforcement Learning in Natural Language",
         "We focus on the task of creating a reinforcement learning agent that is inherently explainable -- with the ability to produce immediate local explanations by thinking out loud while performing a task and analyzing entire trajectories post-hoc to produce causal explanations. This Hierarchically Explainable Reinforcement Learning agent (HEX-RL), operates in Interactive Fictions, text-based game environments in which an agent perceives and acts upon the world using textual natural language. These games are usually structured as puzzles or quests with long-term dependencies in which an agent must complete a sequence of actions to succeed -- providing ideal environments in which to test an agent's ability to explain its actions. Our agent is designed to treat explainability as a first-class citizen, using an extracted symbolic knowledge graph-based state representation coupled with a Hierarchical Graph Attention mechanism that points to the facts in the internal graph representation that most influenced the choice of actions. Experiments show that this agent provides significantly improved explanations over strong baselines, as rated by human participants generally unfamiliar with the environment, while also matching state-of-the-art task performance.",
         "2021",
         "2021-12-16",
         "10.48550/arxiv.2112.08907",
         "Neural Information Processing Systems",
         "conference",
         "http://neurips.cc/",
         "['Computer Science']",
         "['Computer Science']",
         "23",
         "2",
         "40",
         "['962aa5b847f1692af058bd14fc0e8c3f0a0fee73', 'd60f0bc0be7b8ae8e049ceb906530ba7744229c5', '8aed57b61457655e8354f1b68b34ed1cc0a222ef', '63913530782522e0d7ca5deceb40c08d606cafab', '3b2ec1d8e56131d2644b1de89733262adc720716', '0cf3f09f1eaf9f2811883b2a34bf49a35703c5a4', '15b91292ba80adaa87361a0e8894e47899f02f1d', '028c1a07ac62bbdb681d11cacf4c7485f9aa3ef7', '7a064df1aeada7e69e5173f7d4c8606f4470365b', '221d453c165aca6bc1a054289eb510e558a23dca', 'cfb68baa23048e3e0f8845c099fa013797bd623f', 'ef7df5eae54107c013885231eb7af4431f2e6158', '787c0a21cdcf59b50db201a564ce57fe0318360c', '89daae27e7df4a418b9610d307ce3df0e30fc8a2', '4d1c856275744c0284312a3a50efb6ca9dc4cd4c', '3219527aa44d7789c2ed842c90bbc6da0eacd527', 'c43bba87b4237a93d96b2a3e91da25d91fd0bb91', 'ebc3bd9cd67193b3a8f84d43d5b4377107c680dc', '33998aff64ce51df8dee45989cdca4b6b1329ec4', '77d30cf9a34fb6b50979c6a68863099da9a060ad', 'ef731bc5b85867e97fbc01621282159c8a1f6ceb', 'c2ea7edbf76577ec5ac87116cc9cb71e7f744d88', 'bf55591e09b58ea9ce8d66110d6d3000ee804bdd', '7200969d70cf6f3fd343f48e97b8ebf7d563a584', '69e76e16740ed69f4dc55361a3d319ac2f1293dd', '1c4927af526d5c28f7c2cfa492ece192d80a61d4', '3171ec184b5fec0bc7b47356ad74d8598e858ddc', '4d8f2d14af5991d4f0d050d22216825cac3157bd', '639acfebc47ac0b905d390510346c48c0e112624', 'd47677337b1083d6bfa940748da0780b2c9faf7d', '9405cc0d6169988371b2755e573cc28650d14dfe', None, None, None, None, None, None, None, None, None]",
         "['2115814529', '2065904932', '19179135']",
         "['Xiangyu Peng', 'Mark O. Riedl', 'Prithviraj Ammanabrolu']",
         "https://www.semanticscholar.org/paper/049cb2a029702540399f06fe2889dd1d1961a186",
         null,
         "Causal Reinforcement Learning | Causal RL",
         "True",
         "False",
         "False",
         "False",
         "https://openalex.org/W4225712693"
        ],
        [
         "11",
         "0518677c14757b668634748629da478f4b511e1e",
         "A Framework for Empowering Reinforcement Learning Agents with Causal Analysis: Enhancing Automated Cryptocurrency Trading",
         "Despite advances in artificial intelligence-enhanced trading methods, developing a profitable automated trading system remains challenging in the rapidly evolving cryptocurrency market. This research focuses on developing a reinforcement learning (RL) framework to tackle the complexities of trading five prominent altcoins: Binance Coin, Ethereum, Litecoin, Ripple, and Tether. To this end, we present the CausalReinforceNet~(CRN) framework, which integrates both Bayesian and dynamic Bayesian network techniques to empower the RL agent in trade decision-making. We develop two agents using the framework based on distinct RL algorithms to analyse performance compared to the Buy-and-Hold benchmark strategy and a baseline RL model. The results indicate that our framework surpasses both models in profitability, highlighting CRN's consistent superiority, although the level of effectiveness varies across different cryptocurrencies.",
         "2023",
         "2023-10-14",
         "10.48550/arxiv.2310.09462",
         "arXiv.org",
         null,
         "https://arxiv.org",
         "['Computer Science']",
         "['Computer Science']",
         "2",
         "0",
         "74",
         "[]",
         "['50462422', '3204192', '2118338', '98731626']",
         "['R. Amirzadeh', 'D. Thiruvady', 'A. Nazari', 'M. Ee']",
         "https://www.semanticscholar.org/paper/0518677c14757b668634748629da478f4b511e1e",
         null,
         "Causal Reinforcement Learning | Causal RL",
         "False",
         "False",
         "False",
         "False",
         null
        ],
        [
         "12",
         "06147d3cf19e00a6baeeee508f41cec68315db5f",
         "The Isolation, Identification and Treatment of the Causal Agents of Chronic Paronychia in House Wives.",
         "The oral carriage of Cardnld specid \\aas (369/0); this indicates that oral contarnjnation of the hands with Cdn.P.",
         "2005",
         null,
         "nan",
         null,
         null,
         null,
         "['Medicine']",
         "['Medicine']",
         "0",
         "0",
         "0",
         "[]",
         "['1431022669']",
         "['Ameene H. Al-Khafaji']",
         "https://www.semanticscholar.org/paper/06147d3cf19e00a6baeeee508f41cec68315db5f",
         null,
         "Causal Reinforcement Learning | Causal RL",
         "False",
         "False",
         "False",
         "False",
         null
        ],
        [
         "13",
         "06df2005b1244940c711eb4819be41cc26e7eed7",
         "ACE : Off-Policy Actor-Critic with Causality-Aware Entropy Regularization",
         "The varying significance of distinct primitive behaviors during the policy learning process has been overlooked by prior model-free RL algorithms. Leveraging this insight, we explore the causal relationship between different action dimensions and rewards to evaluate the significance of various primitive behaviors during training. We introduce a causality-aware entropy term that effectively identifies and prioritizes actions with high potential impacts for efficient exploration. Furthermore, to prevent excessive focus on specific primitive behaviors, we analyze the gradient dormancy phenomenon and introduce a dormancy-guided reset mechanism to further enhance the efficacy of our method. Our proposed algorithm, ACE: Off-policy Actor-critic with Causality-aware Entropy regularization, demonstrates a substantial performance advantage across 29 diverse continuous control tasks spanning 7 domains compared to model-free RL baselines, which underscores the effectiveness, versatility, and efficient sample efficiency of our approach. Benchmark results and videos are available at https://ace-rl.github.io/.",
         "2024",
         "2024-02-22",
         "10.48550/arxiv.2402.14528",
         "International Conference on Machine Learning",
         "conference",
         "https://icml.cc/",
         "['Computer Science']",
         "['Computer Science']",
         "13",
         "2",
         "77",
         "[]",
         "['2072498620', '83158497', '2285126202', '2263721561', '2331481931', '2285036703', '2108815599', '2238405926', '2242091789', '2255379295']",
         "['Tianying Ji', 'Yongyuan Liang', 'Yan Zeng', 'Yu Luo', 'Guowei Xu', 'Jiawei Guo', 'Ruijie Zheng', 'Furong Huang', 'Fuchun Sun', 'Huazhe Xu']",
         "https://www.semanticscholar.org/paper/06df2005b1244940c711eb4819be41cc26e7eed7",
         null,
         "Causal Reinforcement Learning | Causal RL",
         "False",
         "False",
         "False",
         "False",
         null
        ],
        [
         "14",
         "07a6be4101e2a57b0e9b72190560e63af01a267e",
         "Hierarchical RNNs-Based Transformers MADDPG for Mixed Cooperative-Competitive Environments",
         "Structural models based on Attention can not only record the relationships between features’ position, but also can measure the importance of different features based on their weights. By establishing dynamically weighted parameters for choosing relevant and irrelevant features, the key information can be strengthened, and the irrelevant information can be weakened. Therefore, the efficiency of Deep Learning algorithms can be significantly elevated and improved. Although Transformer have been performed very well in many fields including Reinforcement Learning (RL). We tried to integrate Transformers into RL, however there are some challenge in this task. Especially, MARL (known as Multi-Agent Reinforcement Learning), which can be recognized as a set of independent agents trying to adapt and learn through their way to reach the goal. In order to emphasize the relationship between each MDP decision in a certain time period, we applied the hierarchical coding method and validated the effectiveness of this method. This paper proposed a Hierarchical Transformer MADDPG based on recurrent neural network(RNN) which we call it Hierarchical RNNs-Based Transformers MADDPG(HRTMADDPG). It consists of a lower level encoder based on RNNs that encodes multiple step sizes in each time sequence, and it also consists of an upper sequence level encoder based on Transformer for learning the correlations between multiple sequences. Then we can capture the causal relationship between sub-time sequences and make HRTMADDPG more efficient.",
         "2021",
         "2021-05-11",
         "10.3233/jifs-212795",
         "Journal of Intelligent & Fuzzy Systems",
         "journal",
         "http://content.iospress.com/journals/journal-of-intelligent-and-fuzzy-systems/",
         "['Computer Science']",
         "['Computer Science']",
         "6",
         "0",
         "49",
         "['5685abf9e7bb2c16449ae1eb181051e503602a55', '80bb30e65c36545f7dcaae8fa9f1e66e550d4731', '566f12bc938fab439ca3a1f6162f54be02abddf2', '131ee42e4839d153333e17f46facdb6806e98c73', 'e151c8d6beccba19f0814b3ce1f5e6fab870fbc2', 'a4069dd677b205fba61b4dea75e26c148dee99c5', '962dc29fdc3fbdc5930a10aba114050b82fe5a3e', 'e8984c6e6c24aab26c332728a5fff616dfb3adbb', '129983331ca874142a3e8eb2d93d820bdf1f9aca', 'ce6a1a1fb3fd09b72d5a8ccddbd07c29d67be1c4', 'a0480ccac8090bac02357d1a824fce06aada1e32', '46b3ba0f3cb8340bc94f26e0fdf6dc4e38f68948', '59a916cdc943f0282908e6f3fa0360f4c5fb78d0', 'abfd24722ccbf4bb3a15029202f4d3193187a4e6', '7cc730da554003dda77796d2cb4f06da5dfd5592', '6dd9466b1fe3482843f73f0957541b073fb9597c', '1dee2b674aaa8f3695d8dbf46dcea08f4bb84929', '28e66d188efbd0bbb64242b611d96769be910c15', '929bef0066bad871ba971b673c053112d055d29f', '72c85ffe4491abd6f9e537d7dadf8f23c314849e', '5152277235d03179a277bc26ef573d8744d6de5b', '642c1b4a9da95ea4239708afc5929a5007a1870d', '38fb1902c6a2ab4f767d4532b28a92473ea737aa', 'dce6f9d4017b1785979e7520fd0834ef8cf02f4b', '204e3073870fae3d05bcbc2f6a8e263d9b72e776', '7c3ece1ba41c415d7e81cfa5ca33a8de66efd434', '5d2f5c2dc11c18c0d45203e2b980fe375a56d774', '97fb4e3d45bb098e27e0071448b6152217bd35a5', '69e76e16740ed69f4dc55361a3d319ac2f1293dd', '846aedd869a00c09b40f1f1f35673cb22bc87490', 'bcfe915d5983dffbfe95801e9e6757205b3a4723', '3b9732bb07dc99bde5e1f9f75251c6ea5039373e', '024006d4c2a89f7acacc6e4438d156525b60a98f', '449532187c94af3dd3aa55e16d2c50f7854d2199', 'fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5', '8a756d4d25511d92a45d0f4545fa819de993851d', '687d0e59d5c35f022ce4638b3e3a6142068efc94', '2319a491378867c7049b3da055c5df60e1671158', '6471fd1cbc081fb3b7b5b14d6ab9eaaba02b5c17', 'a97b5db17acc731ef67321832dbbaf5766153135', 'e9fac1091d9a1646314b1b91e58f40dae3a750cd', '2e9d221c206e9503ceb452302d68d10e293f2a10', 'e93f1fb38372e3e48e915971c8cf889f8b95d7ef', 'df2b0e26d0599ce3e70df8a9da02e51594e0e992', None, None, 'ac4af1df88e178386d782705acc159eaa0c3904a', None, None]",
         "['50652938', '49576045', '3213583', '144380740', '9168351', '2307761', '2091443272']",
         "['Xiaolong Wei', 'Lifang Yang', 'Xianglin Huang', 'Gang Cao', 'Zhulin Tao', 'Zhengyang Du', 'Jing An']",
         "https://www.semanticscholar.org/paper/07a6be4101e2a57b0e9b72190560e63af01a267e",
         "http://arxiv.org/pdf/2105.04888",
         "Causal Reinforcement Learning | Causal RL",
         "False",
         "False",
         "False",
         "False",
         null
        ],
        [
         "15",
         "091fcdc8808dee10d6b8a26bb6be25ec49abc470",
         "ATMS: Algorithmic Trading-Guided Market Simulation",
         "Algorithmic Trading-guided Market Simulation (ATMS) is introduced by optimizing the effectiveness of the proposed metric and it is shown that ATMS generates market data with improved similarity to reality compared to the state-of-the-art conditional Wasserstein Generative Adversarial Network approach.",
         "2023",
         "2023-09-04",
         "10.48550/arxiv.2309.01784",
         "arXiv.org",
         null,
         "https://arxiv.org",
         "['Computer Science', 'Economics', 'Mathematics']",
         "['Computer Science', 'Economics', 'Mathematics']",
         "1",
         "0",
         "64",
         "[]",
         "['2238127668', '2237802902', '2128057', '50412755']",
         "['Song Wei', 'Andrea Coletta', 'Svitlana Vyetrenko', 'T. Balch']",
         "https://www.semanticscholar.org/paper/091fcdc8808dee10d6b8a26bb6be25ec49abc470",
         "https://arxiv.org/pdf/2309.01784",
         "Causal Reinforcement Learning | Causal RL",
         "False",
         "False",
         "False",
         "False",
         null
        ],
        [
         "16",
         "094b9ef2864abdaa32691548063f4f87e9aae73c",
         "Robust Inverse Reinforcement Learning under Transition Dynamics Mismatch",
         "We study the inverse reinforcement learning (IRL) problem under the \\emph{transition dynamics mismatch} between the expert and the learner. In particular, we consider the Maximum Causal Entropy (MCE) IRL learner model and provide an upper bound on the learner's performance degradation based on the $\\ell_1$-distance between the two transition dynamics of the expert and the learner. Then, by leveraging insights from the Robust RL literature, we propose a robust MCE IRL algorithm, which is a principled approach to help with this mismatch issue. Finally, we empirically demonstrate the stable performance of our algorithm compared to the standard MCE IRL algorithm under transition mismatches in finite MDP problems.",
         "2020",
         "2020-07-02",
         "10.48550/arxiv.2007.01174",
         "Neural Information Processing Systems",
         "conference",
         "http://neurips.cc/",
         "['Computer Science', 'Mathematics']",
         "['Computer Science', 'Mathematics']",
         "30",
         "0",
         "69",
         "['b1bac99777448be9844594d5f37063f6911bd1be', 'd2a67ff819aba9c9d615b0380aa092273396a404', '34601270eb6274c5331c634be82f701ef394eacf', '8562b0dd9ce862d4c587fee807d4711173a9ef9b', '129983331ca874142a3e8eb2d93d820bdf1f9aca', '1c1081922f849ced90745a5bb699ad0a93625be4', '222baa4e9e7ce691fdfddbc826a70e027daed70d', '6946ae0c23257586c12d01675e05167d74cb89fa', 'c9598772c8456dd6ff3f1911930ec536cf5e72cf', 'ceeb815de098c353b83fa04389b88f7963e052d9', '6e81e3b82c875063dca2aee551aa5d50cc3c69cc', '896e5529de1da1e4494033404721b70339bb9557', 'b3b3d1d6d36ac203cd06c00bb37e66c000430275', 'bcdb21ca1703fc6f62df420626e36d138480a6a1', 'e0c117d65ab42850cf6448fd8e4de9305665b46e', '1f1f8330cddf1f4bf7bd73478223e5c02b69a1ff', '3a523641cab99381db21ff30df4050f7e2f546b3', 'e58ef68b95d9b03f37991c31ef2363fab8d0a5b4', '8d6adaa16ed0af9935a1130a305c85e8bdf8780d', '8f2f2d8cba5bf44bb0c10fd53adf25228655e8ae', '5e2c4e7b3302549b3718601c44d9af6c7554efef', 'c27db32efa8137cbf654902f8f728f338e55cd1c', '0af8cdb71ce9e5bf37ad2a11f05af293cfe62172', 'dce6f9d4017b1785979e7520fd0834ef8cf02f4b', '9c4082bfbd46b781e70657f14895306c57c842e3', '72d8af32991a69d0fa3ff2d8f80c8553cca7c0cd', 'b99c1205778a7e19915a8a739fa763aa476f2185', '4ab53de69372ec2cd2d90c126b6a100165dc8ed1', '2ad9a450d07e23b91da660beca0e63365f50012a', '024006d4c2a89f7acacc6e4438d156525b60a98f', '9ba266a4a4644e877fc37a64be3beddce8904cf7', 'b6b8a1b80891c96c28cc6340267b58186157e536', '340f48901f72278f6bf78a04ee5b01df208cc508', '449532187c94af3dd3aa55e16d2c50f7854d2199', 'a6cb366736791bcccc5c8639de5a8f9636bf87e8', 'beb9856632007d8fd0babe64ca180e182d02591b', '687d0e59d5c35f022ce4638b3e3a6142068efc94', 'b7782af5fb454ed719f506462d38c7dc80e844a5', '9eae0c6ca4a52fc5e6b6f9eb111ab6fdbecdf9a6', '1e045f3447f69d9a7cac18ef23062ea8dd661285', '7bff4e9b01e9a79703ca1f3ec60cf000cd802501', 'ad32f87eb152fed47fdb0252c180b3c60c9aef54', 'c8221c054459e37edbf313668523d667fe5c1536', '6553b04761e1030b95755a83337627535a372c18', '117a50fbdfd473e43e550c6103733e6cb4aecb4c', '6db16608fccddef51202af84112b34cfebfbe20a', 'ab4a1c4dfe23b3a1e3d077df467452cc68f64de8', '427ce4ce5f64295f39c8714de6ff09fb3bb7d187', 'f65020fc3b1692d7989e099d6b6e698be5a50a93', 'dc649486b881e672eea6546da48c46e1f98daf32', 'a20f0ce0616def7cc9a87446c228906cd5da093b', '7fbf55baccbc5fdc7ded1ba18330605909aef5e5', '8ef87e938b53c7f3ffdf47dfc317aa9b82848535', '3c81be86261a0e4be2c500dee8641e7fb0499dc6', '2a65434d43ffa6554eaf14b728780919ad4f33eb', '0b0cc2c941c10ff180e036c068d51c0d6cc1676c', '25e5af0da0a10f4806cebabc83522114d66b48a7', 'b05b67aca720d0bc39bc9afad02a19f522c7a1bc', None, 'd5a6663ce5f0d444c2c8bc06cdb543853b80929c', None, None, None, None, None, None, None, None, None]",
         "['1785336654', '2157104597', '2197201', '2325729167', '1678641']",
         "['Luca Viano', 'Yu-ting Huang', 'Parameswaran Kamalaruban', 'Adrian Weller', 'V. Cevher']",
         "https://www.semanticscholar.org/paper/094b9ef2864abdaa32691548063f4f87e9aae73c",
         null,
         "Causal Reinforcement Learning | Causal RL",
         "True",
         "False",
         "False",
         "False",
         "https://openalex.org/W4287728432"
        ],
        [
         "17",
         "094dcb5cd5d4af0288e254d4a691ab951a0bf5f9",
         "Deep Distributional Learning with Non-crossing Quantile Network",
         "In this paper, we introduce a non-crossing quantile (NQ) network for conditional distribution learning. By leveraging non-negative activation functions, the NQ network ensures that the learned distributions remain monotonic, effectively addressing the issue of quantile crossing. Furthermore, the NQ network-based deep distributional learning framework is highly adaptable, applicable to a wide range of applications, from classical non-parametric quantile regression to more advanced tasks such as causal effect estimation and distributional reinforcement learning (RL). We also develop a comprehensive theoretical foundation for the deep NQ estimator and its application to distributional RL, providing an in-depth analysis that demonstrates its effectiveness across these domains. Our experimental results further highlight the robustness and versatility of the NQ network.",
         "2025",
         "2025-04-11",
         "10.48550/arxiv.2504.08215",
         "arXiv.org",
         null,
         "https://arxiv.org",
         "['Mathematics', 'Computer Science']",
         "['Mathematics', 'Computer Science']",
         "0",
         "0",
         "0",
         "[]",
         "['2355080446', '2308120361', '2227582405', '1999430015', '2265521745', '2353133938']",
         "['Guohao Shen', 'Runpeng Dai', 'Guojun Wu', 'S. Luo', 'Chengchun Shi', 'Hongtu Zhu']",
         "https://www.semanticscholar.org/paper/094dcb5cd5d4af0288e254d4a691ab951a0bf5f9",
         null,
         "Causal Reinforcement Learning | Causal RL",
         "False",
         "False",
         "False",
         "False",
         null
        ],
        [
         "18",
         "0953daae619a0017683dfd66b571bd57f3a6e8f5",
         "Predicting Liquidity-Aware Bond Yields using Causal GANs and Deep Reinforcement Learning with LLM Evaluation",
         "Financial bond yield forecasting is challenging due to data scarcity, nonlinear macroeconomic dependencies, and evolving market conditions. In this paper, we propose a novel framework that leverages Causal Generative Adversarial Networks (CausalGANs) and Soft Actor-Critic (SAC) reinforcement learning (RL) to generate high-fidelity synthetic bond yield data for four major bond categories (AAA, BAA, US10Y, Junk). By incorporating 12 key macroeconomic variables, we ensure statistical fidelity by preserving essential market properties. To transform this market dependent synthetic data into actionable insights, we employ a finetuned Large Language Model (LLM) Qwen2.5-7B that generates trading signals (BUY/HOLD/SELL), risk assessments, and volatility projections. We use automated, human and LLM evaluations, all of which demonstrate that our framework improves forecasting performance over existing methods, with statistical validation via predictive accuracy, MAE evaluation(0.103%), profit/loss evaluation (60% profit rate), LLM evaluation (3.37/5) and expert assessments scoring 4.67 out of 5. The reinforcement learning-enhanced synthetic data generation achieves the least Mean Absolute Error of 0.103, demonstrating its effectiveness in replicating real-world bond market dynamics. We not only enhance data-driven trading strategies but also provides a scalable, high-fidelity synthetic financial data pipeline for risk&volatility management and investment decision-making. This work establishes a bridge between synthetic data generation, LLM driven financial forecasting, and language model evaluation, contributing to AI-driven financial decision-making.",
         "2025",
         "2025-02-24",
         "10.48550/arxiv.2502.17011",
         "arXiv.org",
         null,
         "https://arxiv.org",
         "['Computer Science', 'Economics']",
         "['Computer Science', 'Economics']",
         "1",
         "0",
         "33",
         "[]",
         "['2208667280', '2331321508', '2293614174', '2346971549']",
         "['J. Walia', 'Aarush Sinha', 'Srinitish Srinivasan', 'Srihari Unnikrishnan']",
         "https://www.semanticscholar.org/paper/0953daae619a0017683dfd66b571bd57f3a6e8f5",
         null,
         "Causal Reinforcement Learning | Causal RL",
         "False",
         "False",
         "False",
         "False",
         null
        ],
        [
         "19",
         "098f070dbd6f659c95fa863451884050981760a8",
         "Assessing the Impact of Distribution Shift on Reinforcement Learning Performance",
         "Research in machine learning is making progress in fixing its own reproducibility crisis. Reinforcement learning (RL), in particular, faces its own set of unique challenges. Comparison of point estimates, and plots that show successful convergence to the optimal policy during training, may obfuscate overfitting or dependence on the experimental setup. Although researchers in RL have proposed reliability metrics that account for uncertainty to better understand each algorithm's strengths and weaknesses, the recommendations of past work do not assume the presence of out-of-distribution observations. We propose a set of evaluation methods that measure the robustness of RL algorithms under distribution shifts. The tools presented here argue for the need to account for performance over time while the agent is acting in its environment. In particular, we recommend time series analysis as a method of observational RL evaluation. We also show that the unique properties of RL and simulated dynamic environments allow us to make stronger assumptions to justify the measurement of causal impact in our evaluations. We then apply these tools to single-agent and multi-agent environments to show the impact of introducing distribution shifts during test time. We present this methodology as a first step toward rigorous RL evaluation in the presence of distribution shifts.",
         "2024",
         "2024-02-05",
         "10.48550/arxiv.2402.03590",
         "arXiv.org",
         null,
         "https://arxiv.org",
         "['Computer Science']",
         "['Computer Science']",
         "6",
         "0",
         "73",
         "[]",
         "['2142563846', '2282961533', '2180993273', '143669775']",
         "['Ted Fujimoto', 'Joshua Suetterlein', 'Samrat Chatterjee', 'A. Ganguly']",
         "https://www.semanticscholar.org/paper/098f070dbd6f659c95fa863451884050981760a8",
         null,
         "Causal Reinforcement Learning | Causal RL",
         "False",
         "False",
         "False",
         "False",
         null
        ],
        [
         "20",
         "09927bf6b2b547e5fcc001a16c4be6d3b1cb7c1e",
         "Offline Reinforcement Learning with Causal Structured World Models",
         "Model-based methods have recently shown promising for offline reinforcement learning (RL), aiming to learn good policies from historical data without interacting with the environment. Previous model-based offline RL methods learn fully connected nets as world-models that map the states and actions to the next-step states. However, it is sensible that a world-model should adhere to the underlying causal effect such that it will support learning an effective policy generalizing well in unseen states. In this paper, We first provide theoretical results that causal world-models can outperform plain world-models for offline RL by incorporating the causal structure into the generalization error bound. We then propose a practical algorithm, oFfline mOdel-based reinforcement learning with CaUsal Structure (FOCUS), to illustrate the feasibility of learning and leveraging causal structure in offline RL. Experimental results on two benchmarks show that FOCUS reconstructs the underlying causal structure accurately and robustly. Consequently, it performs better than the plain model-based offline RL algorithms and other causal model-based RL algorithms.",
         "2022",
         "2022-06-03",
         "10.48550/arxiv.2206.01474",
         "arXiv.org",
         null,
         "https://arxiv.org",
         "['Computer Science', 'Mathematics']",
         "['Computer Science', 'Mathematics']",
         "19",
         "3",
         "28",
         "['095e79db633d8b8f6031591a9c50352719207042', 'e2bc67e9a7bf30ba447b9cff051df11027815cfc', '1632f51c8e573264730061dc5c9e7db821535bc4', 'bdc9b5603304c806eee092bb06445f8e8cef39ad', '83551ac1e6358182a2c0f2ec223fb3c6f736c8e1', 'dea0f1c5949f8d898b9b6ff68226a781558e413c', '309c2c5ee60e725244da09180f913cd8d4b8d4e9', '320b227027030fc291de2896fc3c6da49d7614be', '207033829813aadc2f2dca8f93279352d39de759', 'c441a7e9b1c7ecf54f583cd61e896faa10b60358', '492ba3ad3f0cb85f0636bc275fecd7e7960709da', '9aa3ae820772b5f25df9d498bd60c72e70d1b5e6', '41cca0b0a27ba363ca56e7033569aeb1922b0ac9', 'b4a4edebbb25211333098a54c060847e54c1a991', '4f0b8f730273e9f11b2bfad2415485414b96299f', '27dfecb6bb0308c7484e13dcaefd5eeebba677d3', '869f4fc59d74a96098ed46935cb6fd1a537c38ce', '5f0625c30014c12f333eb518268647673d18f9f1', 'b354ee518bfc1ac0d8ac447eece9edb69e92eae1', '0a5ee656e9bf9e18fdee1655f545d5c62527fc33', '36eb6fea39ce06e2807f074fa3d5e79ed0f2bcef', '59746b6c7a3060d0c9de12076f5ffddcfd9502b8', 'f3595e1691041c1f95fb319f56a0817d04f013ee', '1c6356a688a273889fb71bab17973ed10483d97f', '1473110f6c33b483251ade10b79416d3efee2da4', 'bff20fb30adad8d1c173963089df5fc9664304f0', 'eb2182720a17bcd5bf664cadd2cb1bef6f23f701', None]",
         "['1999326708', '2108968873', '2153440065', '2119017332', '2152845240']",
         "['Zhengbang Zhu', 'Xiong-Hui Chen', 'Hong Tian', 'Kun Zhang', 'Yang Yu']",
         "https://www.semanticscholar.org/paper/09927bf6b2b547e5fcc001a16c4be6d3b1cb7c1e",
         "https://arxiv.org/pdf/2206.01474",
         "Causal Reinforcement Learning | Causal RL",
         "False",
         "False",
         "False",
         "False",
         null
        ],
        [
         "21",
         "0b26cb46675c0b00483c254488c9a14d89eec8e5",
         "DeepTrader: A Deep Reinforcement Learning Approach for Risk-Return Balanced Portfolio Management with Market Conditions Embedding",
         "Most existing reinforcement learning (RL)-based portfolio management models do not take into account the market conditions, which limits their performance in risk-return balancing. In this paper, we propose DeepTrader, a deep RL method to optimize the investment policy. In particular, to tackle the risk-return balancing problem, our model embeds macro market conditions as an indicator to dynamically adjust the proportion between long and short funds, to lower the risk of market fluctuations, with the negative maximum drawdown as the reward function. Additionally, the model involves a unit to evaluate individual assets, which learns dynamic patterns from historical data with the price rising rate as the reward function. Both temporal and spatial dependencies between assets are captured hierarchically by a specific type of graph structure. Particularly, we find that the estimated causal structure best captures the interrelationships between assets, compared to industry classification and correlation. The two units are complementary and integrated to generate a suitable portfolio which fits the market trend well and strikes a balance between return and risk effectively. Experiments on three well-known stock indexes demonstrate the superiority of DeepTrader in terms of risk-gain criteria.",
         "2021",
         "2021-05-18",
         "10.1609/aaai.v35i1.16144",
         "AAAI Conference on Artificial Intelligence",
         "conference",
         "http://www.aaai.org/",
         "['Computer Science']",
         "['Computer Science']",
         "107",
         "8",
         "27",
         "['3a9d030daf272945ce87abf6afdb63068f2ec26d', '61bee52afa721d13982289497f3408e54444f85b', 'cf039163bc505e615ff77f12454b26c48195898a', 'fae129338c0899576524506008427f64477d3967', 'cdc04b748e440f43547c0516f77480ffb8bf5cda', '65a2534e3bd229eb368bbaad32f92a68bd0e936a', '9ad9ec5012dcbc328d76d64d72eb198d383addb8', 'b3f9a777cf1a00a4601264a6451f0c6876a4d0f6', '204e3073870fae3d05bcbc2f6a8e263d9b72e776', 'f96478d0694f18384934fc19a2655170f32e2d8c', '2c03df8b48bf3fa39054345bafabfeff15bfd11d', '7f5fc84819c0cf94b771fe15141f65b123f7b8ec', '797398408f967a6e6bc4570db7df7daeccd6fa61', '669e3d70ddd5cd3b780e4d347df97d4d7485b393', 'de5e03b5f95a6ac2fae1778d3df65abe95708c39', '1c6356a688a273889fb71bab17973ed10483d97f', '7203fc2f7d6c231bf7cde97b5c75787df2cfd0f4', 'b6763008a5136a12f444c33a497ab4525c1e81f2', '170cc2f8373322ab91036bbb66fc52b5c5c37e83', '2e9d221c206e9503ceb452302d68d10e293f2a10', 'e26d8c3b76afac1bfc668f678b9b9ca43e1eaa31', '827ac797cfd5d30827aaad5c5416ff393d160a27', '71e727e5526f543d1cae7f8aa10d5a0e30fde1b1', 'c339a7f5dd4d822feedf4b4b8098645cb441db5e', '90ad8fed17daebaee0a743f9f9847e022739a534', '3005f14510e08f1a0ec8d22336c766f5acf0d1d6', 'feabab1b28a9bfb3d517bb12ae08f53d02b68cbf']",
         "['2108248149', '1938684', '1701972', '2119016656', '145652961']",
         "['Zhicheng Wang', 'Biwei Huang', 'Shikui Tu', 'Kun Zhang', 'L. Xu']",
         "https://www.semanticscholar.org/paper/0b26cb46675c0b00483c254488c9a14d89eec8e5",
         "https://ojs.aaai.org/index.php/AAAI/article/download/16144/15951",
         "Causal Reinforcement Learning | Causal RL",
         "False",
         "False",
         "False",
         "False",
         null
        ],
        [
         "22",
         "0b3e119248286aeedc95330ab7b67999f522d574",
         "Explainable reinforcement learning for broad-XAI: a conceptual framework and survey",
         "Broad-XAI moves away from interpreting individual decisions based on a single datum and aims to provide integrated explanations from multiple machine learning algorithms into a coherent explanation of an agent’s behaviour that is aligned to the communication needs of the explainee. Reinforcement Learning (RL) methods, we propose, provide a potential backbone for the cognitive model required for the development of Broad-XAI. RL represents a suite of approaches that have had increasing success in solving a range of sequential decision-making problems. However, these algorithms operate as black-box problem solvers, where they obfuscate their decision-making policy through a complex array of values and functions. EXplainable RL (XRL) aims to develop techniques to extract concepts from the agent’s: perception of the environment; intrinsic/extrinsic motivations/beliefs; Q-values, goals and objectives. This paper aims to introduce the Causal XRL Framework (CXF), that unifies the current XRL research and uses RL as a backbone to the development of Broad-XAI. CXF is designed to incorporate many standard RL extensions and integrated with external ontologies and communication facilities so that the agent can answer questions that explain outcomes its decisions. This paper aims to: establish XRL as a distinct branch of XAI; introduce a conceptual framework for XRL; review existing approaches explaining agent behaviour; and identify opportunities for future research. Finally, this paper discusses how additional information can be extracted and ultimately integrated into models of communication, facilitating the development of Broad-XAI.",
         "2021",
         "2021-08-20",
         "10.1007/s00521-023-08423-1",
         "Neural computing & applications (Print)",
         null,
         null,
         "['Computer Science']",
         "['Computer Science']",
         "67",
         "6",
         "278",
         "['6ddbf6148defa91ec0475916aa74baa1bccc7078', 'e1d2f2a717aa03280126f87c8e5fad695f52bf7c', 'a0f4a43911d47da8cf51c606e49149e6844de6e7', '1c705125d3336a189eca0f20ef69edc8420a9f3d', '03b7abe67442f384ed837c4ff08a1527ef051aa8', '5872cf9accfbdf40f8d5066c76cf3977a9487f6d', '2c3c45b356b45189bb210da53b3b3cc20f8c7483', '3e8fc7f9bc7ff938e3b8f557990cc1986b11a18e', 'b15c250f1e74a4627f3a79b589de698293bed32b', '443d21148f28d56b8d52a1fbb49971956efed999', '5aa1045136814bbff763c9897d116ac4177d1f94', '47dc11152d8b90e794a32cc0d55e6115d9290c46', '00b8b56d92ef34483fba2e734083deb9a7367320', 'c0c6d4e8d321de5a418ac6b48b1f1166dbc919d8', 'd835e503c3b46b7967629d8f24887f4a14ecf225', '86136c83e9ff3ffefddfa3e026515c3a31043f9c', 'c782d6c30ab1d678d1ad053fad23ed72e0f4a9c4', '2e0b1b6ac7a48e1c992cb26967c7272c5e798757', 'ea5988da726cb50d2376584b04d668008aad0a3f', 'e5ea03c1eba7273ff8649d646d182926324ebfc6', '9f325e77850899ad69905e23e4febb2a0aa9b5df', '785964444715fca13abca787cc98a4f058d02a86', 'e654e71b65de3c8b549a553c526f3779042f1e2e', 'e1b77eb03d753da13d1c6ce7cb45dd98e64c7fb6', '726c253b524f7238c93f8f71aaabd2fcea8dd790', '694b71bab2d4a9a5fae87644fa5552f949eb65b1', '634d65bba4efc94b17dcc4a60fd0bb2de29ac2c4', 'e5f23114655227ed84e417d54dd74a3906e7684a', '93485ad218129dd4694ab0a3a6b44e9a1b87d93c', 'd1f9a55d0c03d13513048513b39f48dd77deb619', '502cce7207a00c4520e130116b9ff8c1a0ea559b', '886b604f74d65bb420bc1311e9647c44fe8fa91e', 'fe50822a700c7104f8d7e6fcb797103ee5f903dd', '1efe9c3efd044e0d3d5ee7e7cab970273d2b6cfa', 'fbc5486a1ffb9039dbb5046b84f0eb32e4ce8eea', '7085c77c3c2f76e5eb5c09c38501818c55c99411', '8c22123fd77be05362fcd041385274e6afc8d888', '1933f4654acc42a4433e1c5bf67dddf3815cc234', '5eb441a6fd7ca12b70fd5005a476c8f3d08822ab', '00629b91468ba555384d91065e16fb3dcd6e3831', 'b0c34618ffd1154f35863e2ce7250ac6b6f2c424', '0d4f238ef520cafe6eafef9ee214e36bce3447dc', '2908a3d2d7efbf24638109ed7abd954165bfc0ce', '87a3676647d1bd553ceb08cf83439b1833f1036f', 'b0474588c6ddf2d6a0bb638dd0e60881c3d7b54d', '72be39943d06037a11c804c36fce652494f6404c', 'fce215e1529caf6eb0b91ccc9a502cf6430f75ae', 'c5da1ee96285050bcad31ee85b95ca61afd11795', '0b02dd64487f5c5d6231da8f52c9a5a7d8814569', 'b2869ced3341ca1505200252eee4fe195a103048', 'cf98ca55304d2e199920f0044f6cb6c6bdc93918', '07194ea6eee36737894b633715c32f345dc6b1a3', 'cfb68baa23048e3e0f8845c099fa013797bd623f', '6ca59c07829718c6fa65465b3eb6ed27544ffcf7', 'b89594e658b233f9c057f0f79e68738d0c107458', 'adb85b89e5b562007b365bc4bf7d2c1ea146f3e2', '44fb1cdeb42b84a764494db79aa816d2a55a6261', '87858adfe16be9b1ee69760384e2d561e614594a', '22e43e30bd23d08b549cb05ed2d6e15c5149928c', '70dbe3e740a5e7927ccce00fd615365b08a6eaae', 'adc4ddeb10dd8da115d4bde9569794ff409fcd40', 'b17e0b5b5c7e632ecf5f35b0616cbf820382fcf7', '8d044613d4e874c7ff3807a0fcf231db3d50f825', '433cefd40db8b4d365ca2cde2bde7cb4d10999db', 'ef7df5eae54107c013885231eb7af4431f2e6158', 'b7f809e39946bbb42ff8380865881631cadbce24', '6bc692616db7b1a7ef2ea7c270c893adfb57ed0e', 'cbdb103a298c20644553c201c9dfb0399b552b98', 'ca084f6ba992157bf86945cb75772ee5324de5b2', '6202286f75cc93ee04e2b975b3f3bc7a97dcd3a2', '7e27d44e3fac723ccb703e0a83b22711bd42efe8', '6001948ea46812c4f1c0a050957377853d4fb44b', '21dff47a4142445f83016da0819ffe6dd2947f66', 'a7e07e0ecd1727778ade42d2e1df856171ec0898', 'b618b5f212d8b93e8bb866a1e1ce18079946723b', 'f1173ca43481c1b33f4e7891ce77200e51eecba2', '76931c3ea4b010ee2253357b85eff21c4e059f58', '1bd32735fa93c37577ce25c45ba862ef50d5c3d6', 'f611acc0ee253f5903dafe30bbcd65f823374448', '50a808f74f392b77b241c4c9a4b1fb6e6bbea17c', '800f145955d696126edbae7403a4fb7cafd2bbb2', '585b34dbc5a3a456d6b7c6071eaee75ab64b8acb', '14c86681aa1d24feffe2c8735efd5c83d595b99b', 'ea7887fadc666d6faf92e569d4a10d994ee91297', '9d671a4de50b98c3f00623ee597e37c9f00ba0cc', 'd7701e78e0bfc92b03a89582e80cfb751ac03f26', '6d5884782ed492891015d0eb7dc729729abb472c', '2a0e4eeea84f6352a6ac012f10581e5a228b6a92', 'c43bba87b4237a93d96b2a3e91da25d91fd0bb91', 'bdba2d26e60a60a11738718e591e0bc5e6d9085e', '49c076bbc21ab76720b610ab3840c15ce3dc4e6c', 'ce1c28ca2f52a42c6e60d792cd71ba894abc47d5', '748dc54a89a276639f9d571cdd8e7d1ae3f9a57a', '4c281ab9e305c8424d539782705f99125112fb34', 'ca781fb293c2521d0737899d252d47a97eca0d58', '3f721241c97cee4670cb4e3e01eabfaccc7ed419', '0403275945c0f6d96fb22f69447b70c8967403f1', 'b98187f1f43b095f06ae44f341fd7627afbbd64c', 'e362d2909f389ca4929239baa651fd42b5ad7a7e', 'b4e669bae43216cb0e77836a411b88dea4bb6034', '627fc90f9bc87faef48f817136a8b3699a146fc4', '1a0e50ef0a31e31ce8197708ff282e0e0b37d868', '906f27ffbe1508f3b3d33dee3dab2d23b69d9505', '5b1c86ee83df029405f8e270d2a257679ab879fe', 'b89c46948c413e3b20d37b8e8a498c22b2ff8586', '6127b8dc39497a2388a0fce2512595e0dcb7121b', 'cf90552b5d2e992e93ab838fd615e1c36618e31c', '41578296efae9cdb1a54c7ef642465a3f91d8724', '0754707f042652e257eb7bf35e21c8ca16b24037', 'e89dfa306723e8ef031765e9c44e5f6f94fd8fda', '78a0e406b4b03d60af44dc529686a15f30b1da37', '471f9742b4e32d8ee68f9ee493768ff0466a231d', 'd15c2ce2c20c29d1a350caf6f328afc094440511', 'fe2ef22089712fcff33a77761860a10b7834da47', 'ef731bc5b85867e97fbc01621282159c8a1f6ceb', '5c39e37022661f81f79e481240ed9b175dec6513', '3e6eaa94130adac7d12d7188109e2a9bb388f797', '3576c0d521ddad0ceacc43dc19300b1facefe2b4', '9f1e9e56d80146766bc2316efbc54d8b770a23df', 'ab2e9398954142727aa78ac68517159906e3fffd', '28fc114c96755e9f9c3fc19415286c16a1da6153', 'd35b05f440b5ba00d9429139edef7182bf9f7ce7', '15b26d8cb35d7e795c8832fe08794224ee1e9f84', 'cc5afe344cc7ed7acd68a28b9774ea8023a162dc', '2f40843c695a75917c766e3a0b9efd183f4cf890', '910be0354a017e058ae9a6d527ac71e08247d6d2', '1464776f20e2bccb6182f183b5ff2e15b0ae5e56', 'd37620e6f8fe678a43e12930743281cd8cca6a66', 'c0883f5930a232a9c1ad601c978caede29155979', '7200969d70cf6f3fd343f48e97b8ebf7d563a584', '846aedd869a00c09b40f1f1f35673cb22bc87490', '03f6b91dfd653b7787a3f15fae2082efdf02e90f', '409d7702e0b8a0606940dbdcaa3ec8a636776a94', '4c05d7caa357148f0bbd61720bdd35f0bc05eb81', 'c6170fa90d3b2efede5a2e1660cb23e1c824f2ca', 'f96294b7e0cb0931b6dd9cd52470a4c9096759e5', '6121b3d97a38109c85941eeb6a93722e584e7519', '12806c298e01083a79db77927530367d85939907', 'b6b8a1b80891c96c28cc6340267b58186157e536', '68fbd17dde0e0f550567b3421b90f0f6a2cc3cee', '9fdd7881c52650db939a30c2b287936b0092732c', '340f48901f72278f6bf78a04ee5b01df208cc508', '3986189136ffc764af8fd2e56cba9cad8dfab705', 'f60d179ec6930354fef3913973a2ed7750434b77', 'eb42cf88027de515750f230b23b1a057dc782108', '7207dc8e9b4c54cce8847245a8148d74deff4930', '485d79f5d42349d5186cfdd59f1bff275144ca0d', '28b0563b1b844f656198fb826bd69c1500e6c130', '3a0202bc8587be03557d0751c0273bd8414be493', 'a82795a372cec5266e7247054e9aace8f3b6a4f3', '37391358d2da421c960012eafae80b4aa13e6f68', '2b8e962d2893a099c1ff7073e7ba7bb0d6e7df20', 'd1d828ff8b9ed26e4e64c8ae89ac2b98d683a576', '23a165c12b138cdaa726ef375b0930c8b56d4821', '05f8587b24bb34455654027eee7b42d9ba115850', 'b1591a35d9ad650e4a4926781b4bff73afa40d63', 'c853e313e597407ebc8a7c96b9c6f18dca37d1c8', '0180c56bfbfb21243f8605e4c6f6aab2779d3ef0', 'c5ffd0bdbecbeee18232676d2e272ba24b025797', '9888852479cbc560c768d46588fd4831be9eb8a6', '95c0f06e35045f4aa3be98bb8f567d61c304e3a7', '3f42b2d0c4d81e5be16ec95821014765faf0d601', '3c564bf1a8b23e32bf055d6d5bcdb06a2a3c44cc', '5bd9d3c01510f6ca571791abcfce80808c44ec73', '12fd7c0417c1e4d845a921050d6ef14f33a5c53d', 'cc22d04377d75c35fd806620687143e7a120db5d', '3402002efd65b8002d03d1d455b196087abe7fc8', '293da8eb80ac868ff13b464d9bc3c89de340567b', 'e1925eb6104b94d487e51f5710f697a08eec2778', '69a60c793575c96a9a779d8e8d0707a1f31df215', '5ff2ed71cb3b7e3afc5ba76286d09b871aba3c11', '5476abcbc4bd51c389e6843c8f19b8702565d7bf', '96150f208f4dce8a8b27c20bb6c1d3864b9ddaca', 'd7c642e366acead6cdec1d6b3e8198c93b2d5630', '3c66a7ea21dda2f54c6fb161b8c53cd2a1b64c7d', '5695a62310a3acb574a81e4125b921ddf4d5a397', '451b505f5b628754674641648471aebb03081b42', '197c59752a569991550d6bbfb9490e2cbb5ded58', '427ce4ce5f64295f39c8714de6ff09fb3bb7d187', '12d6fde053e2c7174a76fe1bbdb97dd039a3b662', '6373ce3730a3eebee4c63f85b20af48f49d93620', '02954dd24ddd69650ad9db634ed945c27a892a76', '2dfc4e6847aef5e87a2c78eee4c6a58694c03b24', '46b1928aa6713c67988532ab28254ce4a15a6afc', 'fb7ee0e304f870227821df62c5e9213fb799eb80', '6bd12c325f47f20a2514fd68c38e2335c706b570', '3eaa71f15cb9785b14c859a0e647ae4f86f70ac0', 'dfb837449cd20570b2384b74024e852f97478af8', '0e7638dc16a5e5e9e46c91272bfb9c3dd242ef6d', '645af3039537e1ab2419a621b62cc17006a15729', '1925ea622329168887837d29646b96eb5163506d', '141884afb251a2ba7e6ea870aa60d7870f635f2f', '947f1fcbdf42e52df2e92a73e907e35d57509879', '161ffb54a3fdf0715b198bb57bd22f910242eb49', '82be6b18333e4ec411a3dda7e49bdf1029f037b2', '1554f29baf1c5c1fcba74ebb9115dc1d75c9c841', '5c8fe9a0412a078e30eb7e5eeb0068655b673e86', '7d196ad6907b12ebb24faa57219770917e506493', 'cebfdfc6d4b1c1927f4a9798450212266ca62098', 'c342b3c046472955f63c88400c89bc2a2e2964e6', 'd6fc2904b3247e855b65c041003cebef2dcfe281', 'ad9ac8938d230ef41cf2aa6a795743c8b1520200', '572175048f460d76de1780f715fed53178e259f8', '9986e3e74afb71165663072374456901dc63d3ae', '9685a12caecc824e10a8c8f725da1bb1428a7285', '1aca0588fb5cf8d512548b4b15d48f1c9de2396d', '53c6ffe8ad96a745dc4788ba18470e798a49fd6c', 'e2f6994f57e1a8b07de7d82bf0b751a52e1a887f', 'b94e626f046b04221922d20108d9c840e722f585', '7abed618e349427fde64d68cc98c6b6bb844abb4', None, None, 'd197b52f2a252ab92cc789120c7d51cbaed8f0b1', '05848f86b121d2534f9b089617bd23b92d23ecec', None, 'd47677337b1083d6bfa940748da0780b2c9faf7d', '85dd699528dae0783be770a8f9e0ca80c1f0ddff', '3cad6ca5efc5fd3ea4bedb86f8d98a9a0727a8aa', '224b00003cdaa9f698be41520a3164d67d47951f', '80744eac4cfe08c45d461a6347bb10de5ae4b7e5', None, None, None, None, None, None, None, 'a4b6c2c9d3d0b8eba180eaa6c6be73eb18aacc7b', '6bb8e5c50eb219ce6344523bf9ebf0afc8b53123', '45b08d91f2e3369d0641875078c5684b4adcc848', 'c68796f833a7151f0a63d1d1608dc902b4fdc9b6', None, None, 'b7c1bdf7d9c92f7d38aeb2ca106189bffc903f5c', 'dbd5d1374f98eb7c290d7d5a06d38e3a7ca38ef6', None, None, None, '435ae39e996ba784821346cf660cd420980b129a', None, 'c3907b4d06310780a0bdc8b218a6976b720c2ff3', 'a50703b48dbe57646d3188438914daa20482d095', '6b4f147cddaa0664a6722ca534d1028d6f915415', '6b0607ee363c5a8c4a3c5b95043624ab8113f12a', None, '552458194a30871af3d0793cfab4ca40f31661d1', '78c72d21b62656f71c294133f544a655546c6af6', 'd9e339dfd63eb30995b198ab89ce5aee0261290d', '06e5315f35713abe266c30a124d88efbdf643493', '1b4471121868c9774aba82d81cb3af75963aeedd', '043ea63a6c67da878d0bd1680129e20de608ec69', '184e4112139a052e65da76e5407d8ac076f1d227', '4e7085c3d87f5256a822ed17ba63e78dfddb6e9c', 'e80a978640d82bfab318d949eaae52c55329a1a5', '0a8149fb5aa8a5684e7d530c264451a5cb9250f5', 'a28c4a55514bf6a8ddd536e2aeaa4fc6a31018c8', '308181b6b83dab171a2f1f9cf0afb8562b6c982d', '5627b6deb8cd38f718ae47207f6c3b8ccf559399', 'bc72c120421fcb29fa830ff6535d6f56dd83792c', '94ba19382a5d5968af889c870a783fa09c94ed70', '75fa9bf9fd70b3560245739b48874fea479d8f21', 'd1be7fad578cfd2cef36a3c545a5e17bcdb3020f', 'ea899c8a3806a02a225061a35f802b00d90a0a20', '6df43f70f383007a946448122b75918e3a9d6682', '8c2c2523508baa15b4529e33e3f213829975c0c0', '7c73c2a6c5fbde46158f2b0d52141f1d352d50e1', '0d5b9e6cb44714d5ac3348112b010429dfb3b525', '8ba4429283b265c2e42139d39680524e6180d847', '6a07f4839c89d3c0a47890f5009daff4685f276b', 'f64f58740780eeb87eca43f3b3bd89a543312804', '9c794c43a3e196e94e05659f792c1b22fe8ea0a5', '07e616ff52ab1bb8bb153a7528d02fbd310aa1f9', '552dc4b225983c166e4a5c6e1250f399a9797fc2', '35cc84075511da3cf59ebd9c3e4ccec8030fb7e8', '6d3ddedce1550d1d288d3a6da1b21b65503a9a55', None, None, None]",
         "['3327913', '1990124', '152480455']",
         "['Richard Dazeley', 'P. Vamplew', 'Francisco Cruz']",
         "https://www.semanticscholar.org/paper/0b3e119248286aeedc95330ab7b67999f522d574",
         "https://link.springer.com/content/pdf/10.1007/s00521-023-08423-1.pdf",
         "Causal Reinforcement Learning | Causal RL",
         "False",
         "False",
         "False",
         "False",
         null
        ],
        [
         "23",
         "0b92a2c5312ed64160a2228927145cfd1e99dc8c",
         "U NIFYING C AUSAL I NFERENCE AND R EINFORCEMENT L EARNING USING H IGHER -O RDER C ATEGORY T HEORY ∗",
         "A uniﬁed formalism for structure discovery of causal models and predictive state representation models in reinforcement learning (RL) using higher-order category theory and the abstract problem of structure discovery in both settings in terms of adjoint functors is presented.",
         "2022",
         null,
         "nan",
         null,
         null,
         null,
         "[]",
         "[]",
         "0",
         "0",
         "43",
         "[]",
         "['1850503']",
         "['S. Mahadevan']",
         "https://www.semanticscholar.org/paper/0b92a2c5312ed64160a2228927145cfd1e99dc8c",
         null,
         "Causal Reinforcement Learning | Causal RL",
         "False",
         "False",
         "False",
         "False",
         null
        ],
        [
         "24",
         "0c23350a713a5529ef01213d7ce83b45b342e99a",
         "Quantifying First-Order Markov Violations in Noisy Reinforcement Learning: A Causal Discovery Approach",
         "Reinforcement learning (RL) methods frequently assume that each new observation completely reflects the environment's state, thereby guaranteeing Markovian (one-step) transitions. In practice, partial observability or sensor/actuator noise often invalidates this assumption. This paper proposes a systematic methodology for detecting such violations, combining a partial correlation-based causal discovery process (PCMCI) with a novel Markov Violation score (MVS). The MVS measures multi-step dependencies that emerge when noise or incomplete state information disrupts the Markov property. Classic control tasks (CartPole, Pendulum, Acrobot) serve as examples to illustrate how targeted noise and dimension omissions affect both RL performance and measured Markov consistency. Surprisingly, even substantial observation noise sometimes fails to induce strong multi-lag dependencies in certain domains (e.g., Acrobot). In contrast, dimension-dropping investigations show that excluding some state variables (e.g., angular velocities in CartPole and Pendulum) significantly reduces returns and increases MVS, while removing other dimensions has minimal impact. These findings emphasize the importance of locating and safeguarding the most causally essential dimensions in order to preserve effective single-step learning. By integrating partial correlation tests with RL performance outcomes, the proposed approach precisely identifies when and where the Markov assumption is violated. This framework offers a principled mechanism for developing robust policies, informing representation learning, and addressing partial observability in real-world RL scenarios. All code and experimental logs are accessible for reproducibility (https://github.com/ucsb/markovianess).",
         "2025",
         "2025-02-28",
         "10.48550/arxiv.2503.00206",
         "arXiv.org",
         null,
         "https://arxiv.org",
         "['Computer Science', 'Mathematics']",
         "['Computer Science', 'Mathematics']",
         "0",
         "0",
         "25",
         "[]",
         "['2348263944']",
         "['Naveen Mysore']",
         "https://www.semanticscholar.org/paper/0c23350a713a5529ef01213d7ce83b45b342e99a",
         null,
         "Causal Reinforcement Learning | Causal RL",
         "False",
         "False",
         "False",
         "False",
         null
        ],
        [
         "25",
         "0c240d3f2d46d34c652a081ece634afff133aba0",
         "Graph Decision Transformer",
         "Offline reinforcement learning (RL) is a challenging task, whose objective is to learn policies from static trajectory data without interacting with the environment. Recently, offline RL has been viewed as a sequence modeling problem, where an agent generates a sequence of subsequent actions based on a set of static transition experiences. However, existing approaches that use transformers to attend to all tokens naively can overlook the dependencies between different tokens and limit long-term dependency learning. In this paper, we propose the Graph Decision Transformer (GDT), a novel offline RL approach that models the input sequence into a causal graph to capture potential dependencies between fundamentally different concepts and facilitate temporal and causal relationship learning. GDT uses a graph transformer to process the graph inputs with relation-enhanced mechanisms, and an optional sequence transformer to handle fine-grained spatial information in visual tasks. Our experiments show that GDT matches or surpasses the performance of state-of-the-art offline RL methods on image-based Atari and OpenAI Gym.",
         "2023",
         "2023-03-07",
         "10.48550/arxiv.2303.03747",
         "arXiv.org",
         null,
         "https://arxiv.org",
         "['Computer Science']",
         "['Computer Science']",
         "19",
         "2",
         "54",
         "['fcf0cbae0d4986fb33b201f99426723a437d16e7', '0133647614f4ba58c01ddbffe3c870c7cd2f29cf', 'fbb15aa7303586d25dc73f84c23f9b5447b0c06b', '7b604cd12bfd735f16d2097357b3d6ca584d53a1', 'b7d27c5af2d314f6ec45b6d88984fb45220eb379', '6b72135bf31e78ccee78478228b635201326d217', 'eb92a453cf982126fa2125d4c8915352a52af54d', '387a17823d7c47c0bd3390a124708933032989e0', '3b03753bff6d934c6d403d1039ecaca0c98e9a69', 'b08acfab4f762a8c462c0f4b02f9b9beaa7ec6ff', 'c879b25308026d6538e52b27bcf4fd3cb60855f3', '9121f675388a96def8cd99c18112847c32ac7638', 'f864d4d2267abba15eb43db54f58286aef78292b', 'c1ad5f9b32d80f1c65d67894e5b8c2fdf0ae4500', '362cc80481b288874af0428107ab31e955dcf09f', '2cd605106b88c85d7d8b865b1ef0f8c8293debf1', 'b44bb1762640ed72091fd5f5fdc20719a6dc24af', '3cb8e96faba73efa027fa858e2a78cd1fc3c6e4d', '3cd1041ff41a4d27ee26358a3eaf4a09a30083c6', '28db20a81eec74a50204686c3cf796c42a020d2e', 'dea0f1c5949f8d898b9b6ff68226a781558e413c', '309c2c5ee60e725244da09180f913cd8d4b8d4e9', '5e7bc93622416f14e6948a500278bfbe58cd3890', 'a326d9f2d2d351001fece788165dbcbb524da2e4', '55999400a3eed52ea9dd2f4b9f1b71ccb5c51238', '0881655dcdf891f529ebe7ac18301e138a5e265b', '7b0871c783e721bfbf9b5d16e575130a07a672cd', '24e0cec8c71421fdb5d002a7776d2b17c5dc975b', '7a7a7847041e7b25febb1491d65d842a6c65927e', 'bb98bc96e02396d199fc899287d9b84393c86e79', 'ad14227e4f51276892ffc37aa43fd8750bb5eba8', '1586ac4b32ff18dd0a7ba8d449f0d905ddb11f47', '9be492858863c8c7c24be1ecb75724de5086bd8e', '0e142cf658f3fc673dbc53453999222e177e6540', '4012d4ab621f3f5f04b0f91849a60c6eaabe64b4', '82b4b03a4659d6e04bd7cbf51d6e08fde1348dbd', '12c0751b4f51ed833172a713b7e32390032ead93', '5285cb8faada5de8a92a47622950f6cfd476ac1d', '7f77058976e2fe75e98280371962c43d98c98321', '3a58efcc4558727cc5c131c44923635da4524f33', '249408527106d7595d45dd761dd53c83e5a02613', 'fe3e91e40a950c6b6601b8f0a641884774d949ae', 'c27db32efa8137cbf654902f8f728f338e55cd1c', '0ab3f7ecbdc5a33565a234215604a6ca9d155a33', 'dce6f9d4017b1785979e7520fd0834ef8cf02f4b', '204e3073870fae3d05bcbc2f6a8e263d9b72e776', '340f48901f72278f6bf78a04ee5b01df208cc508', 'f82e4ff4f003581330338aaae71f60316e58dd26', '282001869bd502c7917db8b32b75593addfbbc68', 'f65020fc3b1692d7989e099d6b6e698be5a50a93', '78020db7e3d968f6e6cc26d18e31e5b668ca7fee', 'e3d833eeae571fb9f269206634863c1f33cae6e3', '9d557d85c206ddbf1b9fb1ad0c848a64e0973360', 'f6ba746a91d5285897623223f15e41b63c9ee7b2']",
         "['2176837980', '2144035454', '2108037077', '2135519749']",
         "['Shengchao Hu', 'Li Shen', 'Ya Zhang', 'Dacheng Tao']",
         "https://www.semanticscholar.org/paper/0c240d3f2d46d34c652a081ece634afff133aba0",
         "http://arxiv.org/pdf/2303.03747",
         "Causal Reinforcement Learning | Causal RL",
         "False",
         "False",
         "False",
         "False",
         null
        ],
        [
         "26",
         "0c3d604ca8f16b2408e50e8d9a3cf400c8533347",
         "LIDS-P-2027 Parallel Smoothing Algorithms for Causal and Acausal Systemsi",
         "This paper describes parallel processing algorithms for optimal smoothing for discrete time linear systems described by two point boundary value difference equations and presents both a two filter implementation of this step as well as a highly parallel implementation exactly matched to the hypercube computer architecture.",
         "2006",
         null,
         "nan",
         null,
         null,
         null,
         "[]",
         "[]",
         "0",
         "0",
         "13",
         "[]",
         "['36887207', '2060098327', '2056041353', '1409222580']",
         "['Darrin Taylor', 'Alan', 'S.', 'Willsky']",
         "https://www.semanticscholar.org/paper/0c3d604ca8f16b2408e50e8d9a3cf400c8533347",
         null,
         "Causal Reinforcement Learning | Causal RL",
         "False",
         "False",
         "False",
         "False",
         null
        ],
        [
         "27",
         "0ccbd20107694986b7047274748727bb28948265",
         "Blood Glucose Control Via Pre-trained Counterfactual Invertible Neural Networks",
         "Type 1 diabetes mellitus (T1D) is characterized by insulin deficiency and blood glucose (BG) control issues. The state-of-the-art solution for continuous BG control is reinforcement learning (RL), where an agent can dynamically adjust exogenous insulin doses in time to maintain BG levels within the target range. However, due to the lack of action guidance, the agent often needs to learn from randomized trials to understand misleading correlations between exogenous insulin doses and BG levels, which can lead to instability and unsafety. To address these challenges, we propose an introspective RL based on Counterfactual Invertible Neural Networks (CINN). We use the pre-trained CINN as a frozen introspective block of the RL agent, which integrates forward prediction and counterfactual inference to guide the policy updates, promoting more stable and safer BG control. Constructed based on interpretable causal order, CINN employs bidirectional encoders with affine coupling layers to ensure invertibility while using orthogonal weight normalization to enhance the trainability, thereby ensuring the bidirectional differentiability of network parameters. We experimentally validate the accuracy and generalization ability of the pre-trained CINN in BG prediction and counterfactual inference for action. Furthermore, our experimental results highlight the effectiveness of pre-trained CINN in guiding RL policy updates for more accurate and safer BG control.",
         "2024",
         "2024-05-23",
         "10.48550/arxiv.2405.17458",
         "arXiv.org",
         null,
         "https://arxiv.org",
         "['Computer Science']",
         "['Computer Science']",
         "1",
         "1",
         "59",
         "[]",
         "['2259229', '2303466987', '2303933385', '2153979217']",
         "['Jingchi Jiang', 'Rujia Shen', 'Boran Wang', 'Yi Guan']",
         "https://www.semanticscholar.org/paper/0ccbd20107694986b7047274748727bb28948265",
         null,
         "Causal Reinforcement Learning | Causal RL",
         "False",
         "False",
         "False",
         "False",
         null
        ],
        [
         "28",
         "0f2f7a191334d6883ad24fed1f123dec4015d12f",
         "Tree Search in DAG Space with Model-based Reinforcement Learning for Causal Discovery",
         "Identifying causal structure is central to many fields ranging from strategic decision making to biology and economics. In this work, we propose Causal Discovery Upper Confidence Bound for Trees (CD-UCT), a model-based reinforcement learning (RL) method for causal discovery based on tree search that builds directed acyclic graphs (DAGs) incrementally. We also formalize and prove the correctness of an efficient algorithm for excluding edges that would introduce cycles, which enables deeper discrete search and sampling. The proposed method can be applied broadly to causal Bayesian networks with both discrete and continuous random variables. We conduct a comprehensive evaluation on synthetic and real-world datasets showing that CD-UCT substantially outperforms the state-of-the-art model-free RL technique that operates in DAG space and greedy search, constituting a promising advancement for combinatorial methods.",
         "2023",
         "2023-10-20",
         "10.48550/arxiv.2310.13576",
         "Proceedings of the Royal Society A",
         "journal",
         "https://www.jstor.org/journal/procmathphysengi",
         "['Computer Science']",
         "['Computer Science']",
         "2",
         "0",
         "64",
         "[]",
         "['41031873', '2257204110', '2256154162']",
         "['Victor-Alexandru Darvariu', 'Stephen Hailes', 'Mirco Musolesi']",
         "https://www.semanticscholar.org/paper/0f2f7a191334d6883ad24fed1f123dec4015d12f",
         null,
         "Causal Reinforcement Learning | Causal RL",
         "False",
         "False",
         "False",
         "False",
         null
        ],
        [
         "29",
         "0fffb641035550c97d14eca0cb0159b970dd5387",
         "Inferring Time-delayed Causal Relations in POMDPs from the Principle of Independence of Cause and Mechanism",
         "This paper introduces an algorithm for discovering implicit and delayed causal relations between events observed by a robot at regular or arbitrary times, with the objective of improving data-efficiency and interpretability of model-based reinforcement learning (RL) techniques. The proposed algorithm initially predicts observations with the Markov assumption, and incrementally introduces new hidden variables to explain and reduce the stochasticity of the observations. The hidden variables are memory units that keep track of pertinent past events. Such events are systematically identified by their information gains. A test of independence between inputs and mechanisms is performed to identify cases when there is a causal link between events and those when the information gain is due to confounding variables. The learned transition and reward models are then used in a Monte Carlo tree search for planning. Experiments on simulated and real robotic tasks, and the challenging 3D game Doom show that this method significantly improves over current RL techniques.",
         "2021",
         "2021-08-01",
         "10.24963/ijcai.2021/268",
         "International Joint Conference on Artificial Intelligence",
         "conference",
         "http://www.ijcai.org/",
         "['Computer Science']",
         "['Computer Science']",
         "3",
         "0",
         "24",
         "[]",
         "['2118630', '2209847']",
         "['Junchi Liang', 'Abdeslam Boularias']",
         "https://www.semanticscholar.org/paper/0fffb641035550c97d14eca0cb0159b970dd5387",
         "https://www.ijcai.org/proceedings/2021/0268.pdf",
         "Causal Reinforcement Learning | Causal RL",
         "False",
         "False",
         "False",
         "False",
         null
        ],
        [
         "30",
         "102def87a05681a2367c16825d250c84e769c9f6",
         "Fermionic vacua and entanglement in hyperbolic de Sitter spacetime",
         "We point out a non-trivial feature of the vacuum structure of free massive or massless Dirac fields in the hyperbolic de Sitter spacetime. Here we have two causally disconnected regions, say $R$ and $L$ separated by another region, $C$, appropriate to discuss the long range correlations of two superhorizon separated observers. There are local modes of the Dirac field having supports individually either in $R$ or $L$ as well as global modes found via analytically continuing the $R$ modes to $L$ and vice versa. However, we show that unlike the case of a real scalar field, the analytic continuation does not preserve the orthogonality of the resulting global modes. Accordingly, we need to orthonormalise these modes by making their suitable linear combinations prior to the field quantisation, in order to preserve the canonical anti-commutation relations. The most general form of this orthonormalisation is achieved via introducing a spacetime independent continuous parameter, $\\theta_{\\rm RL}$, resulting in a one-parameter family of global vacua. We emphasise that unlike the case of the so called de Sitter $\\alpha$-vacua, introducing such parametrisation is mandatory in our present scenario, in order to preserve the natural canonical structure. Using these vacua, we next investigate both entanglement and Renyi entropies by tracing over the states belonging to either $R$ or $L$ region and demonstrate their variations with respect to the parameter $\\theta_{\\rm RL}$.",
         "2018",
         "2018-12-18",
         "10.1140/epjc/s10052-019-7319-x",
         "The European Physical Journal C",
         null,
         null,
         "['Physics']",
         "['Physics']",
         "5",
         "0",
         "47",
         "['171ba077c9f4e33f49dd1689c0fc7098a3147142', '5ad4206e7121052a9ec4132101bcbf8549a48a31', '53a899e0e5e40fb584e98f660e7dd346624f64b9', 'e2bf19a8bce2a3f8303e39a906a002cacdf10846', '896b71ac1de1ff6d0876944e4f6636dcf04e9c97', '62111194251f914d42572929597c341c685cb624', 'fcb4e812ce98fb0a83721eee0501afababbc1a19', '9cf51eb6aa0e1fd661fe48c6adf0011e4d528f85', 'd1870673fd38abcc31598fca5ba768b42f65ac4c', '48313568d621532b188546aee1bbe71f743b5372', '25c1f4f5638724c11cf2636075d12309a99767db', '43dbcb818b1ba1eded9289c721db076ef8f1d040', 'f08d9fae9ddb873cf15c8647ad693c7b12702395', '8a7d410d603e1660e03ab727d409d00fe1efb4a8', '82817766a0cf641f5adcd030aaad851acc5a0cec', '99583e5abe697d9e1692cf3ac6bd20b1d41dfb9b', '2c71767446b5a0971183937eadf9bb08f5a58c2e', '734cc1b40f361faf6e56bff6928f0590e015b14d', 'c87d2a9da682e81e9b13fc0fa29a7f09be349094', '8da1c9ecdb30f146191a916cd2591c548b7fddde', '89156eeeb191fda44fbf726223350f74fac583c7', 'f8dc8aa0b98972d230ced1e18d5d5ba5044b6e26', '8001d6ac3db892ef4ee02abb0046fbefd521cdd2', '262ef707f2f61e7aceb1115b01ffc121abd26b45', 'ddbf9bc7a13e503f9afcaa4aea1a6495afb41dc8', 'dd5fc1fa5669abed85c846847fd8300c6fa06168', '232df42c97c8a52eb09b85320ae66a71d5ccfba4', '1a67b9bc1e954e6ba8c8994a660dfebbb84f586e', 'c89b5a550534412f371ac44189c7ff57181a2626', '9467f8cba966e685cca5d615702136f81a0d8234', 'a8f5e18f72a61cf710d0f89a42abcddaf3bc991a', '1c488e771460a548971861cf1ecb82937ca79b5d', '528a230a4196cff3141404a2c0f28abb0f631465', 'ea98d25075f16c8377abf8f97db50f6c4ad306ac', '1f010d833bb691de44f33c3322f8b86c295084e3', '3ded6fa16dcae21351cd39a2981dfaba5a630871', 'f62fcf577d23b23fa58a66dba71ed5eab3260476', None, None, None, None, None, None, None, '0665b56ad2271286f2a71cfef5c6ab310ea4de50', None, 'def9a773c4aefaacc1a355a02187d48cb6dd4080']",
         "['46508728', '103192109', '1382445838']",
         "['Sourav Bhattacharya', 'S. Chakrabortty', 'Shivang Goyal']",
         "https://www.semanticscholar.org/paper/102def87a05681a2367c16825d250c84e769c9f6",
         "https://link.springer.com/content/pdf/10.1140/epjc/s10052-019-7319-x.pdf",
         "Causal Reinforcement Learning | Causal RL",
         "False",
         "True",
         "False",
         "False",
         "https://openalex.org/W3098731377"
        ],
        [
         "31",
         "103881e6eb432286ad86b2a345880e276c60ff50",
         "Using Confounded Data in Latent Model-Based Reinforcement Learning",
         "A safe method to exploit confounded o � ine data in model-based RL, which improves the sample-e � ciency of an interactive agent that collects and learns from online, unconfounded data.",
         "2023",
         null,
         "nan",
         "Trans. Mach. Learn. Res.",
         null,
         null,
         "['Computer Science']",
         "['Computer Science']",
         "2",
         "0",
         "46",
         "[]",
         "['2001205298', '2115380653', '2084198786', '1720664']",
         "['Maxime Gasse', 'Damien Grasset', 'Guillaume Gaudron', 'Pierre-Yves Oudeyer']",
         "https://www.semanticscholar.org/paper/103881e6eb432286ad86b2a345880e276c60ff50",
         null,
         "Causal Reinforcement Learning | Causal RL",
         "False",
         "False",
         "False",
         "False",
         "https://openalex.org/W4391157052"
        ],
        [
         "32",
         "118b63b6700bd02dd00e9a39e272d22e21294cc6",
         "IWH Research Alert-May 22, 2020",
         "A prospective double-blind randomized controlled trial to examine the effect of adding aerobic exercise to neck-specific exercise treatment for patients with neck pain to reduce pain and disability and marginal structural models can be particularly useful for observational data.",
         "2020",
         null,
         "nan",
         null,
         null,
         null,
         "[]",
         "[]",
         "0",
         "0",
         "9",
         "[]",
         "[]",
         "[]",
         "https://www.semanticscholar.org/paper/118b63b6700bd02dd00e9a39e272d22e21294cc6",
         null,
         "Causal Reinforcement Learning | Causal RL",
         "False",
         "False",
         "False",
         "False",
         null
        ],
        [
         "33",
         "119ad6b55970b90696c620b7b3985b86845cb533",
         "Counterfactual Data Augmentation using Locally Factored Dynamics",
         "Many dynamic processes, including common scenarios in robotic control and reinforcement learning (RL), involve a set of interacting subprocesses. Though the subprocesses are not independent, their interactions are often sparse, and the dynamics at any given time step can often be decomposed into locally independent causal mechanisms. Such local causal structures can be leveraged to improve the sample efficiency of sequence prediction and off-policy reinforcement learning. We formalize this by introducing local causal models (LCMs), which are induced from a global causal model by conditioning on a subset of the state space. We propose an approach to inferring these structures given an object-oriented state representation, as well as a novel algorithm for model-free Counterfactual Data Augmentation (CoDA). CoDA uses local structures and an experience replay to generate counterfactual experiences that are causally valid in the global model. We find that CoDA significantly improves the performance of RL agents in locally factored tasks, including the batch-constrained and goal-conditioned settings.",
         "2020",
         "2020-07-06",
         "10.48550/arxiv.2007.02863",
         "Neural Information Processing Systems",
         "conference",
         "http://neurips.cc/",
         "['Computer Science', 'Mathematics']",
         "['Computer Science', 'Mathematics']",
         "95",
         "10",
         "93",
         "['72260c19441259404ed24003d9e27588fb3613ae', '5e7bc93622416f14e6948a500278bfbe58cd3890', '744139d65c3bf6da6a6acd384a32d94a06f44f62', '6568423cfaca7e24c88ea208cb0e67129e43aa9b', 'b76bd29b659388a8a0e83348d002e41e5e4a3489', 'bc0f283f92ad99481500b0b53d5ea9e2c757fef1', '3538c520244b508945476f0814d2ba1e8f22307e', '9d55314573ec254569df35ecc4cc8d464431f2cc', '00b97d826879113a1da447795b5bfc7e7752eeb1', 'd4a28c5c2c37d57501d4529a135fb1f9cd8e3234', '19b924dd9121f01165276a7afb764cf394acb80b', '67a9dde04f367efc903b6d06097df9bdd9887ae7', '4ee70fb32981f84f9dddc57bd59a69e677c91759', '9001698e033524864d4d45f051a5ba362d4afd9e', '7b983cfa013c532e8316e894ecb78d3cf3aa48b1', '1eb7f46b1a0a7df823194d86543e5554aa21021a', 'a4a2d99d1c237d0818971ec9205e89128c57fb02', 'ae39e54c451897999032cde7d3e1c33139d7fdc3', 'cfb68baa23048e3e0f8845c099fa013797bd623f', 'd0ca8e0b9e1d31d832b4b25bfe8ef825abb7c833', '7cd12c64940d2ee4bd5fce4b959e49bbe9a92c39', 'ab131a2e397a3d5b987d78fef31c3b060a118619', 'd25b1edda507cba944938ec8784d8b124c2381a5', 'b08256a6a3faba052bdd3445d917dd656d248477', '5285cb8faada5de8a92a47622950f6cfd476ac1d', 'ef2bc452812d6005ab0a66af6c3f97b6b0ba837e', '9c5c794094fbf5da8c48df5c3242615dc0b1d245', '335afb1066996d2c1e2b33da28a45bbf851e1be6', '512b8ef0002e0bfd0ecb5ab17d533c1762eb9786', '70e28eb8ee40cf5caa704ac7f87940c0818ba28e', '745a134eca192982e8e0c16d6f36cfe24f9bdd08', 'aea46419847f0a7d0db45b2934f62e3e41a8ad89', '44d7bec471a7e5821f9d6e0107ca71c9bf5edcd0', '8ad3bc604adc58c828c30e55e9adef0a81bf7e81', 'd37a34c204a8beefcaef4dddddb7a90c16e973d4', '3aadab924520c58be81781aafd51e6807e9c4576', '00ec8123dd2ba03afab7c1fa02f774062f769181', 'bf38b796a0f3da7c61b13846144c0961a5a40405', '5e49c80f8b12a100c5f4518897c4cbf72710c252', '93787eee4e36ab3e5b19f94b99dc9d315f5ace25', 'a71f1480abe044ae90494a23f994dfb5b40e6f8c', '2804ea29eeaca35c6c80e9f42a00170dc8d3fc1c', 'ce1c28ca2f52a42c6e60d792cd71ba894abc47d5', '4debb99c0c63bfaa97dd433bc2828e4dac81c48b', '1bdcf6fe02ed2ff097e5f4ffdc5af159cd9a713a', '1f6c3f1def78919f06efe050e9403e85d5fa3ac9', '46f6a90fcf0ecc4b60470a1f35cd95d65d5f8d9b', '818c52f4ba56cb8cf152ad614f2f4803057a5cfe', '5c717445179991e002333182df3b233fe502e357', 'b22b4817757778bdca5b792277128a7db8206d08', 'cf020b27d06efb28f3e5db264aceeec1f397817b', '429ed4c9845d0abd1f8204e1d7705919559bc2a2', '204e3073870fae3d05bcbc2f6a8e263d9b72e776', '7c3ece1ba41c415d7e81cfa5ca33a8de66efd434', '2b292ff89d808fba10579871591a22f1649cd039', '1a995c3bce73629a14fdeb827d7bc4be9b11c0a0', 'a456265138c088a894301c0433dae938705a9bec', '2f85b7376769473d2bed56f855f115e23d727094', 'ccf6a69a7f33bcf052aa7def176d3b9de495beb7', '97fb4e3d45bb098e27e0071448b6152217bd35a5', 'bd6b6291c3c14551cf9f2aa0e04e2e33c86b800e', 'de5e7320729f5d3cbb6709eb6329ec41ace8c95d', 'dc3e905bfb27d21675ee1720413e007b014b37d3', 'ec8a2f6cfe72309f5f1608d22ec28778d3ee976a', 'c6170fa90d3b2efede5a2e1660cb23e1c824f2ca', '024006d4c2a89f7acacc6e4438d156525b60a98f', '5dc2a215bd7cd5bdd3a0baa8c967575632696fac', '3a21ef1a4761fd90eb2092dee455d8da5dc8f317', '90f72fbbe5f0a29e627db28999e01a30a9655bc6', 'a6cb366736791bcccc5c8639de5a8f9636bf87e8', '91dfdd05a99256c2293b952c3b008326b015d4eb', '471e452dc02edcb9c8c0ec446cc2eb22188dd86b', 'abd1c342495432171beb7ca8fd9551ef13cbd0ff', 'f82e4ff4f003581330338aaae71f60316e58dd26', 'b8a3067d6c1fb255c9cdb5ac66037b82152ab3bb', '73ab134a1264d895d9b67395ffab967439a86479', '4705e2d2d8f4a8d093e22a966da24e14e3bae023', 'f0fa332388686497e20fca449487af2b799377ab', 'f9e1d192d9b46e9f2c001d267d30cd7f604e3fdd', '1c6356a688a273889fb71bab17973ed10483d97f', '2430b4748c4ffe8782ae4763d327ce48f3655639', 'e3678df4a8c183fc50d59e6277284078785600e6', '936a67aad36a9d9a7799237f0499d2f588d6e8ba', '5bdd9a3317b43966f97b5c70c55c46fd19335049', 'd3bfc039f870388491f6d89f8f2a88c0b117ee0c', 'a9cd8efe9184dddb1bedbbec3a356c4dfb22fe63', '03b7e51c52084ac1db5118342a00b5fbcfc587aa', '831edc3d67457db83da40d260e93bfd7559347ae', '6a7c63a73724c0ca68b1675e256bb8b9a35c94f4', '97efafdb4a3942ab3efba53ded7413199f79c054', '6df43f70f383007a946448122b75918e3a9d6682', 'bf485a9a4ad17212d05fce564d5a6d4eaf840df4', None]",
         "['32305445', '3422145', '1873736']",
         "['Silviu Pitis', 'Elliot Creager', 'Animesh Garg']",
         "https://www.semanticscholar.org/paper/119ad6b55970b90696c620b7b3985b86845cb533",
         null,
         "Causal Reinforcement Learning | Causal RL",
         "True",
         "False",
         "False",
         "False",
         "https://openalex.org/W4287727072"
        ],
        [
         "34",
         "11b30a4d696c1f053c0d7d9d374d70386d14f798",
         "Understanding How Reader Characteristics Affect Comprehension of Text.",
         "A review of the literature was conducted to summarize' how characteristics of individual readers help to determine the nature and quality of their comprehension of specific texts and to discuss how this-knowledge might be used by teachers to promote effective readin4-ih their classes.",
         "1988",
         "1988-09-01",
         "nan",
         null,
         null,
         null,
         "['Computer Science']",
         "['Computer Science']",
         "0",
         "0",
         "92",
         "[]",
         "['69058190']",
         "['Cheryl L. Spaulding']",
         "https://www.semanticscholar.org/paper/11b30a4d696c1f053c0d7d9d374d70386d14f798",
         null,
         "Causal Reinforcement Learning | Causal RL",
         "False",
         "False",
         "False",
         "False",
         "https://openalex.org/W60081284"
        ],
        [
         "35",
         "11cca3968b569ef58bcf8ec4b5716e681e140e90",
         "Robust Decision Pipelines: Opportunities and Challenges for AI in Business Process Modelling ⋆",
         "Building on existing verification technologies, a fast growing research effort is tackling the problem of computing robustness guarantees for deep learning; examples include search-based safety verification using SMT for DL, guaranteed robust explanations for DL, and provable robustness to causal interventions for DL decisions.",
         "2023",
         null,
         "10.1109/access.2021.3140175",
         "IEEE Access",
         null,
         null,
         "[]",
         "[]",
         "0",
         "0",
         "6",
         "[]",
         "['49444240']",
         "['M. Kwiatkowska']",
         "https://www.semanticscholar.org/paper/11cca3968b569ef58bcf8ec4b5716e681e140e90",
         null,
         "Causal Reinforcement Learning | Causal RL",
         "True",
         "True",
         "False",
         "False",
         "https://openalex.org/W4206484811"
        ],
        [
         "36",
         "144c28a1dec6e8f3a30889ce6401a696db401f07",
         "CausalPlayground: Addressing Data-Generation Requirements in Cutting-Edge Causality Research",
         "Research on causal effects often relies on synthetic data due to the scarcity of real-world datasets with ground-truth effects. Since current data-generating tools do not always meet all requirements for state-of-the-art research, ad-hoc methods are often employed. This leads to heterogeneity among datasets and delays research progress. We address the shortcomings of current data-generating libraries by introducing CausalPlayground, a Python library that provides a standardized platform for generating, sampling, and sharing structural causal models (SCMs). CausalPlayground offers fine-grained control over SCMs, interventions, and the generation of datasets of SCMs for learning and quantitative research. Furthermore, by integrating with Gymnasium, the standard framework for reinforcement learning (RL) environments, we enable online interaction with the SCMs. Overall, by introducing CausalPlayground we aim to foster more efficient and comparable research in the field. All code and API documentation is available at https://github.com/sa-and/CausalPlayground.",
         "2024",
         "2024-05-21",
         "10.48550/arxiv.2405.13092",
         "arXiv.org",
         null,
         "https://arxiv.org",
         "['Computer Science']",
         "['Computer Science']",
         "2",
         "0",
         "36",
         "[]",
         "['2267243941', '2280332096', '2562595']",
         "['Andreas Sauter', 'Erman Acar', 'A. Plaat']",
         "https://www.semanticscholar.org/paper/144c28a1dec6e8f3a30889ce6401a696db401f07",
         null,
         "Causal Reinforcement Learning | Causal RL",
         "False",
         "False",
         "False",
         "False",
         null
        ],
        [
         "37",
         "1632f51c8e573264730061dc5c9e7db821535bc4",
         "Invariant Action Effect Model for Reinforcement Learning",
         "Good representations can help RL agents perform concise modeling of their surroundings, and thus support effective decision-making in complex environments. \n Previous methods learn good representations by imposing extra constraints on dynamics.\n However, in the causal perspective, the causation between the action and its effect is not fully considered in those methods, which leads to the ignorance of the underlying relations among the action effects on the transitions. \n Based on the intuition that the same action always causes similar effects among different states, we induce such causation by taking the invariance of action effects among states as the relation.\n By explicitly utilizing such invariance, in this paper, we show that a better representation can be learned and potentially improves the sample efficiency and the generalization ability of the learned policy. \n We propose Invariant Action Effect Model (IAEM) to capture the invariance in action effects, where the effect of an action is represented as the residual of representations from neighboring states.\n IAEM is composed of two parts:\n (1) a new contrastive-based loss to capture the underlying invariance of action effects;\n (2) an individual action effect and provides a self-adapted weighting strategy to tackle the corner cases where the invariance does not hold.\n The extensive experiments on two benchmarks, i.e. Grid-World and Atari, show that the representations learned by IAEM preserve the invariance of action effects. \n Moreover, with the invariant action effect, IAEM can accelerate the learning process by 1.6x, rapidly generalize to new environments by fine-tuning on a few components, and outperform other dynamics-based representation methods by 1.4x in limited steps.",
         "2022",
         "2022-06-28",
         "10.1609/aaai.v36i8.20913",
         "AAAI Conference on Artificial Intelligence",
         "conference",
         "http://www.aaai.org/",
         "['Computer Science']",
         "['Computer Science']",
         "12",
         "1",
         "32",
         "['a4161927bb90f44b37746739b937b901ae75c4aa', 'e06e573edffdc741c2e9e232b7ee6ee87229cfe1', 'b978c8605bbe8abf1f7f4292b510a3f8d4f2b2d9', '518b827e340c26582b5093401283a4f5cff605b9', '9efb64f20ab1f157ca9f4050d4aaacf6c3f9b2b2', '54e1a1f3a60ce51eae5e27e1724294032cc60929', '2f0e54d462b52d3ab88074abdc33887bdd5a2caa', '7d2c7517a59669108588b98de1876f54ca23f6b2', '19b924dd9121f01165276a7afb764cf394acb80b', '2e73ba5c098e5207aaecf159b6e6620b3f2ea56e', '188dac491f04c56e1eb7d7b33ac6aa0b87303232', '1cae417456711c4da184f5efcd1b7464a7a0661a', 'fea3e63c97c7292dc6fbcb3ffe7131eb54053986', '8e372ed2b688de0e4dcffbec1d2abdd0fc7ea27a', 'd6dd369c4893de5b0a674f45dba6d41429aa0660', '93adca9ce6f4a0fab9ea027c90b4df828cfa10d7', 'b227f3e4c0dc96e5ac5426b85485a70f2175a205', 'ad0a1b0991a9150b765c2a45eb2b368702b35cd1', '5572169a9adeedb99a6c20f0ede09b320d3fec61', 'a9a3ed69c94a3e1c08ef1f833d9199f57736238b', '0ab3f7ecbdc5a33565a234215604a6ca9d155a33', '2adae2da173b9dd720c8bcac0250a90a7f1ec697', '69e76e16740ed69f4dc55361a3d319ac2f1293dd', '846aedd869a00c09b40f1f1f35673cb22bc87490', '340f48901f72278f6bf78a04ee5b01df208cc508', '2319a491378867c7049b3da055c5df60e1671158', 'f82e4ff4f003581330338aaae71f60316e58dd26', '02707d4ffb6f5965a2270f89d6767e9e9b8abf9d', '86f899ad953659ab6c267ae658c038fcaf1faca2', 'bff20fb30adad8d1c173963089df5fc9664304f0', '1c200fd0057b32e1c67084999d2912f3dec6e1cd', '133583b9c4634702ab2579205ee3eb78714b7feb']",
         "['1999326708', '2119326931', '40685903', '144705629', '2119017332']",
         "['Zhengbang Zhu', 'Shengyi Jiang', 'Yu-Ren Liu', 'Yang Yu', 'Kun Zhang']",
         "https://www.semanticscholar.org/paper/1632f51c8e573264730061dc5c9e7db821535bc4",
         "https://ojs.aaai.org/index.php/AAAI/article/download/20913/20672",
         "Causal Reinforcement Learning | Causal RL",
         "False",
         "False",
         "False",
         "False",
         null
        ],
        [
         "38",
         "1a095767b00b7cebe3d03d80a5f7a4a3f867d4d1",
         "EVIDENCIA DE LA ESTABILIDAD CARIOTÍPICA DURANTE LA DIVERGENCIA EVOLUTIVA ENTRE Paralabrax maculatofasciatus Y P. nebulifer (PERCIFORMES: SERRANIDAE)",
         "This study determined the cytogenetic similarity between Paralabrax maculatofasciatus and P. nebulifer by comparing number, type and size of chromosomes of these taxa and supported the hypothesis that evolutionary divergence is supported.",
         "2012",
         null,
         "nan",
         null,
         null,
         null,
         "['Biology']",
         "['Biology']",
         "3",
         "2",
         "0",
         "[]",
         "['2086680541', '1436608633', '2094461015', '103891894']",
         "['J. M. Martínez-Brown', 'J. D. Medel-Narváez', 'N. K. Hernández-Ibarra', 'J. L. B. Galindo']",
         "https://www.semanticscholar.org/paper/1a095767b00b7cebe3d03d80a5f7a4a3f867d4d1",
         null,
         "Causal Reinforcement Learning | Causal RL",
         "False",
         "False",
         "False",
         "False",
         null
        ],
        [
         "39",
         "1aa4d9eb8aa15b22a8d75b0bfa569c09b0c73443",
         "Building Minimal and Reusable Causal State Abstractions for Reinforcement Learning",
         "Two desiderata of reinforcement learning (RL) algorithms are the ability to learn from relatively little experience and the ability to learn policies that generalize to a range of problem specifications. \nIn factored state spaces, one approach towards achieving both goals is to learn state abstractions, which only keep the necessary variables for learning the tasks at hand. \nThis paper introduces Causal Bisimulation Modeling (CBM), a method that learns the causal relationships in the dynamics and reward functions for each task to derive a minimal, task-specific abstraction. \nCBM leverages and improves implicit modeling to train a high-fidelity causal dynamics model that can be reused for all tasks in the same environment. \nEmpirical validation on two manipulation environments and four tasks reveals that CBM's learned implicit dynamics models identify the underlying causal relationships and state abstractions more accurately than explicit ones. Furthermore, the derived state abstractions allow a task learner to achieve near-oracle levels of sample efficiency and outperform baselines on all tasks.",
         "2024",
         "2024-01-23",
         "10.1609/aaai.v38i14.29507",
         "AAAI Conference on Artificial Intelligence",
         "conference",
         "http://www.aaai.org/",
         "['Computer Science']",
         "['Computer Science']",
         "6",
         "0",
         "36",
         "[]",
         "['2881858', '2160103649', '2118724825', '2322736301', '2261286584']",
         "['Zizhao Wang', 'Caroline Wang', 'Xuesu Xiao', 'Yuke Zhu', 'Peter Stone']",
         "https://www.semanticscholar.org/paper/1aa4d9eb8aa15b22a8d75b0bfa569c09b0c73443",
         "https://ojs.aaai.org/index.php/AAAI/article/download/29507/30841",
         "Causal Reinforcement Learning | Causal RL",
         "False",
         "False",
         "False",
         "False",
         null
        ],
        [
         "40",
         "1b577cafcbda916ccf32a5e0291210afde5c663b",
         "THE IMPLICIT PREFERENCE INFORMATION",
         "An algorithm based on Maximum Causal Entropy IRL is developed and it is found that information from the initial state can be used to infer both side effects that should be avoided as well as preferences for how the environment should be organized.",
         "2018",
         null,
         "10.1109/cvpr.2018.00745",
         "arXiv (Cornell University)",
         null,
         null,
         "[]",
         "[]",
         "0",
         "0",
         "27",
         "[]",
         "['2078844297']",
         "['AN In']",
         "https://www.semanticscholar.org/paper/1b577cafcbda916ccf32a5e0291210afde5c663b",
         null,
         "Causal Reinforcement Learning | Causal RL",
         "True",
         "True",
         "False",
         "False",
         "https://openalex.org/W2752782242"
        ],
        [
         "41",
         "1b5d51f621af962d2dc3695fd20829eea2024bf5",
         "Factored Adaptation for Non-Stationary Reinforcement Learning",
         "Dealing with non-stationarity in environments (e.g., in the transition dynamics) and objectives (e.g., in the reward functions) is a challenging problem that is crucial in real-world applications of reinforcement learning (RL). While most current approaches model the changes as a single shared embedding vector, we leverage insights from the recent causality literature to model non-stationarity in terms of individual latent change factors, and causal graphs across different environments. In particular, we propose Factored Adaptation for Non-Stationary RL (FANS-RL), a factored adaption approach that learns jointly both the causal structure in terms of a factored MDP, and a factored representation of the individual time-varying change factors. We prove that under standard assumptions, we can completely recover the causal graph representing the factored transition and reward function, as well as a partial structure between the individual change factors and the state components. Through our general framework, we can consider general non-stationary scenarios with different function types and changing frequency, including changes across episodes and within episodes. Experimental results demonstrate that FANS-RL outperforms existing approaches in terms of return, compactness of the latent state representation, and robustness to varying degrees of non-stationarity.",
         "2022",
         "2022-03-30",
         "10.48550/arxiv.2203.16582",
         "Neural Information Processing Systems",
         "conference",
         "http://neurips.cc/",
         "['Computer Science']",
         "['Computer Science']",
         "38",
         "3",
         "79",
         "['7cd1e05e0679dac5c5b65f9894756c7f6bcb4f94', '227062c151fb48f222f6c3b946a1618a46c01660', '97575233a5501d03566af33d795015dca455d179', '955536024c5db4166e63d41406c290fcf7ade696', 'ca47b92d53f4554495e4452c1057e2a6674c3864', 'aa4ca2d92b79d19bd938748d1233f7dc58195955', '99b9595ac6a13bf986db40a301e3f11442eec36b', '924656f720c47d7bc215b522c6837a708f54fc45', 'e30bdcfc8cc2769ffd6d39c5d60651787a8d9ab3', 'a56f5cb13ee11a2d3750294b592e647372f8c77b', 'b124114c0f9a4145892e328f19df7850333f207e', '3803ea42e1fc773db3b1d0fa05f41b5ebf0a61d1', '5f1adc14a77fb61aa463fac728397bd32e00b617', '116a9a94df0213b11aa0585af81109e853f9deb3', 'acf24ff124d9359d0404ed77967d292fc2e0a342', 'b0887ad68a41d8e796cc7afe562a5d0da93fcda0', 'fac54bfee9d6efd82bef3f62c953f8eaef5fc426', '48487c1d8e922ab4ee40f7544d4866cdc67ba79e', '16ce156a802e43d34929f0b8d32b93db7e852690', 'd83aa8b4afaad092b3e8a1f8c4898d71180cbdd7', '67a65737d17713ae8bee1b69b853ee658bc6626f', '0e58395ec5677ac3e6876c51cd1dba0cf299261e', '0bc855f84668b35cb65618d996d09f6e434d28c9', '67a9dde04f367efc903b6d06097df9bdd9887ae7', 'cdc04b748e440f43547c0516f77480ffb8bf5cda', 'd48312697f84bc802272d1482762a0415664b283', '9aa3ae820772b5f25df9d498bd60c72e70d1b5e6', 'd2cb30c20a6578e16a5465b1e9e2ac13091a193b', 'd6dd369c4893de5b0a674f45dba6d41429aa0660', '41cca0b0a27ba363ca56e7033569aeb1922b0ac9', '11943efec248fcac57ff6913424e230d0a02e977', '4d3b69bdcd1d325d29badc6a38f2d6cc504fe7d1', '944bd3b472c8a30163bbfc1b5cbab8545693c3e0', '249408527106d7595d45dd761dd53c83e5a02613', '811df72e210e20de99719539505da54762a11c6d', 'd93b0b37ee0e87a0e096ef803667b0798f465528', '6ac5eb309dd937d801d180c830377e4d551699a2', 'f98788f32b0d33d200c9bc7d900d0ef39519c927', 'c889d6f98e6d79b89c3a6adf8a921f88fa6ba518', '2b10281297ee001a9f3f4ea1aa9bea6b638c27df', 'b6b8a1b80891c96c28cc6340267b58186157e536', '340f48901f72278f6bf78a04ee5b01df208cc508', '3a21ef1a4761fd90eb2092dee455d8da5dc8f317', '04955600df47c66a055591d927c024e6e9c72c61', '0883b0cb9329c52e80184163a521000a17757357', '0fda3565c03956c4a7a9d3e0febb57aea73940a5', '62f46f7fbcb8263a76e46c4a808c80953ddae8b1', 'b354ee518bfc1ac0d8ac447eece9edb69e92eae1', 'caa3126f6cb1811e65a79b7595f3a0a5595abe35', '01f8f26ec31b47aed045ef4d2d907987ad0d690f', '1c6356a688a273889fb71bab17973ed10483d97f', '7f8673fb4d9bf5d72d7d73895df20f538326d8b0', '955165328ff9d889b0a3e6d77f9804e1c502405f', 'e3678df4a8c183fc50d59e6277284078785600e6', '2e9d221c206e9503ceb452302d68d10e293f2a10', '18f4a1a78a826bcd5ec3eb1546304c5a6534a87b', None, 'c7629a4d7e1c87fd1ea73850bcb800538fd0aa4b', None, '5e86e17d83c97dafa3413d1d0dae219bd527ed61', '97efafdb4a3942ab3efba53ded7413199f79c054', '404d1df16b673659f52a8017bde4ba00f901e8bd', None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]",
         "['1486433080', '1938684', '2119017332', '1745665']",
         "['Fan Feng', 'Biwei Huang', 'Kun Zhang', 'Sara Magliacane']",
         "https://www.semanticscholar.org/paper/1b5d51f621af962d2dc3695fd20829eea2024bf5",
         "http://arxiv.org/pdf/2203.16582",
         "Causal Reinforcement Learning | Causal RL",
         "False",
         "False",
         "False",
         "False",
         null
        ],
        [
         "42",
         "1bad6960b875f6ff8937d7eb833245a9b2c6501f",
         "SOAP-RL: Sequential Option Advantage Propagation for Reinforcement Learning in POMDP Environments",
         "This work compares ways of extending Reinforcement Learning algorithms to Partially Observed Markov Decision Processes (POMDPs) with options. One view of options is as temporally extended action, which can be realized as a memory that allows the agent to retain historical information beyond the policy's context window. While option assignment could be handled using heuristics and hand-crafted objectives, learning temporally consistent options and associated sub-policies without explicit supervision is a challenge. Two algorithms, PPOEM and SOAP, are proposed and studied in depth to address this problem. PPOEM applies the forward-backward algorithm (for Hidden Markov Models) to optimize the expected returns for an option-augmented policy. However, this learning approach is unstable during on-policy rollouts. It is also unsuited for learning causal policies without the knowledge of future trajectories, since option assignments are optimized for offline sequences where the entire episode is available. As an alternative approach, SOAP evaluates the policy gradient for an optimal option assignment. It extends the concept of the generalized advantage estimation (GAE) to propagate option advantages through time, which is an analytical equivalent to performing temporal back-propagation of option policy gradients. This option policy is only conditional on the history of the agent, not future actions. Evaluated against competing baselines, SOAP exhibited the most robust performance, correctly discovering options for POMDP corridor environments, as well as on standard benchmarks including Atari and MuJoCo, outperforming PPOEM, as well as LSTM and Option-Critic baselines. The open-sourced code is available at https://github.com/shuishida/SoapRL.",
         "2024",
         "2024-07-26",
         "10.48550/arxiv.2407.18913",
         "arXiv.org",
         null,
         "https://arxiv.org",
         "['Computer Science']",
         "['Computer Science']",
         "0",
         "0",
         "49",
         "[]",
         "['2280066789', '2280069150']",
         "['Shu Ishida', 'João F. Henriques']",
         "https://www.semanticscholar.org/paper/1bad6960b875f6ff8937d7eb833245a9b2c6501f",
         null,
         "Causal Reinforcement Learning | Causal RL",
         "False",
         "False",
         "False",
         "False",
         null
        ],
        [
         "43",
         "1c73d712dd614d2eebf06f55229348f8e8b46b47",
         "On the Importance of Feature Decorrelation for Unsupervised Representation Learning in Reinforcement Learning",
         "Recently, unsupervised representation learning (URL) has improved the sample efficiency of Reinforcement Learning (RL) by pretraining a model from a large unlabeled dataset. The underlying principle of these methods is to learn temporally predictive representations by predicting future states in the latent space. However, an important challenge of this approach is the representational collapse, where the subspace of the latent representations collapses into a low-dimensional manifold. To address this issue, we propose a novel URL framework that causally predicts future states while increasing the dimension of the latent manifold by decorrelating the features in the latent space. Through extensive empirical studies, we demonstrate that our framework effectively learns predictive representations without collapse, which significantly improves the sample efficiency of state-of-the-art URL methods on the Atari 100k benchmark. The code is available at https://github.com/dojeon-ai/SimTPR.",
         "2023",
         "2023-06-09",
         "10.48550/arxiv.2306.05637",
         "International Conference on Machine Learning",
         "conference",
         "https://icml.cc/",
         "['Computer Science']",
         "['Computer Science']",
         "7",
         "1",
         "72",
         "['4c4bc4e9f9d6ab6b476ae582574e9532529be443', '542905f5fc96bce7572f6ded7f56aedfa62270c1', '127ebdb7b87fe5c8c8ff1bb9173584b75eec8f47', 'b62f6f765f033c1f023c4a424a20571564e61d97', '02d9dc238ae825e45e728607c3c83b77d07f4017', '65fc1f1c567801fee3788974e753cdbf934f07e9', '69b80ce5ab6c2263b145b1eaa23664145938f351', '0a818d426c813b7a4758c959d38ece5f2cc11476', '1c35807e1a4c24e2013fa0a090cee9cc4716a5f5', '28c17db217f2d7af12482a087d197851f0a97db0', '558ca2e8c7eb56edd77a52b084e6cc24dffe5bcd', '0f1382cb004b4834cc3ca7824a61d0d6b86a5763', 'c1ad5f9b32d80f1c65d67894e5b8c2fdf0ae4500', '8653cbe908c64c0e4a3591fe652d239ab7cf98c1', '8a9d84d86ac0d76e63914802f9738325c3bece9c', '57eaad10369de402d3363c1d99c93810463eb03c', '0e23d2f14e7e56e81538f4a63e11689d8ac1eb9d', 'af424c489ada416912634f1e580a485e10e53770', '17985b57240bfaea02a6098a7a34e71e780180eb', '7c4356ec0dca6e6df0af7a882e2cd1571c8bf3dc', '1e1e10d75c4ebabdbfb7912ca4cc06a27ffa85af', '38f93092ece8eee9771e61c1edaf11b1293cae1b', '90974d9e0df8466a50338601e839fa0ea69c9872', '28db20a81eec74a50204686c3cf796c42a020d2e', '127d6f4b0d377d5362fca84910e37d470a8bc979', '5e7bc93622416f14e6948a500278bfbe58cd3890', '6568423cfaca7e24c88ea208cb0e67129e43aa9b', '38643c2926b10f6f74f122a7037e2cd20d77c0f1', '9efb64f20ab1f157ca9f4050d4aaacf6c3f9b2b2', '43f2ad297941db230c089ba353efc3f281ab678c', '7af72a461ed7cda180e7eab878efd5f35d79bbf4', '0cc956565c7d249d4197eeb1dbab6523c648b2c9', '8d814620a1ca77e745bc8a33b96b86148f2804fe', 'add2f205338d70e10ce5e686df4a690e2851bdfc', '361c00b22e29d0816ca896513d2c165e26399821', '4012d4ab621f3f5f04b0f91849a60c6eaabe64b4', '2e73ba5c098e5207aaecf159b6e6620b3f2ea56e', '4f2eda8077dc7a69bb2b4e0a1a086cf054adb3f9', 'bc65f1fbf0ac429ffa1104cd8e026df861167331', '1fd4694e7c2d9c872a427d50e81b5475056de6bc', '5285cb8faada5de8a92a47622950f6cfd476ac1d', 'fea3e63c97c7292dc6fbcb3ffe7131eb54053986', 'b227f3e4c0dc96e5ac5426b85485a70f2175a205', 'e8270f523375d22fd417bb583a5b50411e5d5f58', 'f18d245627d6089cb8a0e4a7757f45c13b96bdaf', '408570c02ba213a856bc8186c62a4e5bf91a18de', 'a9a3ed69c94a3e1c08ef1f833d9199f57736238b', 'a729073d3ec9a159b86fd438e06f3dff2735beb5', 'd07284a6811f1b2745d91bdb06b040b57f226882', '0ab3f7ecbdc5a33565a234215604a6ca9d155a33', '79cfb51a51fc093f66aac8e858afe2e14d4a1f20', '3ed67ded2b4d3614b38798b3f17a8e69803d0980', 'e37b999f0c96d7136db07b0185b837d5decd599a', '846aedd869a00c09b40f1f1f35673cb22bc87490', 'df43c5669e6c358572c38e29b7dcfbaaf09d19fa', 'a61992160c1075035f184f28c1a01414971b2b58', '340f48901f72278f6bf78a04ee5b01df208cc508', '6fc6803df5f9ae505cae5b2f178ade4062c768d0', '1eb09fecd75eb27825dce4f964b97f4f5cc399d7', 'c93c57ee7b5ffbf25131b046948aa45d93629f7c', '5f5dc5b9a2ba710937e2c413b37b053cd673df02', 'f82e4ff4f003581330338aaae71f60316e58dd26', 'e3cedb92e4a02ede6e6201d32b5def6dedd32ec8', '4a7de0669fd835b2efcab97c7d3dc28ea7a1e6a3', '5ed59f49c1bb7de06cfa2a9467d5efb535103277', 'c2e8806f0bd1d504bcb395ef1f6fe509a023a048', 'eb9a33e7db8f56a2caf5b7a49ae11cf6de927c0e', '1974b52e96bda50e4ff200be06d1b089c452550c', None, 'cd18800a0fe0b668a1cc19f2ec95b5003d0a5035', None, None]",
         "['2163406260', '2110019663', '2140537743', '2162205515', '7613239', '1795455']",
         "['Hojoon Lee', 'Ko-tik Lee', 'Dongyoon Hwang', 'Hyunho Lee', 'ByungKun Lee', 'J. Choo']",
         "https://www.semanticscholar.org/paper/1c73d712dd614d2eebf06f55229348f8e8b46b47",
         "http://arxiv.org/pdf/2306.05637",
         "Causal Reinforcement Learning | Causal RL",
         "False",
         "False",
         "False",
         "False",
         null
        ],
        [
         "44",
         "1c7c173ed92067ac537d4d5ece4e76b2ae62aae4",
         "A Framework for Following Temporal Logic Instructions with Unknown Causal Dependencies",
         ". Teaching a deep reinforcement learning (RL) agent to follow instructions in multi-task environments is a challenging problem. We consider that user deﬁnes every task by a linear temporal logic (LTL) formula. However, some causal dependencies in complex environments may be unknown to the user in advance. Hence, when human user is specifying instructions, the robot cannot solve the tasks by simply following the given instructions. In this work, we propose a hierarchical reinforcement learning (HRL) framework in which a symbolic transition model is learned to eﬃciently produce high-level plans that can guide the agent eﬃciently solve diﬀerent tasks. Speciﬁcally, the symbolic transition model is learned by inductive logic programming (ILP) to capture logic rules of state transitions. By planning over the product of the symbolic transition model and the automaton derived from the LTL formula, the agent can resolve causal dependencies and break a causally complex problem down into a sequence of simpler low-level sub-tasks. We evaluate the proposed framework on three environments in both discrete and continuous domains, showing advantages over previous representative methods.",
         "2022",
         "2022-04-07",
         "10.48550/arxiv.2204.03196",
         "arXiv.org",
         null,
         "https://arxiv.org",
         "['Computer Science']",
         "['Computer Science']",
         "2",
         "0",
         "52",
         "['d69e2356d469139eced650bce41ab2073cf559f8', '3b513d8d62ee7a76cb6f1abd6beb214dd2402e88', 'a11d252edacb7c72075350e7e755ebb52745da62', '869cd60e0fe1c0ae5e93854f218bb33eaa45c1d4', 'c64a2d64cfc68c58e66c0114add1b897fcf1d4cf', '98559f4d9acbbaf12c8e83aea1a0940d713c319c', '6778d6a0f959cdcc42718ee9fc279fd1f00f3d88', 'e3450150135b97ad379cc35fe44afde021c37d44', '8770280ae538dffe083e87a7093195b1c42d6b73', 'a6aa61ada70b6ad88183bc409de5f3b9dfe8e23f', '38bcf084d0706a992e21e3461c0d41e5f8a3d914', '5abfd3c004f79402a3a321564e8963797f60c6b9', '418a2d5a6c6027e079357d872a0596ec5b344289', 'ac4383bc27ff06fd301b4eeb4aa0bd8a745fb8c0', 'c3d0e3c5ca9fa56cf2ff7303a2f67bf44694e6d4', 'a5c0645162f4b74895b86a98db86df8af680de77', 'f0824eafbc40a9b1b39983511ed050b66164db4c', '7dc156eb9d84ae8fd521ecac5ccc5b5426a42b50', 'af1dbd44d60e42b52f7629099376e1eaed3685ed', '3a6447361b20c249f5306ae17dee43f645430e31', '055c80ed8e9c5c80babf07b6a98159cae53257e2', '03e5e22509386a03cda56146ec56510dcaaab269', '3aadab924520c58be81781aafd51e6807e9c4576', '00ec8123dd2ba03afab7c1fa02f774062f769181', '811445a434f370327e7068f8295b12ad8638f356', '4df7bbe3ca7806f39a490c99f17867a0ac299bc3', 'caf4c59f1d84c2d8178e60c09a43e544ce41fc55', '5889e9afbcc3935867f9ae16fe46c71b9f2b071f', '471f9742b4e32d8ee68f9ee493768ff0466a231d', 'e77bb3a20de767338ce0e5d733e54461221419b3', '3a13f7c43b767b1fb72ef107ef62a4ddd48dd2a7', '8cc73657fec6c9dc61e2bce4d8d1d68d0cbeade6', '3b9732bb07dc99bde5e1f9f75251c6ea5039373e', '5c60def91632b2494bd39b84b22bf925087b898b', '340f48901f72278f6bf78a04ee5b01df208cc508', '2c7522d14a889130069b9e83ee2d8dcf7d866922', '1ddda53e7249463cc8cf3045476e0e8476c59754', '7de34d3ef25fe154d911ca40587c26a9afc99bb5', 'e88ee4fada3147f925a8c1cbfa8b335b3db4f8e7', '8736ed033cbdeacc8fdf468add5ac2af1e75445f', '57b6d3bb0a5f6065300e6ef8ee3291a36fec27dc', '1e90953131b400ee78801580df5503fdf763226d', '7e9690058b4f04875a2c7781efd1247a43bf6747', None, None, None, None, None, None, None, None, None]",
         "['145548210', '1730720']",
         "['Duo Xu', 'F. Fekri']",
         "https://www.semanticscholar.org/paper/1c7c173ed92067ac537d4d5ece4e76b2ae62aae4",
         "http://arxiv.org/pdf/2204.03196",
         "Causal Reinforcement Learning | Causal RL",
         "False",
         "False",
         "False",
         "False",
         null
        ],
        [
         "45",
         "1dce46450c1a08aa4cc0da964162733b67909fc0",
         "A Roadmap Towards Improving Multi-Agent Reinforcement Learning With Causal Discovery And Inference",
         "Causal reasoning is increasingly used in Reinforcement Learning (RL) to improve the learning process in several dimensions: efficacy of learned policies, efficiency of convergence, generalisation capabilities, safety and interpretability of behaviour. However, applications of causal reasoning to Multi-Agent RL (MARL) are still mostly unexplored. In this paper, we take the first step in investigating the opportunities and challenges of applying causal reasoning in MARL. We measure the impact of a simple form of causal augmentation in state-of-the-art MARL scenarios increasingly requiring cooperation, and with state-of-the-art MARL algorithms exploiting various degrees of collaboration between agents. Then, we discuss the positive as well as negative results achieved, giving us the chance to outline the areas where further research may help to successfully transfer causal RL to the multi-agent setting.",
         "2025",
         "2025-03-22",
         "10.48550/arxiv.2503.17803",
         "arXiv.org",
         null,
         "https://arxiv.org",
         "['Computer Science', 'Mathematics']",
         "['Computer Science', 'Mathematics']",
         "1",
         "0",
         "57",
         "[]",
         "['2277024426', '2255349359', '2261989530']",
         "['Giovanni Briglia', 'Stefano Mariani', 'Franco Zambonelli']",
         "https://www.semanticscholar.org/paper/1dce46450c1a08aa4cc0da964162733b67909fc0",
         null,
         "Causal Reinforcement Learning | Causal RL",
         "False",
         "False",
         "False",
         "False",
         null
        ],
        [
         "46",
         "1e6342fec0abb57fa9008d5589219da6c4d0f71e",
         "Model-based Reinforcement Learning for Predictions and Control for Limit Order Books",
         "We build a profitable electronic trading agent with Reinforcement Learning that places buy and sell orders in the stock market. An environment model is built only with historical observational data, and the RL agent learns the trading policy by interacting with the environment model instead of with the real-market to minimize the risk and potential monetary loss. Trained in unsupervised and self-supervised fashion, our environment model learned a temporal and causal representation of the market in latent space through deep neural networks. We demonstrate that the trading policy trained entirely within the environment model can be transferred back into the real market and maintain its profitability. We believe that this environment model can serve as a robust simulator that predicts market movement as well as trade impact for further studies.",
         "2019",
         "2019-10-09",
         "10.48550/arxiv.1910.03743",
         "arXiv.org",
         null,
         "https://arxiv.org",
         "['Computer Science']",
         "['Computer Science']",
         "25",
         "1",
         "38",
         "['1fd4694e7c2d9c872a427d50e81b5475056de6bc', 'ba9bfeff69fcf260c3f77f8af633728910d21c90', '2eeace98cf3c105a8d37884dc8d33c50ae4b7ddb', '5d4ebbaa1ef8feeac885e2869f45c0276c18834f', 'e87532f456571b3dc88a583fe9873b68d8e28a26', 'ee9893ff2aa325ff3c9920f247436c514fd8b512', '085870597c8a8421390d6590425003a13deefdd4', 'a13a070940d48bce111f31ba9c0233884a17ae62', '45c9794aefa4d266cd0c40511336fed6883d13c3', 'ff332c21562c87cab5891d495b7d0956f2d9228b', '37d522c6b4b9c1d9244783658012681e8af0ef44', 'c55462ce08d583cbe2bad8af6f4c9487e91c4b9a', '8a36eb02c398c6225e8afe122619ac3c0bc251bf', 'f1d33d85efc0f9e3de8686bd12e9250cebd5fcda', '3d96070accf20db30427802da94e51164b931afd', '37fa040ec0c4bc1b85f3ca2929445f3229ed7f72', '716776b39660f9e13859fa79790eb416d826fff3', '2ce382de24b859862aa87e53cce4e97c24591439', '69e76e16740ed69f4dc55361a3d319ac2f1293dd', '3b9732bb07dc99bde5e1f9f75251c6ea5039373e', '024006d4c2a89f7acacc6e4438d156525b60a98f', 'e4257bc131c36504a04382290cbc27ca8bb27813', '340f48901f72278f6bf78a04ee5b01df208cc508', '569339594ac5a39ca7f5fd88eee747688718f2c5', 'a4cbdacc182a589cbdd371882bc271e226711221', '235a9b0733accac54785ec272fcba0c07f4e8b0f', '3243c8bd06778b1265e43c03f54111a5a079d73b', '2d2d3a8a9c467877fbbde09dd0817a201db91469', '922864ede84bc49be4ac676951278a9b568b6383', None, None, None, None, None, 'd3a223ee8217f9dc00e725e5b7d63a43daaf1466', '4c915c1eecb217c123a36dc6d3ce52d12c742614', '65ae46eba524aea3766e91bd1005453c87af8ea2', 'd4ca18249446328c86d9da295a21c679aea1ed77']",
         "['2109394061', '2146016461', '1718611', '35204413']",
         "['Haoran Wei', 'Yuanbo Wang', 'L. Mangu', 'Keith S. Decker']",
         "https://www.semanticscholar.org/paper/1e6342fec0abb57fa9008d5589219da6c4d0f71e",
         null,
         "Causal Reinforcement Learning | Causal RL",
         "True",
         "False",
         "False",
         "False",
         "https://openalex.org/W2979430717"
        ],
        [
         "47",
         "1e85d4333f20ab00f466f3ad76a8003e12915ed0",
         "A Converse Bound Characterization on Zero-Delay Indirect RDF for Vector Gauss-Markov Sources.",
         "This work considers a zero-delay remote source coding problem where a hidden source modeled as a time-invariant vector-valued Gauss-Markov process is partially observed to an encoder whereas the performance criterion is the long-term mean squared-error (MSE) distortion between the hidden process and the reconstructed process.",
         "2020",
         "2020-01-12",
         "nan",
         "arXiv (Cornell University)",
         null,
         null,
         "['Mathematics']",
         "['Mathematics']",
         "0",
         "0",
         "33",
         "[]",
         "['145657810', '1707959']",
         "['Photios A. Stavrou', 'M. Skoglund']",
         "https://www.semanticscholar.org/paper/1e85d4333f20ab00f466f3ad76a8003e12915ed0",
         null,
         "Causal Reinforcement Learning | Causal RL",
         "False",
         "True",
         "False",
         "False",
         "https://openalex.org/W3172359402"
        ],
        [
         "48",
         "1ea0a4c86eaf441a6aafe6b52213985cd06304d9",
         "When Should Reinforcement Learning Use Causal Reasoning?",
         "Using the theory of causal graphs, it is shown formally that when the behavioral policy is executable by the learning agent, conditional probabilities are causal, and can be used to estimate expected rewards as done in traditional RL, however, when the behavioral policy is not executable by the learning agent, conditional probabilities may be confounded and provide misleading estimates of expected rewards.",
         "2025",
         null,
         "10.1145/3641289",
         "Trans. Mach. Learn. Res.",
         null,
         null,
         "['Computer Science']",
         "['Computer Science']",
         "0",
         "0",
         "40",
         "[]",
         "['2285668686', '2245466516']",
         "['Oliver Schulte', 'Pascal Poupart']",
         "https://www.semanticscholar.org/paper/1ea0a4c86eaf441a6aafe6b52213985cd06304d9",
         null,
         "Causal Reinforcement Learning | Causal RL",
         "True",
         "False",
         "False",
         "False",
         "https://openalex.org/W4391136507"
        ],
        [
         "49",
         "1ef902af4360bbab861ad9815f74299f01eab121",
         "Deep RL-Based Time Scheduling and Power Allocation in EH Relay Communication Networks",
         "Powering relays with harvested renewable ambient energy has been emerging as a promising solution to reduce the on-grid energy consumption and greenhouse gas emissions in green relaying communication networks. In this paper, we study the joint time scheduling and power allocation problem for the Decode-and-Forward energy-harvesting relay communication network. Particularly, our goal is to maximize the end-to-end throughput by a deadline subject to the finite data and energy storage. Due to the multi-slot optimization, the traditional deep reinforcement learning (RL) framework cannot be directly applied to obtain the optimal solution of maximizing the end-to-end throughput by a deadline in the online manner. To this end, we explore a novel deep reinforcement learning framework consisting of multiple computation units to obtain the online time scheduling and power allocation based on the current causal knowledge of energy arrivals and channel fading at each time slot. Simulation results show that the proposed deep reinforcement learning based algorithm can achieve more than 90% of maximum end-to-end throughput.",
         "2019",
         "2019-05-01",
         "10.1109/icc.2019.8761525",
         "ICC 2019 - 2019 IEEE International Conference on Communications (ICC)",
         null,
         null,
         "['Computer Science']",
         "['Computer Science']",
         "4",
         "0",
         "2",
         "[]",
         "['144331823', '51453449', '2112521186', '2108185380']",
         "['L. Qian', 'Anqi Feng', 'Xu Feng', 'Yuan Wu']",
         "https://www.semanticscholar.org/paper/1ef902af4360bbab861ad9815f74299f01eab121",
         null,
         "Causal Reinforcement Learning | Causal RL",
         "False",
         "False",
         "False",
         "False",
         null
        ]
       ],
       "shape": {
        "columns": 25,
        "rows": 2055
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>year</th>\n",
       "      <th>publicationDate</th>\n",
       "      <th>doi</th>\n",
       "      <th>venue</th>\n",
       "      <th>venue_type</th>\n",
       "      <th>venue_url</th>\n",
       "      <th>fields_of_study</th>\n",
       "      <th>...</th>\n",
       "      <th>author_id</th>\n",
       "      <th>authors</th>\n",
       "      <th>s2_url</th>\n",
       "      <th>open_access_pdf</th>\n",
       "      <th>query</th>\n",
       "      <th>doi_from_openalex</th>\n",
       "      <th>venue_from_openalex</th>\n",
       "      <th>abstract_from_openalex</th>\n",
       "      <th>year_from_openalex</th>\n",
       "      <th>openalex_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00b75f61f8bd3246fff75f84d852ba3e80d5338e</td>\n",
       "      <td>Applications of information Nonanticipative Ra...</td>\n",
       "      <td>The objective of this paper is to further inve...</td>\n",
       "      <td>2014</td>\n",
       "      <td>2014-01-22</td>\n",
       "      <td>10.1109/isit.2014.6875397</td>\n",
       "      <td>2014 IEEE International Symposium on Informati...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['Mathematics', 'Computer Science']</td>\n",
       "      <td>...</td>\n",
       "      <td>['145657810', '2081852', '1745427']</td>\n",
       "      <td>['Photios A. Stavrou', 'C. Kourtellaris', 'C. ...</td>\n",
       "      <td>https://www.semanticscholar.org/paper/00b75f61...</td>\n",
       "      <td>http://arxiv.org/pdf/1401.5828</td>\n",
       "      <td>Causal Reinforcement Learning | Causal RL</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01befcd360d36d520f595b34d5d26e37e0ac16f3</td>\n",
       "      <td>Explainable Agency in Reinforcement Learning A...</td>\n",
       "      <td>This thesis explores how reinforcement learnin...</td>\n",
       "      <td>2020</td>\n",
       "      <td>2020-04-03</td>\n",
       "      <td>10.1609/aaai.v34i10.7134</td>\n",
       "      <td>AAAI Conference on Artificial Intelligence</td>\n",
       "      <td>conference</td>\n",
       "      <td>http://www.aaai.org/</td>\n",
       "      <td>['Computer Science']</td>\n",
       "      <td>...</td>\n",
       "      <td>['9303604']</td>\n",
       "      <td>['Prashan Madumal']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/01befcd3...</td>\n",
       "      <td>https://doi.org/10.1609/aaai.v34i10.7134</td>\n",
       "      <td>Causal Reinforcement Learning | Causal RL</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01e9241dbb9eaca99b86468bb079f4b631b71671</td>\n",
       "      <td>Causal prompting model-based offline reinforce...</td>\n",
       "      <td>Model-based offline Reinforcement Learning (RL...</td>\n",
       "      <td>2024</td>\n",
       "      <td>2024-06-03</td>\n",
       "      <td>10.48550/arxiv.2406.01065</td>\n",
       "      <td>arXiv.org</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://arxiv.org</td>\n",
       "      <td>['Computer Science']</td>\n",
       "      <td>...</td>\n",
       "      <td>['2116329956', '2153979217', '2303466987', '22...</td>\n",
       "      <td>['Xuehui Yu', 'Yi Guan', 'Rujia Shen', 'Xin Li...</td>\n",
       "      <td>https://www.semanticscholar.org/paper/01e9241d...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Causal Reinforcement Learning | Causal RL</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>026dc8d3cbb360bdd12d19c924bc633221c9b423</td>\n",
       "      <td>Learning Causal Overhypotheses through Explora...</td>\n",
       "      <td>Despite recent progress in reinforcement learn...</td>\n",
       "      <td>2022</td>\n",
       "      <td>2022-02-21</td>\n",
       "      <td>10.48550/arxiv.2202.10430</td>\n",
       "      <td>CLEaR</td>\n",
       "      <td>conference</td>\n",
       "      <td>http://www.jolace.com/publications/clear/</td>\n",
       "      <td>['Computer Science']</td>\n",
       "      <td>...</td>\n",
       "      <td>['8519553', '2150491107', '39229748', '1525028...</td>\n",
       "      <td>['Eliza Kosoy', 'Adrian Liu', 'Jasmine Collins...</td>\n",
       "      <td>https://www.semanticscholar.org/paper/026dc8d3...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Causal Reinforcement Learning | Causal RL</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>https://openalex.org/W4221153082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0348b36927f740b82f51afcd1c35cae8386bc336</td>\n",
       "      <td>Segmented Encoding for Sim2Real of RL-based En...</td>\n",
       "      <td>Among the challenges in the recent research of...</td>\n",
       "      <td>2022</td>\n",
       "      <td>2022-06-05</td>\n",
       "      <td>10.1109/iv51971.2022.9827374</td>\n",
       "      <td>2022 IEEE Intelligent Vehicles Symposium (IV)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['Computer Science']</td>\n",
       "      <td>...</td>\n",
       "      <td>['47238664', '39530824', '2179287901', '673452...</td>\n",
       "      <td>['Seung H. Chung', 'S. Kong', 'S. Cho', 'I. M....</td>\n",
       "      <td>https://www.semanticscholar.org/paper/0348b369...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Causal Reinforcement Learning | Causal RL</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2050</th>\n",
       "      <td>97372d4647c90b9ead00e26c249c86e5bb435614</td>\n",
       "      <td>Intelligent Algorithms for Coordinated Control...</td>\n",
       "      <td>\\n In an effort to lessen the impact of cities...</td>\n",
       "      <td>2025</td>\n",
       "      <td>2025-01-01</td>\n",
       "      <td>10.1115/1.4068494</td>\n",
       "      <td>ASME Open Journal of Engineering</td>\n",
       "      <td>journal</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>['2361199578', '2343294431', '2293586726', '23...</td>\n",
       "      <td>['Yanfang Liu', 'Songling Pang', 'Ruien Zhang'...</td>\n",
       "      <td>https://www.semanticscholar.org/paper/97372d46...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(Safe Reinforcement Learning | Robust Reinforc...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2051</th>\n",
       "      <td>b25c560cbf5e997c4be5909877243ba9662b71c4</td>\n",
       "      <td>Research on Optimization Configuration of Urba...</td>\n",
       "      <td>With the deepening of the “dual carbon” policy...</td>\n",
       "      <td>2024</td>\n",
       "      <td>2024-10-25</td>\n",
       "      <td>10.1109/icemce64157.2024.10862876</td>\n",
       "      <td>2024 8th International Conference on Electrica...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>['2345474608', '2345286608', '2345016098', '23...</td>\n",
       "      <td>['Jun Jia', 'Dong Liu', 'Ligang Ge', 'Fanyi Lu']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/b25c560c...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(Safe Reinforcement Learning | Robust Reinforc...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2052</th>\n",
       "      <td>bb0c2063aa407119dbd8d128806f98fffdab015c</td>\n",
       "      <td>Deep Reinforcement Learning for Adaptive Optim...</td>\n",
       "      <td>This work focused on improving the control sch...</td>\n",
       "      <td>2024</td>\n",
       "      <td>2024-06-04</td>\n",
       "      <td>10.1109/gpecom61896.2024.10582714</td>\n",
       "      <td>Global Power, Energy and Communication Conference</td>\n",
       "      <td>conference</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>['2223164613', '152709525']</td>\n",
       "      <td>['Richard Wiencek', 'Sagnika Ghosh']</td>\n",
       "      <td>https://www.semanticscholar.org/paper/bb0c2063...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(Safe Reinforcement Learning | Robust Reinforc...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2053</th>\n",
       "      <td>c29f7df6d625d5c1caf277e55182dfc426603cdc</td>\n",
       "      <td>State Predictive Control of Modular SMES Magne...</td>\n",
       "      <td>Modular superconducting magnetic energy storag...</td>\n",
       "      <td>2022</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.1109/tasc.2022.3148682</td>\n",
       "      <td>IEEE transactions on applied superconductivity</td>\n",
       "      <td>journal</td>\n",
       "      <td>http://ieeexplore.ieee.org/servlet/opac?punumb...</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>['2116459274', '2109351453', '2119110951', '12...</td>\n",
       "      <td>['Zitong Zhang', 'Jing Shi', 'Shuqiang Guo', '...</td>\n",
       "      <td>https://www.semanticscholar.org/paper/c29f7df6...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(Safe Reinforcement Learning | Robust Reinforc...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2054</th>\n",
       "      <td>d1ff3d3098f7a249e34efce67534c19a4b8df168</td>\n",
       "      <td>Prioritized Replay Dueling DDQN Based Grid-Edg...</td>\n",
       "      <td>This paper develops a new prioritized replay d...</td>\n",
       "      <td>2021</td>\n",
       "      <td>2021-11-01</td>\n",
       "      <td>10.1109/tsg.2021.3099133</td>\n",
       "      <td>IEEE Transactions on Smart Grid</td>\n",
       "      <td>journal</td>\n",
       "      <td>http://ieeexplore.ieee.org/servlet/opac?punumb...</td>\n",
       "      <td>['Computer Science']</td>\n",
       "      <td>...</td>\n",
       "      <td>['2149627327', '152891773', '2023394', '497222...</td>\n",
       "      <td>['Hang Song', 'You-bo Liu', 'Junbo Zhao', 'Jun...</td>\n",
       "      <td>https://www.semanticscholar.org/paper/d1ff3d30...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(Safe Reinforcement Learning | Robust Reinforc...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2055 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            id  \\\n",
       "0     00b75f61f8bd3246fff75f84d852ba3e80d5338e   \n",
       "1     01befcd360d36d520f595b34d5d26e37e0ac16f3   \n",
       "2     01e9241dbb9eaca99b86468bb079f4b631b71671   \n",
       "3     026dc8d3cbb360bdd12d19c924bc633221c9b423   \n",
       "4     0348b36927f740b82f51afcd1c35cae8386bc336   \n",
       "...                                        ...   \n",
       "2050  97372d4647c90b9ead00e26c249c86e5bb435614   \n",
       "2051  b25c560cbf5e997c4be5909877243ba9662b71c4   \n",
       "2052  bb0c2063aa407119dbd8d128806f98fffdab015c   \n",
       "2053  c29f7df6d625d5c1caf277e55182dfc426603cdc   \n",
       "2054  d1ff3d3098f7a249e34efce67534c19a4b8df168   \n",
       "\n",
       "                                                  title  \\\n",
       "0     Applications of information Nonanticipative Ra...   \n",
       "1     Explainable Agency in Reinforcement Learning A...   \n",
       "2     Causal prompting model-based offline reinforce...   \n",
       "3     Learning Causal Overhypotheses through Explora...   \n",
       "4     Segmented Encoding for Sim2Real of RL-based En...   \n",
       "...                                                 ...   \n",
       "2050  Intelligent Algorithms for Coordinated Control...   \n",
       "2051  Research on Optimization Configuration of Urba...   \n",
       "2052  Deep Reinforcement Learning for Adaptive Optim...   \n",
       "2053  State Predictive Control of Modular SMES Magne...   \n",
       "2054  Prioritized Replay Dueling DDQN Based Grid-Edg...   \n",
       "\n",
       "                                               abstract  year publicationDate  \\\n",
       "0     The objective of this paper is to further inve...  2014      2014-01-22   \n",
       "1     This thesis explores how reinforcement learnin...  2020      2020-04-03   \n",
       "2     Model-based offline Reinforcement Learning (RL...  2024      2024-06-03   \n",
       "3     Despite recent progress in reinforcement learn...  2022      2022-02-21   \n",
       "4     Among the challenges in the recent research of...  2022      2022-06-05   \n",
       "...                                                 ...   ...             ...   \n",
       "2050  \\n In an effort to lessen the impact of cities...  2025      2025-01-01   \n",
       "2051  With the deepening of the “dual carbon” policy...  2024      2024-10-25   \n",
       "2052  This work focused on improving the control sch...  2024      2024-06-04   \n",
       "2053  Modular superconducting magnetic energy storag...  2022             NaN   \n",
       "2054  This paper develops a new prioritized replay d...  2021      2021-11-01   \n",
       "\n",
       "                                    doi  \\\n",
       "0             10.1109/isit.2014.6875397   \n",
       "1              10.1609/aaai.v34i10.7134   \n",
       "2             10.48550/arxiv.2406.01065   \n",
       "3             10.48550/arxiv.2202.10430   \n",
       "4          10.1109/iv51971.2022.9827374   \n",
       "...                                 ...   \n",
       "2050                  10.1115/1.4068494   \n",
       "2051  10.1109/icemce64157.2024.10862876   \n",
       "2052  10.1109/gpecom61896.2024.10582714   \n",
       "2053          10.1109/tasc.2022.3148682   \n",
       "2054           10.1109/tsg.2021.3099133   \n",
       "\n",
       "                                                  venue  venue_type  \\\n",
       "0     2014 IEEE International Symposium on Informati...         NaN   \n",
       "1            AAAI Conference on Artificial Intelligence  conference   \n",
       "2                                             arXiv.org         NaN   \n",
       "3                                                 CLEaR  conference   \n",
       "4         2022 IEEE Intelligent Vehicles Symposium (IV)         NaN   \n",
       "...                                                 ...         ...   \n",
       "2050                   ASME Open Journal of Engineering     journal   \n",
       "2051  2024 8th International Conference on Electrica...         NaN   \n",
       "2052  Global Power, Energy and Communication Conference  conference   \n",
       "2053     IEEE transactions on applied superconductivity     journal   \n",
       "2054                    IEEE Transactions on Smart Grid     journal   \n",
       "\n",
       "                                              venue_url  \\\n",
       "0                                                   NaN   \n",
       "1                                  http://www.aaai.org/   \n",
       "2                                     https://arxiv.org   \n",
       "3             http://www.jolace.com/publications/clear/   \n",
       "4                                                   NaN   \n",
       "...                                                 ...   \n",
       "2050                                                NaN   \n",
       "2051                                                NaN   \n",
       "2052                                                NaN   \n",
       "2053  http://ieeexplore.ieee.org/servlet/opac?punumb...   \n",
       "2054  http://ieeexplore.ieee.org/servlet/opac?punumb...   \n",
       "\n",
       "                          fields_of_study  ...  \\\n",
       "0     ['Mathematics', 'Computer Science']  ...   \n",
       "1                    ['Computer Science']  ...   \n",
       "2                    ['Computer Science']  ...   \n",
       "3                    ['Computer Science']  ...   \n",
       "4                    ['Computer Science']  ...   \n",
       "...                                   ...  ...   \n",
       "2050                                   []  ...   \n",
       "2051                                   []  ...   \n",
       "2052                                   []  ...   \n",
       "2053                                   []  ...   \n",
       "2054                 ['Computer Science']  ...   \n",
       "\n",
       "                                              author_id  \\\n",
       "0                   ['145657810', '2081852', '1745427']   \n",
       "1                                           ['9303604']   \n",
       "2     ['2116329956', '2153979217', '2303466987', '22...   \n",
       "3     ['8519553', '2150491107', '39229748', '1525028...   \n",
       "4     ['47238664', '39530824', '2179287901', '673452...   \n",
       "...                                                 ...   \n",
       "2050  ['2361199578', '2343294431', '2293586726', '23...   \n",
       "2051  ['2345474608', '2345286608', '2345016098', '23...   \n",
       "2052                        ['2223164613', '152709525']   \n",
       "2053  ['2116459274', '2109351453', '2119110951', '12...   \n",
       "2054  ['2149627327', '152891773', '2023394', '497222...   \n",
       "\n",
       "                                                authors  \\\n",
       "0     ['Photios A. Stavrou', 'C. Kourtellaris', 'C. ...   \n",
       "1                                   ['Prashan Madumal']   \n",
       "2     ['Xuehui Yu', 'Yi Guan', 'Rujia Shen', 'Xin Li...   \n",
       "3     ['Eliza Kosoy', 'Adrian Liu', 'Jasmine Collins...   \n",
       "4     ['Seung H. Chung', 'S. Kong', 'S. Cho', 'I. M....   \n",
       "...                                                 ...   \n",
       "2050  ['Yanfang Liu', 'Songling Pang', 'Ruien Zhang'...   \n",
       "2051   ['Jun Jia', 'Dong Liu', 'Ligang Ge', 'Fanyi Lu']   \n",
       "2052               ['Richard Wiencek', 'Sagnika Ghosh']   \n",
       "2053  ['Zitong Zhang', 'Jing Shi', 'Shuqiang Guo', '...   \n",
       "2054  ['Hang Song', 'You-bo Liu', 'Junbo Zhao', 'Jun...   \n",
       "\n",
       "                                                 s2_url  \\\n",
       "0     https://www.semanticscholar.org/paper/00b75f61...   \n",
       "1     https://www.semanticscholar.org/paper/01befcd3...   \n",
       "2     https://www.semanticscholar.org/paper/01e9241d...   \n",
       "3     https://www.semanticscholar.org/paper/026dc8d3...   \n",
       "4     https://www.semanticscholar.org/paper/0348b369...   \n",
       "...                                                 ...   \n",
       "2050  https://www.semanticscholar.org/paper/97372d46...   \n",
       "2051  https://www.semanticscholar.org/paper/b25c560c...   \n",
       "2052  https://www.semanticscholar.org/paper/bb0c2063...   \n",
       "2053  https://www.semanticscholar.org/paper/c29f7df6...   \n",
       "2054  https://www.semanticscholar.org/paper/d1ff3d30...   \n",
       "\n",
       "                               open_access_pdf  \\\n",
       "0               http://arxiv.org/pdf/1401.5828   \n",
       "1     https://doi.org/10.1609/aaai.v34i10.7134   \n",
       "2                                          NaN   \n",
       "3                                          NaN   \n",
       "4                                          NaN   \n",
       "...                                        ...   \n",
       "2050                                       NaN   \n",
       "2051                                       NaN   \n",
       "2052                                       NaN   \n",
       "2053                                       NaN   \n",
       "2054                                       NaN   \n",
       "\n",
       "                                                  query doi_from_openalex  \\\n",
       "0             Causal Reinforcement Learning | Causal RL             False   \n",
       "1             Causal Reinforcement Learning | Causal RL             False   \n",
       "2             Causal Reinforcement Learning | Causal RL             False   \n",
       "3             Causal Reinforcement Learning | Causal RL              True   \n",
       "4             Causal Reinforcement Learning | Causal RL             False   \n",
       "...                                                 ...               ...   \n",
       "2050  (Safe Reinforcement Learning | Robust Reinforc...             False   \n",
       "2051  (Safe Reinforcement Learning | Robust Reinforc...             False   \n",
       "2052  (Safe Reinforcement Learning | Robust Reinforc...             False   \n",
       "2053  (Safe Reinforcement Learning | Robust Reinforc...             False   \n",
       "2054  (Safe Reinforcement Learning | Robust Reinforc...             False   \n",
       "\n",
       "     venue_from_openalex abstract_from_openalex year_from_openalex  \\\n",
       "0                  False                  False              False   \n",
       "1                  False                  False              False   \n",
       "2                  False                  False              False   \n",
       "3                  False                  False              False   \n",
       "4                  False                  False              False   \n",
       "...                  ...                    ...                ...   \n",
       "2050               False                  False              False   \n",
       "2051               False                  False              False   \n",
       "2052               False                  False              False   \n",
       "2053               False                  False              False   \n",
       "2054               False                  False              False   \n",
       "\n",
       "                           openalex_id  \n",
       "0                                  NaN  \n",
       "1                                  NaN  \n",
       "2                                  NaN  \n",
       "3     https://openalex.org/W4221153082  \n",
       "4                                  NaN  \n",
       "...                                ...  \n",
       "2050                               NaN  \n",
       "2051                               NaN  \n",
       "2052                               NaN  \n",
       "2053                               NaN  \n",
       "2054                               NaN  \n",
       "\n",
       "[2055 rows x 25 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254c4ac6",
   "metadata": {},
   "source": [
    "# 4. Build Papers.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1c418fde",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-29 11:22:44,999 | INFO | Papers.csv written: 2055 rows (dropped 0 duplicates)\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "paper_id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "title",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "abstract",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "year",
         "rawType": "Int64",
         "type": "integer"
        },
        {
         "name": "publication_date",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "doi",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "s2_url",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "open_access_pdf",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "venue",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "added_at",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "source",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "dbbff998-b56e-4bf0-8dc2-887d5f8ecff5",
       "rows": [
        [
         "0",
         "00b75f61f8bd3246fff75f84d852ba3e80d5338e",
         "Applications of information Nonanticipative Rate Distortion Function",
         "The objective of this paper is to further investigate various applications of information Nonanticipative Rate Distortion Function (NRDF) by discussing two working examples, the Binary Symmetric Markov Source with parameter p (BSMS(p)) with Hamming distance distortion, and the multidimensional partially observed Gaussian-Markov source. For the BSMS(p), we give the solution to the NRDF, and we use it to compute the Rate Loss (RL) of causal codes with respect to noncausal codes. For the multidimensional Gaussian-Markov source, we give the solution to the NRDF, we show its operational meaning via joint source-channel matching over a vector of parallel Gaussian channels, and we compute the RL of causal and zero-delay codes with respect to noncausal codes.",
         "2014",
         "2014-01-22",
         "10.1109/isit.2014.6875397",
         "https://www.semanticscholar.org/paper/00b75f61f8bd3246fff75f84d852ba3e80d5338e",
         "http://arxiv.org/pdf/1401.5828",
         "2014 IEEE International Symposium on Information Theory",
         "2025-08-29T09:21:57Z",
         "SemanticScholar"
        ],
        [
         "1",
         "01befcd360d36d520f595b34d5d26e37e0ac16f3",
         "Explainable Agency in Reinforcement Learning Agents",
         "This thesis explores how reinforcement learning (RL) agents can provide explanations for their actions and behaviours. As humans, we build causal models to encode cause-effect relations of events and use these to explain why events happen. Taking inspiration from cognitive psychology and social science literature, I build causal explanation models and explanation dialogue models for RL agents. By mimicking human-like explanation models, these agents can provide explanations that are natural and intuitive to humans.",
         "2020",
         "2020-04-03",
         "10.1609/aaai.v34i10.7134",
         "https://www.semanticscholar.org/paper/01befcd360d36d520f595b34d5d26e37e0ac16f3",
         "https://doi.org/10.1609/aaai.v34i10.7134",
         "AAAI Conference on Artificial Intelligence",
         "2025-08-29T09:21:57Z",
         "SemanticScholar"
        ]
       ],
       "shape": {
        "columns": 11,
        "rows": 2
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper_id</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>year</th>\n",
       "      <th>publication_date</th>\n",
       "      <th>doi</th>\n",
       "      <th>s2_url</th>\n",
       "      <th>open_access_pdf</th>\n",
       "      <th>venue</th>\n",
       "      <th>added_at</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00b75f61f8bd3246fff75f84d852ba3e80d5338e</td>\n",
       "      <td>Applications of information Nonanticipative Ra...</td>\n",
       "      <td>The objective of this paper is to further inve...</td>\n",
       "      <td>2014</td>\n",
       "      <td>2014-01-22</td>\n",
       "      <td>10.1109/isit.2014.6875397</td>\n",
       "      <td>https://www.semanticscholar.org/paper/00b75f61...</td>\n",
       "      <td>http://arxiv.org/pdf/1401.5828</td>\n",
       "      <td>2014 IEEE International Symposium on Informati...</td>\n",
       "      <td>2025-08-29T09:21:57Z</td>\n",
       "      <td>SemanticScholar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01befcd360d36d520f595b34d5d26e37e0ac16f3</td>\n",
       "      <td>Explainable Agency in Reinforcement Learning A...</td>\n",
       "      <td>This thesis explores how reinforcement learnin...</td>\n",
       "      <td>2020</td>\n",
       "      <td>2020-04-03</td>\n",
       "      <td>10.1609/aaai.v34i10.7134</td>\n",
       "      <td>https://www.semanticscholar.org/paper/01befcd3...</td>\n",
       "      <td>https://doi.org/10.1609/aaai.v34i10.7134</td>\n",
       "      <td>AAAI Conference on Artificial Intelligence</td>\n",
       "      <td>2025-08-29T09:21:57Z</td>\n",
       "      <td>SemanticScholar</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   paper_id  \\\n",
       "0  00b75f61f8bd3246fff75f84d852ba3e80d5338e   \n",
       "1  01befcd360d36d520f595b34d5d26e37e0ac16f3   \n",
       "\n",
       "                                               title  \\\n",
       "0  Applications of information Nonanticipative Ra...   \n",
       "1  Explainable Agency in Reinforcement Learning A...   \n",
       "\n",
       "                                            abstract  year publication_date  \\\n",
       "0  The objective of this paper is to further inve...  2014       2014-01-22   \n",
       "1  This thesis explores how reinforcement learnin...  2020       2020-04-03   \n",
       "\n",
       "                         doi  \\\n",
       "0  10.1109/isit.2014.6875397   \n",
       "1   10.1609/aaai.v34i10.7134   \n",
       "\n",
       "                                              s2_url  \\\n",
       "0  https://www.semanticscholar.org/paper/00b75f61...   \n",
       "1  https://www.semanticscholar.org/paper/01befcd3...   \n",
       "\n",
       "                            open_access_pdf  \\\n",
       "0            http://arxiv.org/pdf/1401.5828   \n",
       "1  https://doi.org/10.1609/aaai.v34i10.7134   \n",
       "\n",
       "                                               venue              added_at  \\\n",
       "0  2014 IEEE International Symposium on Informati...  2025-08-29T09:21:57Z   \n",
       "1         AAAI Conference on Artificial Intelligence  2025-08-29T09:21:57Z   \n",
       "\n",
       "            source  \n",
       "0  SemanticScholar  \n",
       "1  SemanticScholar  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Construct Papers.csv as the core node file.\n",
    "# Columns chosen to balance graph normalization with convenience for analytics.\n",
    "\n",
    "papers = pd.DataFrame({\n",
    "    \"paper_id\": df[\"id\"].astype(str),\n",
    "    \"title\": df[\"title\"].fillna(\"\"),\n",
    "    \"abstract\": df[\"abstract\"].fillna(\"\"),\n",
    "    \"year\": df[\"year\"].astype(\"Int64\"),\n",
    "    \"publication_date\": df[\"publicationDate\"].fillna(\"\"),\n",
    "    \"doi\": df[\"doi\"].fillna(\"\"),\n",
    "    \"s2_url\": df[\"s2_url\"].fillna(\"\"),\n",
    "    \"open_access_pdf\": df[\"open_access_pdf\"].astype(str).replace(\"nan\",\"\"),\n",
    "    \"venue\": df[\"venue\"].fillna(\"\"),\n",
    "    \"added_at\": ADDED_AT,\n",
    "    \"source\": \"SemanticScholar\"\n",
    "})\n",
    "\n",
    "# Deduplicate strictly on paper_id\n",
    "before = len(papers)\n",
    "papers = papers.drop_duplicates(subset=[\"paper_id\"])\n",
    "after = len(papers)\n",
    "\n",
    "papers.to_csv(PAPERS_CSV, index=False)\n",
    "logger.info(\"Papers.csv written: %d rows (dropped %d duplicates)\", after, before - after)\n",
    "papers.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841a589c",
   "metadata": {},
   "source": [
    "# 5. Build Queries.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "56facedf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-29 11:23:25,942 | INFO | Queries.csv written: 8 rows\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "query_id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "name",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "added_at",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "ebf842ac-f80a-493d-aa72-77da059e74af",
       "rows": [
        [
         "977",
         "Q:explainable-ai-xai-smart-grid-grid-stability",
         "(Explainable AI | XAI) + (Smart Grid | Grid Stability)",
         "2025-08-29T09:21:57Z"
        ],
        [
         "1996",
         "Q:interpretable-reinforcement-learning-explainable-reinforcement-learning-power-grid",
         "(Interpretable Reinforcement Learning | Explainable Reinforcement Learning\") + (Power Grid)",
         "2025-08-29T09:21:57Z"
        ],
        [
         "1041",
         "Q:reinforcement-learning-deep-reinforcement-learning-vehicle-to-grid-v2g-demand-response",
         "(Reinforcement Learning | Deep Reinforcement Learning) + (Vehicle-to-Grid | V2G | Demand Response)",
         "2025-08-29T09:21:57Z"
        ],
        [
         "2045",
         "Q:safe-reinforcement-learning-robust-reinforcement-learning-power-grid-ev-charging-smart-charging",
         "(Safe Reinforcement Learning | Robust Reinforcement Learning) + (Power Grid | EV Charging | Smart Charging)",
         "2025-08-29T09:21:57Z"
        ],
        [
         "1014",
         "Q:causal-inference-energy-systems-power-grid",
         "Causal Inference + (Energy Systems | Power Grid)",
         "2025-08-29T09:21:57Z"
        ],
        [
         "2043",
         "Q:causal-inference-vehicle-to-grid-ev-charging",
         "Causal Inference + (Vehicle-to-Grid | EV Charging)",
         "2025-08-29T09:21:57Z"
        ],
        [
         "0",
         "Q:causal-reinforcement-learning-causal-rl",
         "Causal Reinforcement Learning | Causal RL",
         "2025-08-29T09:21:57Z"
        ],
        [
         "403",
         "Q:reinforcement-learning-ev-charging-smart-charging",
         "Reinforcement Learning + (EV Charging | Smart Charging)",
         "2025-08-29T09:21:57Z"
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 8
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query_id</th>\n",
       "      <th>name</th>\n",
       "      <th>added_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>977</th>\n",
       "      <td>Q:explainable-ai-xai-smart-grid-grid-stability</td>\n",
       "      <td>(Explainable AI | XAI) + (Smart Grid | Grid St...</td>\n",
       "      <td>2025-08-29T09:21:57Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>Q:interpretable-reinforcement-learning-explain...</td>\n",
       "      <td>(Interpretable Reinforcement Learning | Explai...</td>\n",
       "      <td>2025-08-29T09:21:57Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1041</th>\n",
       "      <td>Q:reinforcement-learning-deep-reinforcement-le...</td>\n",
       "      <td>(Reinforcement Learning | Deep Reinforcement L...</td>\n",
       "      <td>2025-08-29T09:21:57Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2045</th>\n",
       "      <td>Q:safe-reinforcement-learning-robust-reinforce...</td>\n",
       "      <td>(Safe Reinforcement Learning | Robust Reinforc...</td>\n",
       "      <td>2025-08-29T09:21:57Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1014</th>\n",
       "      <td>Q:causal-inference-energy-systems-power-grid</td>\n",
       "      <td>Causal Inference + (Energy Systems | Power Grid)</td>\n",
       "      <td>2025-08-29T09:21:57Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2043</th>\n",
       "      <td>Q:causal-inference-vehicle-to-grid-ev-charging</td>\n",
       "      <td>Causal Inference + (Vehicle-to-Grid | EV Charg...</td>\n",
       "      <td>2025-08-29T09:21:57Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Q:causal-reinforcement-learning-causal-rl</td>\n",
       "      <td>Causal Reinforcement Learning | Causal RL</td>\n",
       "      <td>2025-08-29T09:21:57Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403</th>\n",
       "      <td>Q:reinforcement-learning-ev-charging-smart-cha...</td>\n",
       "      <td>Reinforcement Learning + (EV Charging | Smart ...</td>\n",
       "      <td>2025-08-29T09:21:57Z</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               query_id  \\\n",
       "977      Q:explainable-ai-xai-smart-grid-grid-stability   \n",
       "1996  Q:interpretable-reinforcement-learning-explain...   \n",
       "1041  Q:reinforcement-learning-deep-reinforcement-le...   \n",
       "2045  Q:safe-reinforcement-learning-robust-reinforce...   \n",
       "1014       Q:causal-inference-energy-systems-power-grid   \n",
       "2043     Q:causal-inference-vehicle-to-grid-ev-charging   \n",
       "0             Q:causal-reinforcement-learning-causal-rl   \n",
       "403   Q:reinforcement-learning-ev-charging-smart-cha...   \n",
       "\n",
       "                                                   name              added_at  \n",
       "977   (Explainable AI | XAI) + (Smart Grid | Grid St...  2025-08-29T09:21:57Z  \n",
       "1996  (Interpretable Reinforcement Learning | Explai...  2025-08-29T09:21:57Z  \n",
       "1041  (Reinforcement Learning | Deep Reinforcement L...  2025-08-29T09:21:57Z  \n",
       "2045  (Safe Reinforcement Learning | Robust Reinforc...  2025-08-29T09:21:57Z  \n",
       "1014   Causal Inference + (Energy Systems | Power Grid)  2025-08-29T09:21:57Z  \n",
       "2043  Causal Inference + (Vehicle-to-Grid | EV Charg...  2025-08-29T09:21:57Z  \n",
       "0             Causal Reinforcement Learning | Causal RL  2025-08-29T09:21:57Z  \n",
       "403   Reinforcement Learning + (EV Charging | Smart ...  2025-08-29T09:21:57Z  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a node file for queries so they can be connected later (Paper)-[:FROM_QUERY]->(Query).\n",
    "queries = df[\"query\"].fillna(\"\").astype(str)\n",
    "queries = queries[queries.str.len() > 0].drop_duplicates().sort_values()\n",
    "\n",
    "queries_df = pd.DataFrame({\n",
    "    \"query_id\": queries.apply(lambda s: \"Q:\" + slugify(s)),\n",
    "    \"name\": queries.values,\n",
    "    \"added_at\": ADDED_AT\n",
    "})\n",
    "\n",
    "queries_df.to_csv(QUERIES_CSV, index=False)\n",
    "logger.info(\"Queries.csv written: %d rows\", len(queries_df))\n",
    "queries_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72ec03d",
   "metadata": {},
   "source": [
    "# 6. Build FieldsOfStudy.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fcf0755a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-29 13:29:05,244 | INFO | FieldsOfStudy.csv written: 14 unique concepts\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "concept_id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "name",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "source",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "eead3710-1e18-4d23-9576-eaff2293eb63",
       "rows": [
        [
         "0",
         "F:art",
         "Art",
         "fields_of_study"
        ],
        [
         "1",
         "F:biology",
         "Biology",
         "fields_of_study"
        ],
        [
         "2",
         "F:business",
         "Business",
         "fields_of_study"
        ],
        [
         "3",
         "F:computer-science",
         "Computer Science",
         "fields_of_study"
        ],
        [
         "4",
         "F:economics",
         "Economics",
         "fields_of_study"
        ],
        [
         "5",
         "F:engineering",
         "Engineering",
         "fields_of_study"
        ],
        [
         "6",
         "F:environmental-science",
         "Environmental Science",
         "fields_of_study"
        ],
        [
         "7",
         "F:geography",
         "Geography",
         "fields_of_study"
        ],
        [
         "8",
         "F:mathematics",
         "Mathematics",
         "fields_of_study"
        ],
        [
         "9",
         "F:medicine",
         "Medicine",
         "fields_of_study"
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 10
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>concept_id</th>\n",
       "      <th>name</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>F:art</td>\n",
       "      <td>Art</td>\n",
       "      <td>fields_of_study</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>F:biology</td>\n",
       "      <td>Biology</td>\n",
       "      <td>fields_of_study</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>F:business</td>\n",
       "      <td>Business</td>\n",
       "      <td>fields_of_study</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>F:computer-science</td>\n",
       "      <td>Computer Science</td>\n",
       "      <td>fields_of_study</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>F:economics</td>\n",
       "      <td>Economics</td>\n",
       "      <td>fields_of_study</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>F:engineering</td>\n",
       "      <td>Engineering</td>\n",
       "      <td>fields_of_study</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>F:environmental-science</td>\n",
       "      <td>Environmental Science</td>\n",
       "      <td>fields_of_study</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>F:geography</td>\n",
       "      <td>Geography</td>\n",
       "      <td>fields_of_study</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>F:mathematics</td>\n",
       "      <td>Mathematics</td>\n",
       "      <td>fields_of_study</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>F:medicine</td>\n",
       "      <td>Medicine</td>\n",
       "      <td>fields_of_study</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                concept_id                   name           source\n",
       "0                    F:art                    Art  fields_of_study\n",
       "1                F:biology                Biology  fields_of_study\n",
       "2               F:business               Business  fields_of_study\n",
       "3       F:computer-science       Computer Science  fields_of_study\n",
       "4              F:economics              Economics  fields_of_study\n",
       "5            F:engineering            Engineering  fields_of_study\n",
       "6  F:environmental-science  Environmental Science  fields_of_study\n",
       "7              F:geography              Geography  fields_of_study\n",
       "8            F:mathematics            Mathematics  fields_of_study\n",
       "9               F:medicine               Medicine  fields_of_study"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -- strip wrappers like quotes/brackets and tidy whitespace --\n",
    "def _clean_label(s: str) -> str:\n",
    "    if not isinstance(s, str):\n",
    "        return \"\"\n",
    "    s = s.strip()\n",
    "    # remove a single pair of wrapping quotes if present\n",
    "    if (s.startswith(\"'\") and s.endswith(\"'\")) or (s.startswith('\"') and s.endswith('\"')):\n",
    "        s = s[1:-1].strip()\n",
    "    # remove any leading/trailing bracket/paren/brace/quotes noise\n",
    "    s = re.sub(r\"^[\\s\\[\\](){}'\\\"`]+|[\\s$begin:math:display$$end:math:display$(){}'\\\"`]+$\", \"\", s)\n",
    "    # collapse inner whitespace\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "fos_rows = []\n",
    "\n",
    "def collect_fos(series, source_name):\n",
    "    for raw in series.fillna(\"\").astype(str):\n",
    "        items = parse_list_field(raw) if raw else []\n",
    "        for it in items:\n",
    "            try:\n",
    "                obj = json.loads(it)\n",
    "                label = obj[\"name\"].strip() if isinstance(obj, dict) and \"name\" in obj else str(it).strip()\n",
    "            except Exception:\n",
    "                label = str(it).strip()\n",
    "            # Clean trailing punctuation\n",
    "            label = label.strip(\"[]'\\\" \").strip()\n",
    "            # Collapse inner whitespace\n",
    "            label = re.sub(r\"\\s+\", \" \", label)\n",
    "            if label:\n",
    "                fos_rows.append((label, source_name))\n",
    "\n",
    "collect_fos(df[\"fields_of_study\"], \"fields_of_study\")\n",
    "collect_fos(df[\"s2_fields_of_study\"], \"s2_fields_of_study\")\n",
    "\n",
    "tmp = pd.DataFrame(fos_rows, columns=[\"name\", \"source\"])\n",
    "\n",
    "# Deduplicate by name, preferring fields_of_study\n",
    "def pick_source(sources):\n",
    "    return \"fields_of_study\" if \"fields_of_study\" in sources.values else \"s2_fields_of_study\"\n",
    "\n",
    "fos_df = (\n",
    "    tmp.groupby(\"name\", as_index=False)\n",
    "       .agg(source=(\"source\", pick_source))\n",
    ")\n",
    "\n",
    "# Make concept_id\n",
    "fos_df[\"concept_id\"] = fos_df[\"name\"].apply(lambda n: f\"F:{slugify(n)}\")\n",
    "\n",
    "# Final DataFrame\n",
    "fos_df = fos_df[[\"concept_id\", \"name\", \"source\"]].sort_values(\"name\")\n",
    "\n",
    "fos_df.to_csv(FIELDS_CSV, index=False)\n",
    "logger.info(\"FieldsOfStudy.csv written: %d unique concepts\", len(fos_df))\n",
    "fos_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105a8056",
   "metadata": {},
   "source": [
    "# 7. Build Authors.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "42909c8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-29 13:48:57,676 | INFO | Authors.csv written: 7004 authors\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "author_id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "name",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "70736243-b143-460d-93dd-e91b3e3fb2fc",
       "rows": [
        [
         "803",
         "100508183",
         "Harry Kitsikopoulos"
        ],
        [
         "2353",
         "100560409",
         "M. Tuka"
        ],
        [
         "5270",
         "100638425",
         "N. Madonsela"
        ],
        [
         "195",
         "100667849",
         "S. S. Eshkevari"
        ],
        [
         "2503",
         "100702353",
         "S. Nengroo"
        ],
        [
         "4067",
         "100704713",
         "Y. Villarroel"
        ],
        [
         "8025",
         "100731317",
         "Mehdi Jabbari Zideh"
        ],
        [
         "685",
         "100847016",
         "Bruce Nagy"
        ],
        [
         "962",
         "10115554",
         "Jeremy Nixon"
        ],
        [
         "1003",
         "101181945",
         "Tridib Mukherjee"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 10
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author_id</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>803</th>\n",
       "      <td>100508183</td>\n",
       "      <td>Harry Kitsikopoulos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2353</th>\n",
       "      <td>100560409</td>\n",
       "      <td>M. Tuka</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5270</th>\n",
       "      <td>100638425</td>\n",
       "      <td>N. Madonsela</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>100667849</td>\n",
       "      <td>S. S. Eshkevari</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2503</th>\n",
       "      <td>100702353</td>\n",
       "      <td>S. Nengroo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4067</th>\n",
       "      <td>100704713</td>\n",
       "      <td>Y. Villarroel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8025</th>\n",
       "      <td>100731317</td>\n",
       "      <td>Mehdi Jabbari Zideh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>685</th>\n",
       "      <td>100847016</td>\n",
       "      <td>Bruce Nagy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>962</th>\n",
       "      <td>10115554</td>\n",
       "      <td>Jeremy Nixon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1003</th>\n",
       "      <td>101181945</td>\n",
       "      <td>Tridib Mukherjee</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      author_id                 name\n",
       "803   100508183  Harry Kitsikopoulos\n",
       "2353  100560409              M. Tuka\n",
       "5270  100638425         N. Madonsela\n",
       "195   100667849      S. S. Eshkevari\n",
       "2503  100702353           S. Nengroo\n",
       "4067  100704713        Y. Villarroel\n",
       "8025  100731317  Mehdi Jabbari Zideh\n",
       "685   100847016           Bruce Nagy\n",
       "962    10115554         Jeremy Nixon\n",
       "1003  101181945     Tridib Mukherjee"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build Authors.csv by aligning 'author_id' and 'authors' list-like columns.\n",
    "# If an 'author_id' is missing but a name exists, synthesize a stable id from the name (prefixed with 'name:').\n",
    "\n",
    "author_rows: List[Tuple[str, str]] = []\n",
    "\n",
    "def parse_authors(row: pd.Series) -> List[Tuple[str, str]]:\n",
    "    ids = parse_list_field(row.get(\"author_id\"))\n",
    "    names = parse_list_field(row.get(\"authors\"))\n",
    "    n = max(len(ids), len(names))\n",
    "    out = []\n",
    "    for i in range(n):\n",
    "        aid = ids[i] if i < len(ids) else \"\"\n",
    "        nm  = names[i] if i < len(names) else \"\"\n",
    "        aid = str(aid).strip(\"[]'\\\" \").strip()\n",
    "        nm  = str(nm).strip(\"[]'\\\" \").strip()\n",
    "        if not aid and nm:\n",
    "            aid = f\"name:{slugify(nm)}\"\n",
    "        if aid:\n",
    "            out.append((aid, nm))\n",
    "    return out\n",
    "\n",
    "for _, r in df.iterrows():\n",
    "    for aid, nm in parse_authors(r):\n",
    "        author_rows.append((aid, nm))\n",
    "\n",
    "authors_df = pd.DataFrame(author_rows, columns=[\"author_id\",\"name\"]).drop_duplicates().sort_values(\"author_id\")\n",
    "authors_df.to_csv(AUTHORS_CSV, index=False)\n",
    "logger.info(\"Authors.csv written: %d authors\", len(authors_df))\n",
    "authors_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aebd1258",
   "metadata": {},
   "source": [
    "# 8. Build Venues.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9051e4d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-29 13:58:16,260 | INFO | Venues.csv written: 932 venues\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "venue_id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "name",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "type",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "url",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "054cd794-1475-4823-bac1-2a97abab058a",
       "rows": [
        [
         "0",
         "V:2014-ieee-international-symposium-on-information-theory",
         "2014 IEEE International Symposium on Information Theory",
         "",
         ""
        ],
        [
         "1",
         "V:aaai-conference-on-artificial-intelligence",
         "AAAI Conference on Artificial Intelligence",
         "conference",
         "http://www.aaai.org/"
        ],
        [
         "2",
         "V:arxivorg",
         "arXiv.org",
         "",
         "https://arxiv.org"
        ],
        [
         "3",
         "V:clear",
         "CLEaR",
         "conference",
         "http://www.jolace.com/publications/clear/"
        ],
        [
         "4",
         "V:2022-ieee-intelligent-vehicles-symposium-iv",
         "2022 IEEE Intelligent Vehicles Symposium (IV)",
         "",
         ""
        ],
        [
         "6",
         "V:annals-of-statistics",
         "Annals of Statistics",
         "journal",
         "https://www.jstor.org/journal/annalsstatistics"
        ],
        [
         "7",
         "V:international-conference-on-learning-representations",
         "International Conference on Learning Representations",
         "conference",
         "https://iclr.cc/"
        ],
        [
         "8",
         "V:north-american-chapter-of-the-association-for-computational-linguistics",
         "North American Chapter of the Association for Computational Linguistics",
         "conference",
         "https://www.aclweb.org/portal/naacl"
        ],
        [
         "9",
         "V:the-european-physical-journal-c",
         "The European Physical Journal C",
         "",
         ""
        ],
        [
         "10",
         "V:neural-information-processing-systems",
         "Neural Information Processing Systems",
         "conference",
         "http://neurips.cc/"
        ]
       ],
       "shape": {
        "columns": 4,
        "rows": 10
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>venue_id</th>\n",
       "      <th>name</th>\n",
       "      <th>type</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>V:2014-ieee-international-symposium-on-informa...</td>\n",
       "      <td>2014 IEEE International Symposium on Informati...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>V:aaai-conference-on-artificial-intelligence</td>\n",
       "      <td>AAAI Conference on Artificial Intelligence</td>\n",
       "      <td>conference</td>\n",
       "      <td>http://www.aaai.org/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>V:arxivorg</td>\n",
       "      <td>arXiv.org</td>\n",
       "      <td></td>\n",
       "      <td>https://arxiv.org</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>V:clear</td>\n",
       "      <td>CLEaR</td>\n",
       "      <td>conference</td>\n",
       "      <td>http://www.jolace.com/publications/clear/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>V:2022-ieee-intelligent-vehicles-symposium-iv</td>\n",
       "      <td>2022 IEEE Intelligent Vehicles Symposium (IV)</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>V:annals-of-statistics</td>\n",
       "      <td>Annals of Statistics</td>\n",
       "      <td>journal</td>\n",
       "      <td>https://www.jstor.org/journal/annalsstatistics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>V:international-conference-on-learning-represe...</td>\n",
       "      <td>International Conference on Learning Represent...</td>\n",
       "      <td>conference</td>\n",
       "      <td>https://iclr.cc/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>V:north-american-chapter-of-the-association-fo...</td>\n",
       "      <td>North American Chapter of the Association for ...</td>\n",
       "      <td>conference</td>\n",
       "      <td>https://www.aclweb.org/portal/naacl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>V:the-european-physical-journal-c</td>\n",
       "      <td>The European Physical Journal C</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>V:neural-information-processing-systems</td>\n",
       "      <td>Neural Information Processing Systems</td>\n",
       "      <td>conference</td>\n",
       "      <td>http://neurips.cc/</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             venue_id  \\\n",
       "0   V:2014-ieee-international-symposium-on-informa...   \n",
       "1        V:aaai-conference-on-artificial-intelligence   \n",
       "2                                          V:arxivorg   \n",
       "3                                             V:clear   \n",
       "4       V:2022-ieee-intelligent-vehicles-symposium-iv   \n",
       "6                              V:annals-of-statistics   \n",
       "7   V:international-conference-on-learning-represe...   \n",
       "8   V:north-american-chapter-of-the-association-fo...   \n",
       "9                   V:the-european-physical-journal-c   \n",
       "10            V:neural-information-processing-systems   \n",
       "\n",
       "                                                 name        type  \\\n",
       "0   2014 IEEE International Symposium on Informati...               \n",
       "1          AAAI Conference on Artificial Intelligence  conference   \n",
       "2                                           arXiv.org               \n",
       "3                                               CLEaR  conference   \n",
       "4       2022 IEEE Intelligent Vehicles Symposium (IV)               \n",
       "6                                Annals of Statistics     journal   \n",
       "7   International Conference on Learning Represent...  conference   \n",
       "8   North American Chapter of the Association for ...  conference   \n",
       "9                     The European Physical Journal C               \n",
       "10              Neural Information Processing Systems  conference   \n",
       "\n",
       "                                               url  \n",
       "0                                                   \n",
       "1                             http://www.aaai.org/  \n",
       "2                                https://arxiv.org  \n",
       "3        http://www.jolace.com/publications/clear/  \n",
       "4                                                   \n",
       "6   https://www.jstor.org/journal/annalsstatistics  \n",
       "7                                 https://iclr.cc/  \n",
       "8              https://www.aclweb.org/portal/naacl  \n",
       "9                                                   \n",
       "10                              http://neurips.cc/  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build Venues.csv with a deterministic venue_id derived from the venue name.\n",
    "# Keep venue 'type' and 'url' as attributes (if provided).\n",
    "\n",
    "tmp = df[[\"venue\",\"venue_type\",\"venue_url\"]].copy()\n",
    "tmp[\"venue\"] = tmp[\"venue\"].fillna(\"\").astype(str).str.strip()\n",
    "tmp[\"venue_type\"] = tmp[\"venue_type\"].fillna(\"\").astype(str).str.strip()\n",
    "tmp[\"venue_url\"] = tmp[\"venue_url\"].fillna(\"\").astype(str).str.strip()\n",
    "\n",
    "# Only keep non-empty venue names\n",
    "tmp = tmp[tmp[\"venue\"] != \"\"]\n",
    "\n",
    "venues_df = tmp.drop_duplicates().copy()\n",
    "venues_df[\"venue_id\"] = venues_df[\"venue\"].apply(lambda s: \"V:\" + slugify(s))\n",
    "\n",
    "# Reorder and save\n",
    "venues_df = venues_df[[\"venue_id\",\"venue\",\"venue_type\",\"venue_url\"]].rename(columns={\n",
    "    \"venue\": \"name\",\n",
    "    \"venue_type\": \"type\",\n",
    "    \"venue_url\": \"url\"\n",
    "})\n",
    "venues_df.to_csv(VENUES_CSV, index=False)\n",
    "logger.info(\"Venues.csv written: %d venues\", len(venues_df))\n",
    "venues_df.head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "causal-rl-ev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
