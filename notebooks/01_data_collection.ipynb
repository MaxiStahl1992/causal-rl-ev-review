{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dba6a7c9",
   "metadata": {},
   "source": [
    "# 1.Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279a015d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported and environment loaded.\n"
     ]
    }
   ],
   "source": [
    "# Imports for data handling, API requests, and environment variables\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "print(\"Libraries imported and environment loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad4f200",
   "metadata": {},
   "source": [
    "# 2. Methodology and Objective\n",
    "Traditional Systematic Literature Reviews (SLRs) are often labor-intensive, time-consuming, and prone to error, particularly in rapidly evolving research fields Recent studies highlight significant gaps in the automation of critical phases such as data extraction, quality assessment, and data synthesis. To address these challenges, this study adopts a semi-automated pipeline that leverages Large Language Models (LLMs) and advanced Natural Language Processing (NLP) to enhance the efficiency and rigor of the review process.\n",
    "The approach is directly informed by recently proposed frameworks like PROMPTHEUS, which demonstrate the feasibility of an end-to-end automated SLR pipeline. The methodology consists of three main stages.\n",
    "- First, a **systematic search and screening** phase will use an LLM to generate a robust search query and Sentence-BERT embeddings to filter for the most relevant papers based on semantic similarity. \n",
    "- Second, a **data extraction and topic modeling** phase will employ the BERTopic algorithm to cluster the selected literature into coherent themes. This technique leverages UMAP for dimensionality reduction and HDBSCAN for clustering, providing a robust method for thematic analysis. \n",
    "- Finally, a **knowledge graph** will be constructed and analyzed to map the relationships between concepts, papers, and authors, providing a structural overview of the research landscape. This comprehensive, AI-assisted methodology aims to produce a systematic and data-driven analysis while significantly reducing the manual workload associated with traditional review methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28967fd4",
   "metadata": {},
   "source": [
    "# 3. Defining Search Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "be9a05f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the core search queries for our literature review on CRL for EV charging\n",
    "search_queries = {\n",
    "    \"crl_core\": '\"Causal Reinforcement Learning\" OR \"Causal RL\"',\n",
    "    \"rl_for_ev\": '\"Reinforcement Learning\" AND (\"EV Charging\" OR \"Smart Charging\")',\n",
    "    \"xai_for_grid\": '(\"Explainable AI\" OR \"XAI\") AND (\"Smart Grid\" OR \"Grid Stability\")',\n",
    "    \"causal_inference_energy\": '\"Causal Inference\" AND (\"Energy Systems\" OR \"Power Grid\")',\n",
    "    \"broader_rl_ev\": '(\"Reinforcement Learning\" OR \"Deep Reinforcement Learning\") AND (\"Vehicle-to-Grid\" OR \"V2G\" OR \"Demand Response\")',\n",
    "    \"interpretable_rl_grid\": '(\"Interpretable Reinforcement Learning\" OR \"Explainable Reinforcement Learning\") AND (\"Power Grid\")',\n",
    "    \"causal_v2g\": '(\"Causal Inference\") AND (\"Vehicle-to-Grid\" OR \"EV Charging\")'\n",
    "}\n",
    "\n",
    "semantic_scholar_search_queries = {\n",
    "    \"crl_core\": 'Causal Reinforcement Learning | Causal RL',\n",
    "    \"rl_for_ev\": 'Reinforcement Learning + (EV Charging | Smart Charging)',\n",
    "    \"xai_for_grid\": '(Explainable AI | XAI) + (Smart Grid | Grid Stability)',\n",
    "    \"causal_inference_energy\": 'Causal Inference + (Energy Systems | Power Grid)',\n",
    "    \"broader_rl_ev\": '(Reinforcement Learning | Deep Reinforcement Learning) + (Vehicle-to-Grid | V2G | Demand Response)',\n",
    "    \"interpretable_rl_grid\": '(Interpretable Reinforcement Learning | Explainable Reinforcement Learning\") + (Power Grid)',\n",
    "    \"causal_v2g\": 'Causal Inference + (Vehicle-to-Grid | EV Charging)'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f167fa9a",
   "metadata": {},
   "source": [
    "# 4. API Clients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc08b0f",
   "metadata": {},
   "source": [
    "### ArXiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "13e91445",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_arxiv_papers(query, max_results=200):\n",
    "    \"\"\"\n",
    "    Fetches paper metadata from the ArXiv API based on a search query.\n",
    "    It parses the XML response and returns a list of dictionaries.\n",
    "    \"\"\"\n",
    "    # The base URL for the ArXiv API\n",
    "    base_url = 'http://export.arxiv.org/api/query?'\n",
    "    \n",
    "    # Prepares the search query by replacing spaces with '+'\n",
    "    search_query = f'all:{query.replace(\" \", \"+\")}'\n",
    "    \n",
    "    # Constructs the full request URL with parameters\n",
    "    request_url = f'{base_url}search_query={search_query}&start=0&max_results={max_results}'\n",
    "    \n",
    "    try:\n",
    "        # Sends a GET request to the ArXiv API\n",
    "        response = requests.get(request_url)\n",
    "        # Raises an exception for bad status codes (like 404 or 500)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        # Parses the XML response from the request\n",
    "        root = ET.fromstring(response.content)\n",
    "        \n",
    "        # A list to store the metadata for each paper\n",
    "        papers = []\n",
    "        \n",
    "        # The Atom XML namespace used by ArXiv\n",
    "        namespace = {'atom': 'http://www.w3.org/2005/Atom'}\n",
    "        \n",
    "        # Iterates through each 'entry' tag in the XML, which corresponds to a paper\n",
    "        for entry in root.findall('atom:entry', namespace):\n",
    "            # Extracts the paper ID, title, and summary (abstract)\n",
    "            paper_id = entry.find('atom:id', namespace).text\n",
    "            title = entry.find('atom:title', namespace).text\n",
    "            summary = entry.find('atom:summary', namespace).text\n",
    "            published = entry.find('atom:published', namespace).text\n",
    "            updated = entry.find('atom:updated', namespace).text\n",
    "            \n",
    "            # Extracts all author names\n",
    "            authors = [author.find('atom:name', namespace).text for author in entry.findall('atom:author', namespace)]\n",
    "            \n",
    "            # Appends the extracted data as a dictionary to the list\n",
    "            papers.append({\n",
    "                'id': paper_id,\n",
    "                'title': title.strip(),\n",
    "                'summary': summary.strip(),\n",
    "                'authors': authors,\n",
    "                'year': updated or published,\n",
    "                'source': 'arxiv'\n",
    "            })\n",
    "            \n",
    "        return papers\n",
    "    \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        # Handles potential network errors\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe2abb6",
   "metadata": {},
   "source": [
    "### IEEE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7e271e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time # We need this to handle rate limiting\n",
    "\n",
    "def fetch_ieee_papers(query, max_total_records=500):\n",
    "    \"\"\"\n",
    "    Fetches paper metadata from the IEEE Xplore API, handling pagination.\n",
    "    It parses the JSON response and returns a list of dictionaries.\n",
    "    \"\"\"\n",
    "    # Retrieves the API key from the environment variables\n",
    "   # Retrieves the API key from the environment variables\n",
    "    api_key = os.getenv(\"API_KEY_IEEE\")\n",
    "    if not api_key:\n",
    "        print(\"Error: IEEE API key not found in .env file.\")\n",
    "        return []\n",
    "\n",
    "    # The correct base URL for the modern IEEE API\n",
    "    base_url = \"https://ieeexploreapi.ieee.org/api/v1/search/articles\"\n",
    "    \n",
    "    # Sets the headers to request JSON data, a best practice for REST APIs\n",
    "    headers = {'Accept': 'application/json'}\n",
    "    \n",
    "    # Sets the initial parameters for the first API call\n",
    "    params = {\n",
    "        'apikey': api_key,\n",
    "        'meta_data': query,\n",
    "        'rows_per_page': 200, # Max allowed per call is 200\n",
    "        'page_number': 1\n",
    "    }\n",
    "    \n",
    "    papers = []\n",
    "    \n",
    "    # Sets a flag for the first request to get total records\n",
    "    is_first_request = True\n",
    "    total_records = 0\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            # Sends the GET request to the IEEE API\n",
    "            response = requests.get(base_url, headers=headers, params=params)\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "\n",
    "            # On the first successful request, determine the total number of records available\n",
    "            if is_first_request:\n",
    "                total_records = data.get('total_records', 0)\n",
    "                if total_records == 0:\n",
    "                    break # Exits if the query returned no results\n",
    "                is_first_request = False\n",
    "            \n",
    "            # The API returns a list of articles under the 'articles' key\n",
    "            articles = data.get('articles', [])\n",
    "            if not articles:\n",
    "                break # Exits the loop if no more articles are returned\n",
    "\n",
    "            # Processes each article in the current batch\n",
    "            for article in articles:\n",
    "                papers.append({\n",
    "                    'id': article.get('doi', article.get('article_number')),\n",
    "                    'title': article.get('title', '').strip(),\n",
    "                    'summary': article.get('abstract', '').strip(),\n",
    "                    'authors': [author.get('full_name') for author in article.get('authors', {}).get('authors', [])],\n",
    "                    'source': 'ieee'\n",
    "                })\n",
    "            \n",
    "            # Checks if we have reached our desired limit or fetched all available records\n",
    "            if len(papers) >= max_total_records or len(papers) >= total_records:\n",
    "                break\n",
    "\n",
    "            # Increments the page number for the next request\n",
    "            params['page_number'] += 1\n",
    "            \n",
    "            # Respects the API rate limit (10 calls/sec)\n",
    "            time.sleep(0.2)\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"An API error occurred: {e}\")\n",
    "            break\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Error decoding JSON. Response content: {response.text}\")\n",
    "            break\n",
    "            \n",
    "    # Returns only the number of records requested by the user\n",
    "    return papers[:max_total_records]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3496fce",
   "metadata": {},
   "source": [
    "### Semantic Scholar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "56c198f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_paper_details(paper_ids):\n",
    "    \"\"\"\n",
    "    Fetches detailed paper metadata, including abstracts, for a list of paper IDs\n",
    "    using the /paper/batch endpoint.\n",
    "    \"\"\"\n",
    "    # The endpoint for fetching details of multiple papers\n",
    "    details_url = \"https://api.semanticscholar.org/graph/v1/paper/batch?fields=title,abstract,year,authors,citationCount,externalIds,paperId,tldr\"\n",
    "    \n",
    "    # The API can handle up to 500 IDs per request\n",
    "    chunk_size = 400 # Use a slightly smaller chunk size for safety\n",
    "    \n",
    "    detailed_papers = []\n",
    "\n",
    "    # Process the paper IDs in chunks to respect API limits\n",
    "    for i in range(0, len(paper_ids), chunk_size):\n",
    "        chunk = paper_ids[i:i + chunk_size]\n",
    "        \n",
    "        try:\n",
    "            # This is a POST request, with the IDs sent in the JSON body\n",
    "            response = requests.post(details_url, json={'ids': chunk})\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "            print(data)\n",
    "\n",
    "            # Filter out any null responses which can occur if an ID is not found\n",
    "            valid_papers = [paper for paper in data if paper is not None]\n",
    "            detailed_papers.extend(valid_papers)\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"An API error occurred during detail fetching: {e}\")\n",
    "            continue # Continue to the next chunk\n",
    "            \n",
    "        time.sleep(1) # Pause between chunks\n",
    "\n",
    "    return detailed_papers\n",
    "\n",
    "def fetch_semantic_scholar_papers_bulk(query, max_total_records=500):\n",
    "    \"\"\"\n",
    "    Performs a two-step fetch from Semantic Scholar:\n",
    "    1. Uses the bulk search to efficiently get a list of relevant paper IDs.\n",
    "    2. Uses the batch details endpoint to retrieve full metadata, including abstracts.\n",
    "    \"\"\"\n",
    "    base_query = f\"https://api.semanticscholar.org/graph/v1/paper/search/bulk?query='{query}'\"\n",
    "    \n",
    "    # In the first step, we only need the paperId\n",
    "    query_params = {\"fields\": \"title,year,citationCount\"}\n",
    "    \n",
    "    paper_ids = []\n",
    "    \n",
    "    try:\n",
    "        print(base_query)\n",
    "        response = requests.get(base_query, params=query_params)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"An API error occurred during bulk search: {e}\")\n",
    "\n",
    "    \n",
    "    articles = data.get('data', [])\n",
    "    print(articles)\n",
    "    # Collect the paper IDs from the search results\n",
    "    for article in articles:\n",
    "        paper_ids.append(article['paperId'])\n",
    "        \n",
    "    time.sleep(0.5)\n",
    "\n",
    "    # --- Step 2: Fetch full details for the collected IDs ---\n",
    "    if not paper_ids:\n",
    "        return []\n",
    "\n",
    "    print(f\"Found {len(paper_ids)} paper IDs. Now fetching details...\")\n",
    "    detailed_results = fetch_paper_details(paper_ids[:max_total_records])\n",
    "    print(detailed_results)\n",
    "    \n",
    "    # Final processing to match our desired data structure\n",
    "    final_papers = []\n",
    "    for paper in detailed_results:\n",
    "        final_papers.append({\n",
    "            'id': paper.get('paperId'),\n",
    "            'doi': paper.get('externalIds').get('DOI'),\n",
    "            'title': paper.get('title'),\n",
    "            'summary': (\n",
    "                paper.get('abstract')\n",
    "                or (paper.get('tldr') or {}).get('text')\n",
    "                or \"\"\n",
    "            ),\n",
    "            'authors': [author.get('name') for author in paper.get('authors', [])],\n",
    "            'citation_count': paper.get('citationCount'),\n",
    "            'year': paper.get('year'),\n",
    "            'source': 'semantic_scholar'\n",
    "        })\n",
    "        \n",
    "    return final_papers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228ff3a9",
   "metadata": {},
   "source": [
    "# 5. Fetch Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e1f7e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching papers from ArXiv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:01<00:00,  5.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 85 unique papers from ArXiv to data/raw/raw_arxiv.csv\n"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# Section 1: Fetch and Save ArXiv Data\n",
    "# ===================================================================\n",
    "print(\"Fetching papers from ArXiv...\")\n",
    "arxiv_papers_list = []\n",
    "for name, query in tqdm(search_queries.items()):\n",
    "    papers = fetch_arxiv_papers(query, max_results=500)\n",
    "    arxiv_papers_list.extend(papers)\n",
    "\n",
    "# De-duplicate within the ArXiv results and save\n",
    "arxiv_df = pd.DataFrame(arxiv_papers_list)\n",
    "arxiv_df.drop_duplicates(subset='id', keep='first', inplace=True)\n",
    "arxiv_df.to_csv('./data/raw/raw_arxiv.csv', index=False)\n",
    "print(f\"Saved {len(arxiv_df)} unique papers from ArXiv to data/raw/raw_arxiv.csv\")\n",
    "\n",
    "\n",
    "# ===================================================================\n",
    "# Section 2: Fetch and Save IEEE Xplore Data\n",
    "# ===================================================================\n",
    "##### NO API KEY COULD BE ACQUIRED - LEFT OUT OF FINAL RESULTS ######\n",
    "# print(\"\\nFetching papers from IEEE Xplore...\")\n",
    "# ieee_papers_list = []\n",
    "# for name, query in tqdm(expanded_search_queries.items()):\n",
    "#     papers = fetch_ieee_papers(query, max_total_records=500)\n",
    "#     ieee_papers_list.extend(papers)\n",
    "#\n",
    "# ieee_df = pd.DataFrame(ieee_papers_list)\n",
    "# ieee_df.drop_duplicates(subset='id', keep='first', inplace=True)\n",
    "# ieee_df.to_csv('../data/raw/raw_ieee.csv', index=False)\n",
    "# print(f\"Saved {len(ieee_df)} unique papers from IEEE to data/raw/raw_ieee.csv\")\n",
    "\n",
    "\n",
    "# ===================================================================\n",
    "# Section 3: Fetch and Save Semantic Scholar Data (Bulk Version)\n",
    "# ===================================================================\n",
    "# print(f\"\\nFetching from Semantic Scholar...\")\n",
    "# ss_papers_list = []\n",
    "# for name, query in tqdm(semantic_scholar_search_queries.items()):\n",
    "#     papers = fetch_semantic_scholar_papers_bulk(query, max_total_records=1000)\n",
    "#     ss_papers_list.extend(papers)\n",
    "#     time.sleep(1) \n",
    "\n",
    "# # De-duplicate within the Semantic Scholar results and save\n",
    "# ss_df = pd.DataFrame(ss_papers_list)\n",
    "# if not ss_df.empty:\n",
    "#     ss_df.drop_duplicates(subset='id', keep='first', inplace=True)\n",
    "#     ss_df.to_csv('./data/raw/raw_semantic_scholar.csv', index=False)\n",
    "#     print(f\"Saved {len(ss_df)} unique papers from Semantic Scholar to data/raw/raw_semantic_scholar.csv\")\n",
    "# else:\n",
    "#     print(\"No papers were found from Semantic Scholar.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665d462e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "causal-rl-ev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
