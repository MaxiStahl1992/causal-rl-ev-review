{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd15c209",
   "metadata": {},
   "source": [
    "# 1. Setup and Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9546b1ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 85 records from ArXiv.\n",
      "Successfully loaded 2043 records from Semantic Scholar.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Define the paths to the raw data files\n",
    "arxiv_path = './data/raw/raw_arxiv.csv'\n",
    "semantic_scholar_path = './data/raw/raw_semantic_scholar.csv'\n",
    "\n",
    "# Load the datasets into pandas DataFrames\n",
    "try:\n",
    "    arxiv_df = pd.read_csv(arxiv_path)\n",
    "    print(f\"Successfully loaded {len(arxiv_df)} records from ArXiv.\")\n",
    "except FileNotFoundError:\n",
    "    arxiv_df = pd.DataFrame()\n",
    "\n",
    "try:\n",
    "    ss_df = pd.read_csv(semantic_scholar_path)\n",
    "    print(f\"Successfully loaded {len(ss_df)} records from Semantic Scholar.\")\n",
    "except FileNotFoundError:\n",
    "    ss_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e83f273a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "title",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "summary",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "authors",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "year",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "source",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "ff414fee-1a2b-4aac-8821-af26f5960c78",
       "rows": [
        [
         "0",
         "http://arxiv.org/abs/2409.13423v1",
         "Causal Reinforcement Learning for Optimisation of Robot Dynamics in\n  Unknown Environments",
         "Autonomous operations of robots in unknown environments are challenging due\nto the lack of knowledge of the dynamics of the interactions, such as the\nobjects' movability. This work introduces a novel Causal Reinforcement Learning\napproach to enhancing robotics operations and applies it to an urban search and\nrescue (SAR) scenario. Our proposed machine learning architecture enables\nrobots to learn the causal relationships between the visual characteristics of\nthe objects, such as texture and shape, and the objects' dynamics upon\ninteraction, such as their movability, significantly improving their\ndecision-making processes. We conducted causal discovery and RL experiments\ndemonstrating the Causal RL's superior performance, showing a notable reduction\nin learning times by over 24.5% in complex situations, compared to non-causal\nmodels.",
         "['Julian Gerald Dcruz', 'Sam Mahoney', 'Jia Yun Chua', 'Adoundeth Soukhabandith', 'John Mugabe', 'Weisi Guo', 'Miguel Arana-Catania']",
         "2024-09-20T11:40:51Z",
         "arxiv"
        ],
        [
         "1",
         "http://arxiv.org/abs/2402.04869v2",
         "Learning by Doing: An Online Causal Reinforcement Learning Framework\n  with Causal-Aware Policy",
         "As a key component to intuitive cognition and reasoning solutions in human\nintelligence, causal knowledge provides great potential for reinforcement\nlearning (RL) agents' interpretability towards decision-making by helping\nreduce the searching space. However, there is still a considerable gap in\ndiscovering and incorporating causality into RL, which hinders the rapid\ndevelopment of causal RL. In this paper, we consider explicitly modeling the\ngeneration process of states with the causal graphical model, based on which we\naugment the policy. We formulate the causal structure updating into the RL\ninteraction process with active intervention learning of the environment. To\noptimize the derived objective, we propose a framework with theoretical\nperformance guarantees that alternates between two steps: using interventions\nfor causal structure learning during exploration and using the learned causal\nstructure for policy guidance during exploitation. Due to the lack of public\nbenchmarks that allow direct intervention in the state space, we design the\nroot cause localization task in our simulated fault alarm environment and then\nempirically show the effectiveness and robustness of the proposed method\nagainst state-of-the-art baselines. Theoretical analysis shows that our\nperformance improvement attributes to the virtuous cycle of causal-guided\npolicy learning and causal structure learning, which aligns with our\nexperimental results. Codes are available at\nhttps://github.com/DMIRLAB-Group/FaultAlarm_RL.",
         "['Ruichu Cai', 'Siyang Huang', 'Jie Qiao', 'Wei Chen', 'Yan Zeng', 'Keli Zhang', 'Fuchun Sun', 'Yang Yu', 'Zhifeng Hao']",
         "2025-04-24T07:58:03Z",
         "arxiv"
        ],
        [
         "2",
         "http://arxiv.org/abs/2307.01452v2",
         "Causal Reinforcement Learning: A Survey",
         "Reinforcement learning is an essential paradigm for solving sequential\ndecision problems under uncertainty. Despite many remarkable achievements in\nrecent decades, applying reinforcement learning methods in the real world\nremains challenging. One of the main obstacles is that reinforcement learning\nagents lack a fundamental understanding of the world and must therefore learn\nfrom scratch through numerous trial-and-error interactions. They may also face\nchallenges in providing explanations for their decisions and generalizing the\nacquired knowledge. Causality, however, offers a notable advantage as it can\nformalize knowledge in a systematic manner and leverage invariance for\neffective knowledge transfer. This has led to the emergence of causal\nreinforcement learning, a subfield of reinforcement learning that seeks to\nenhance existing algorithms by incorporating causal relationships into the\nlearning process. In this survey, we comprehensively review the literature on\ncausal reinforcement learning. We first introduce the basic concepts of\ncausality and reinforcement learning, and then explain how causality can\naddress core challenges in non-causal reinforcement learning. We categorize and\nsystematically review existing causal reinforcement learning approaches based\non their target problems and methodologies. Finally, we outline open issues and\nfuture directions in this emerging field.",
         "['Zhihong Deng', 'Jing Jiang', 'Guodong Long', 'Chengqi Zhang']",
         "2023-11-21T03:43:15Z",
         "arxiv"
        ],
        [
         "3",
         "http://arxiv.org/abs/2412.05783v1",
         "Two-way Deconfounder for Off-policy Evaluation in Causal Reinforcement\n  Learning",
         "This paper studies off-policy evaluation (OPE) in the presence of unmeasured\nconfounders. Inspired by the two-way fixed effects regression model widely used\nin the panel data literature, we propose a two-way unmeasured confounding\nassumption to model the system dynamics in causal reinforcement learning and\ndevelop a two-way deconfounder algorithm that devises a neural tensor network\nto simultaneously learn both the unmeasured confounders and the system\ndynamics, based on which a model-based estimator can be constructed for\nconsistent policy value estimation. We illustrate the effectiveness of the\nproposed estimator through theoretical results and numerical experiments.",
         "['Shuguang Yu', 'Shuxing Fang', 'Ruixin Peng', 'Zhengling Qi', 'Fan Zhou', 'Chengchun Shi']",
         "2024-12-08T02:28:58Z",
         "arxiv"
        ],
        [
         "4",
         "http://arxiv.org/abs/2302.13240v1",
         "Q-Cogni: An Integrated Causal Reinforcement Learning Framework",
         "We present Q-Cogni, an algorithmically integrated causal reinforcement\nlearning framework that redesigns Q-Learning with an autonomous causal\nstructure discovery method to improve the learning process with causal\ninference. Q-Cogni achieves optimal learning with a pre-learned structural\ncausal model of the environment that can be queried during the learning process\nto infer cause-and-effect relationships embedded in a state-action space. We\nleverage on the sample efficient techniques of reinforcement learning, enable\nreasoning about a broader set of policies and bring higher degrees of\ninterpretability to decisions made by the reinforcement learning agent. We\napply Q-Cogni on the Vehicle Routing Problem (VRP) and compare against\nstate-of-the-art reinforcement learning algorithms. We report results that\ndemonstrate better policies, improved learning efficiency and superior\ninterpretability of the agent's decision making. We also compare this approach\nwith traditional shortest-path search algorithms and demonstrate the benefits\nof our causal reinforcement learning framework to high dimensional problems.\nFinally, we apply Q-Cogni to derive optimal routing decisions for taxis in New\nYork City using the Taxi & Limousine Commission trip record data and compare\nwith shortest-path search, reporting results that show 85% of the cases with an\nequal or better policy derived from Q-Cogni in a real-world domain.",
         "['Cris Cunha', 'Wei Liu', 'Tim French', 'Ajmal Mian']",
         "2023-02-26T05:50:26Z",
         "arxiv"
        ]
       ],
       "shape": {
        "columns": 6,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>summary</th>\n",
       "      <th>authors</th>\n",
       "      <th>year</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://arxiv.org/abs/2409.13423v1</td>\n",
       "      <td>Causal Reinforcement Learning for Optimisation...</td>\n",
       "      <td>Autonomous operations of robots in unknown env...</td>\n",
       "      <td>['Julian Gerald Dcruz', 'Sam Mahoney', 'Jia Yu...</td>\n",
       "      <td>2024-09-20T11:40:51Z</td>\n",
       "      <td>arxiv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://arxiv.org/abs/2402.04869v2</td>\n",
       "      <td>Learning by Doing: An Online Causal Reinforcem...</td>\n",
       "      <td>As a key component to intuitive cognition and ...</td>\n",
       "      <td>['Ruichu Cai', 'Siyang Huang', 'Jie Qiao', 'We...</td>\n",
       "      <td>2025-04-24T07:58:03Z</td>\n",
       "      <td>arxiv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://arxiv.org/abs/2307.01452v2</td>\n",
       "      <td>Causal Reinforcement Learning: A Survey</td>\n",
       "      <td>Reinforcement learning is an essential paradig...</td>\n",
       "      <td>['Zhihong Deng', 'Jing Jiang', 'Guodong Long',...</td>\n",
       "      <td>2023-11-21T03:43:15Z</td>\n",
       "      <td>arxiv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://arxiv.org/abs/2412.05783v1</td>\n",
       "      <td>Two-way Deconfounder for Off-policy Evaluation...</td>\n",
       "      <td>This paper studies off-policy evaluation (OPE)...</td>\n",
       "      <td>['Shuguang Yu', 'Shuxing Fang', 'Ruixin Peng',...</td>\n",
       "      <td>2024-12-08T02:28:58Z</td>\n",
       "      <td>arxiv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://arxiv.org/abs/2302.13240v1</td>\n",
       "      <td>Q-Cogni: An Integrated Causal Reinforcement Le...</td>\n",
       "      <td>We present Q-Cogni, an algorithmically integra...</td>\n",
       "      <td>['Cris Cunha', 'Wei Liu', 'Tim French', 'Ajmal...</td>\n",
       "      <td>2023-02-26T05:50:26Z</td>\n",
       "      <td>arxiv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  id  \\\n",
       "0  http://arxiv.org/abs/2409.13423v1   \n",
       "1  http://arxiv.org/abs/2402.04869v2   \n",
       "2  http://arxiv.org/abs/2307.01452v2   \n",
       "3  http://arxiv.org/abs/2412.05783v1   \n",
       "4  http://arxiv.org/abs/2302.13240v1   \n",
       "\n",
       "                                               title  \\\n",
       "0  Causal Reinforcement Learning for Optimisation...   \n",
       "1  Learning by Doing: An Online Causal Reinforcem...   \n",
       "2            Causal Reinforcement Learning: A Survey   \n",
       "3  Two-way Deconfounder for Off-policy Evaluation...   \n",
       "4  Q-Cogni: An Integrated Causal Reinforcement Le...   \n",
       "\n",
       "                                             summary  \\\n",
       "0  Autonomous operations of robots in unknown env...   \n",
       "1  As a key component to intuitive cognition and ...   \n",
       "2  Reinforcement learning is an essential paradig...   \n",
       "3  This paper studies off-policy evaluation (OPE)...   \n",
       "4  We present Q-Cogni, an algorithmically integra...   \n",
       "\n",
       "                                             authors                  year  \\\n",
       "0  ['Julian Gerald Dcruz', 'Sam Mahoney', 'Jia Yu...  2024-09-20T11:40:51Z   \n",
       "1  ['Ruichu Cai', 'Siyang Huang', 'Jie Qiao', 'We...  2025-04-24T07:58:03Z   \n",
       "2  ['Zhihong Deng', 'Jing Jiang', 'Guodong Long',...  2023-11-21T03:43:15Z   \n",
       "3  ['Shuguang Yu', 'Shuxing Fang', 'Ruixin Peng',...  2024-12-08T02:28:58Z   \n",
       "4  ['Cris Cunha', 'Wei Liu', 'Tim French', 'Ajmal...  2023-02-26T05:50:26Z   \n",
       "\n",
       "  source  \n",
       "0  arxiv  \n",
       "1  arxiv  \n",
       "2  arxiv  \n",
       "3  arxiv  \n",
       "4  arxiv  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arxiv_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "70e95f32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "doi",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "title",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "summary",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "authors",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "citation_count",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "year",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "source",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "6bcafa53-1b68-46f8-91ff-18136b302aa8",
       "rows": [
        [
         "0",
         "00b75f61f8bd3246fff75f84d852ba3e80d5338e",
         "10.1109/ISIT.2014.6875397",
         "Applications of information Nonanticipative Rate Distortion Function",
         "The objective of this paper is to further investigate various applications of information Nonanticipative Rate Distortion Function (NRDF) by discussing two working examples, the Binary Symmetric Markov Source with parameter p (BSMS(p)) with Hamming distance distortion, and the multidimensional partially observed Gaussian-Markov source. For the BSMS(p), we give the solution to the NRDF, and we use it to compute the Rate Loss (RL) of causal codes with respect to noncausal codes. For the multidimensional Gaussian-Markov source, we give the solution to the NRDF, we show its operational meaning via joint source-channel matching over a vector of parallel Gaussian channels, and we compute the RL of causal and zero-delay codes with respect to noncausal codes.",
         "['Photios A. Stavrou', 'C. Kourtellaris', 'C. Charalambous']",
         "4",
         "2014.0",
         "semantic_scholar"
        ],
        [
         "1",
         "01befcd360d36d520f595b34d5d26e37e0ac16f3",
         "10.1609/aaai.v34i10.7134",
         "Explainable Agency in Reinforcement Learning Agents",
         "This thesis explores how reinforcement learning (RL) agents can provide explanations for their actions and behaviours. As humans, we build causal models to encode cause-effect relations of events and use these to explain why events happen. Taking inspiration from cognitive psychology and social science literature, I build causal explanation models and explanation dialogue models for RL agents. By mimicking human-like explanation models, these agents can provide explanations that are natural and intuitive to humans.",
         "['Prashan Madumal']",
         "1",
         "2020.0",
         "semantic_scholar"
        ],
        [
         "2",
         "01e9241dbb9eaca99b86468bb079f4b631b71671",
         "10.48550/arXiv.2406.01065",
         "Causal prompting model-based offline reinforcement learning",
         "Model-based offline Reinforcement Learning (RL) allows agents to fully utilise pre-collected datasets without requiring additional or unethical explorations. However, applying model-based offline RL to online systems presents challenges, primarily due to the highly suboptimal (noise-filled) and diverse nature of datasets generated by online systems. To tackle these issues, we introduce the Causal Prompting Reinforcement Learning (CPRL) framework, designed for highly suboptimal and resource-constrained online scenarios. The initial phase of CPRL involves the introduction of the Hidden-Parameter Block Causal Prompting Dynamic (Hip-BCPD) to model environmental dynamics. This approach utilises invariant causal prompts and aligns hidden parameters to generalise to new and diverse online users. In the subsequent phase, a single policy is trained to address multiple tasks through the amalgamation of reusable skills, circumventing the need for training from scratch. Experiments conducted across datasets with varying levels of noise, including simulation-based and real-world offline datasets from the Dnurse APP, demonstrate that our proposed method can make robust decisions in out-of-distribution and noisy environments, outperforming contemporary algorithms. Additionally, we separately verify the contributions of Hip-BCPDs and the skill-reuse strategy to the robustness of performance. We further analyse the visualised structure of Hip-BCPD and the interpretability of sub-skills. We released our source code and the first ever real-world medical dataset for precise medical decision-making tasks.",
         "['Xuehui Yu', 'Yi Guan', 'Rujia Shen', 'Xin Li', 'Chen Tang', 'Jingchi Jiang']",
         "0",
         "2024.0",
         "semantic_scholar"
        ],
        [
         "3",
         "026dc8d3cbb360bdd12d19c924bc633221c9b423",
         null,
         "Learning Causal Overhypotheses through Exploration in Children and Computational Models",
         "Despite recent progress in reinforcement learning (RL), RL algorithms for exploration still remain an active area of research. Existing methods often focus on state-based metrics, which do not consider the underlying causal structures of the environment, and while recent research has begun to explore RL environments for causal learning, these environments primarily leverage causal information through causal inference or induction rather than exploration. In contrast, human children - some of the most proficient explorers - have been shown to use causal information to great benefit. In this work, we introduce a novel RL environment designed with a controllable causal structure, which allows us to evaluate exploration strategies used by both agents and children in a unified environment. In addition, through experimentation on both computation models and children, we demonstrate that there are significant differences between information-gain optimal RL exploration in causal environments and the exploration of children in the same environments. We conclude with a discussion of how these findings may inspire new directions of research into efficient exploration and disambiguation of causal structures for RL algorithms.",
         "['Eliza Kosoy', 'Adrian Liu', 'Jasmine Collins', 'David Chan', 'Jessica B. Hamrick', 'Nan Rosemary Ke', 'Sandy H. Huang', 'Bryanna Kaufmann', 'J. Canny', 'A. Gopnik']",
         "9",
         "2022.0",
         "semantic_scholar"
        ],
        [
         "4",
         "0348b36927f740b82f51afcd1c35cae8386bc336",
         "10.1109/iv51971.2022.9827374",
         "Segmented Encoding for Sim2Real of RL-based End-to-End Autonomous Driving",
         "Among the challenges in the recent research of end-to-end (E2E) driving, interpretability and distribution shift in the simulation-to-real (Sim2Real) have drawn considerable attention. Because of low interpretability, we cannot clearly explain the causal relationship between the input image and the control actions by the network. Moreover, the distribution shift problem in Sim2Real degrades the driving performance of the policy in the realworld deployment. In this paper, we propose a segmentation-based classwise disentangled latent encoding algorithm to cope with the two challenges. In the proposed algorithm, multi-class segmentation transfers RGB images in both simulation and real environments to the same domain, while preserving the necessary information of objects of primary classes, such as pedestrian, road, and cars, for driving decisions. Besides, in the class-wise disentangled latent encoding, segmented images are encoded to a latent vector, which improves the interpretability significantly, since the state input has a structured format. The interpretability improvement is testified by the t-stochastic neighbor embedding, image reconstruction and the causal relationship between the real images and the control actions. We deploy the driving policy trained in the simulation directly to an autonomous vehicle platform and show, to the best of our knowledge, the first demonstration of the RL-based E2E autonomous in various real environments.",
         "['Seung H. Chung', 'S. Kong', 'S. Cho', 'I. M. A. Nahrendra']",
         "3",
         "2022.0",
         "semantic_scholar"
        ]
       ],
       "shape": {
        "columns": 8,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>doi</th>\n",
       "      <th>title</th>\n",
       "      <th>summary</th>\n",
       "      <th>authors</th>\n",
       "      <th>citation_count</th>\n",
       "      <th>year</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00b75f61f8bd3246fff75f84d852ba3e80d5338e</td>\n",
       "      <td>10.1109/ISIT.2014.6875397</td>\n",
       "      <td>Applications of information Nonanticipative Ra...</td>\n",
       "      <td>The objective of this paper is to further inve...</td>\n",
       "      <td>['Photios A. Stavrou', 'C. Kourtellaris', 'C. ...</td>\n",
       "      <td>4</td>\n",
       "      <td>2014.0</td>\n",
       "      <td>semantic_scholar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01befcd360d36d520f595b34d5d26e37e0ac16f3</td>\n",
       "      <td>10.1609/aaai.v34i10.7134</td>\n",
       "      <td>Explainable Agency in Reinforcement Learning A...</td>\n",
       "      <td>This thesis explores how reinforcement learnin...</td>\n",
       "      <td>['Prashan Madumal']</td>\n",
       "      <td>1</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>semantic_scholar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01e9241dbb9eaca99b86468bb079f4b631b71671</td>\n",
       "      <td>10.48550/arXiv.2406.01065</td>\n",
       "      <td>Causal prompting model-based offline reinforce...</td>\n",
       "      <td>Model-based offline Reinforcement Learning (RL...</td>\n",
       "      <td>['Xuehui Yu', 'Yi Guan', 'Rujia Shen', 'Xin Li...</td>\n",
       "      <td>0</td>\n",
       "      <td>2024.0</td>\n",
       "      <td>semantic_scholar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>026dc8d3cbb360bdd12d19c924bc633221c9b423</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Learning Causal Overhypotheses through Explora...</td>\n",
       "      <td>Despite recent progress in reinforcement learn...</td>\n",
       "      <td>['Eliza Kosoy', 'Adrian Liu', 'Jasmine Collins...</td>\n",
       "      <td>9</td>\n",
       "      <td>2022.0</td>\n",
       "      <td>semantic_scholar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0348b36927f740b82f51afcd1c35cae8386bc336</td>\n",
       "      <td>10.1109/iv51971.2022.9827374</td>\n",
       "      <td>Segmented Encoding for Sim2Real of RL-based En...</td>\n",
       "      <td>Among the challenges in the recent research of...</td>\n",
       "      <td>['Seung H. Chung', 'S. Kong', 'S. Cho', 'I. M....</td>\n",
       "      <td>3</td>\n",
       "      <td>2022.0</td>\n",
       "      <td>semantic_scholar</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         id                           doi  \\\n",
       "0  00b75f61f8bd3246fff75f84d852ba3e80d5338e     10.1109/ISIT.2014.6875397   \n",
       "1  01befcd360d36d520f595b34d5d26e37e0ac16f3      10.1609/aaai.v34i10.7134   \n",
       "2  01e9241dbb9eaca99b86468bb079f4b631b71671     10.48550/arXiv.2406.01065   \n",
       "3  026dc8d3cbb360bdd12d19c924bc633221c9b423                           NaN   \n",
       "4  0348b36927f740b82f51afcd1c35cae8386bc336  10.1109/iv51971.2022.9827374   \n",
       "\n",
       "                                               title  \\\n",
       "0  Applications of information Nonanticipative Ra...   \n",
       "1  Explainable Agency in Reinforcement Learning A...   \n",
       "2  Causal prompting model-based offline reinforce...   \n",
       "3  Learning Causal Overhypotheses through Explora...   \n",
       "4  Segmented Encoding for Sim2Real of RL-based En...   \n",
       "\n",
       "                                             summary  \\\n",
       "0  The objective of this paper is to further inve...   \n",
       "1  This thesis explores how reinforcement learnin...   \n",
       "2  Model-based offline Reinforcement Learning (RL...   \n",
       "3  Despite recent progress in reinforcement learn...   \n",
       "4  Among the challenges in the recent research of...   \n",
       "\n",
       "                                             authors  citation_count    year  \\\n",
       "0  ['Photios A. Stavrou', 'C. Kourtellaris', 'C. ...               4  2014.0   \n",
       "1                                ['Prashan Madumal']               1  2020.0   \n",
       "2  ['Xuehui Yu', 'Yi Guan', 'Rujia Shen', 'Xin Li...               0  2024.0   \n",
       "3  ['Eliza Kosoy', 'Adrian Liu', 'Jasmine Collins...               9  2022.0   \n",
       "4  ['Seung H. Chung', 'S. Kong', 'S. Cho', 'I. M....               3  2022.0   \n",
       "\n",
       "             source  \n",
       "0  semantic_scholar  \n",
       "1  semantic_scholar  \n",
       "2  semantic_scholar  \n",
       "3  semantic_scholar  \n",
       "4  semantic_scholar  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2e0d5a",
   "metadata": {},
   "source": [
    "# 2. Standardize Schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "91b38b15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 85 entries, 0 to 84\n",
      "Data columns (total 8 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   arxiv_id        85 non-null     object \n",
      " 1   title           85 non-null     object \n",
      " 2   summary         85 non-null     object \n",
      " 3   authors         85 non-null     object \n",
      " 4   year            85 non-null     int32  \n",
      " 5   source          85 non-null     object \n",
      " 6   doi             0 non-null      float64\n",
      " 7   citation_count  0 non-null      float64\n",
      "dtypes: float64(2), int32(1), object(5)\n",
      "memory usage: 5.1+ KB\n"
     ]
    }
   ],
   "source": [
    "# --- Standardize ArXiv DataFrame ---\n",
    "if not arxiv_df.empty:\n",
    "    # Rename the 'id' column to be more specific\n",
    "    arxiv_df.rename(columns={'id': 'arxiv_id'}, inplace=True)\n",
    "    # Add placeholder columns that exist in the Semantic Scholar data\n",
    "    arxiv_df['doi'] = np.nan\n",
    "    arxiv_df['citation_count'] = np.nan\n",
    "    # Year as number\n",
    "    arxiv_df[\"year\"] = pd.to_datetime(arxiv_df[\"year\"], errors=\"coerce\").dt.year\n",
    "\n",
    "arxiv_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f5af0d14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2043 entries, 0 to 2042\n",
      "Data columns (total 9 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   paper_id_s2     2043 non-null   object \n",
      " 1   doi             1764 non-null   object \n",
      " 2   title           2043 non-null   object \n",
      " 3   summary         1888 non-null   object \n",
      " 4   authors         2043 non-null   object \n",
      " 5   citation_count  2043 non-null   int64  \n",
      " 6   year            2004 non-null   Int64  \n",
      " 7   source          2043 non-null   object \n",
      " 8   arxiv_id        0 non-null      float64\n",
      "dtypes: Int64(1), float64(1), int64(1), object(6)\n",
      "memory usage: 145.8+ KB\n"
     ]
    }
   ],
   "source": [
    "# --- Standardize Semantic Scholar DataFrame ---\n",
    "if not ss_df.empty:\n",
    "    # Rename the 'id' column to avoid confusion\n",
    "    ss_df.rename(columns={'id': 'paper_id_s2'}, inplace=True)\n",
    "    # Add a placeholder for the arxiv_id\n",
    "    ss_df['arxiv_id'] = np.nan\n",
    "    # Year as number\n",
    "    ss_df[\"year\"] = pd.to_numeric(ss_df[\"year\"], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "ss_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f8de73",
   "metadata": {},
   "source": [
    "# 3. Combine and De-Duplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b4055b7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total records before de-duplication: 2128\n",
      "Total unique records after de-duplication: 1758\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "arxiv_id",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "title",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "summary",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "authors",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "year",
         "rawType": "Int64",
         "type": "integer"
        },
        {
         "name": "source",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "doi",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "citation_count",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "paper_id_s2",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "b9ced0af-8c74-4d33-b74a-1a7ce1e93de5",
       "rows": [
        [
         "399",
         null,
         "Decision Transformer: Reinforcement Learning via Sequence Modeling",
         "We introduce a framework that abstracts Reinforcement Learning (RL) as a sequence modeling problem. This allows us to draw upon the simplicity and scalability of the Transformer architecture, and associated advances in language modeling such as GPT-x and BERT. In particular, we present Decision Transformer, an architecture that casts the problem of RL as conditional sequence modeling. Unlike prior approaches to RL that fit value functions or compute policy gradients, Decision Transformer simply outputs the optimal actions by leveraging a causally masked Transformer. By conditioning an autoregressive model on the desired return (reward), past states, and actions, our Decision Transformer model can generate future actions that achieve the desired return. Despite its simplicity, Decision Transformer matches or exceeds the performance of state-of-the-art model-free offline RL baselines on Atari, OpenAI Gym, and Key-to-Door tasks.",
         "['Lili Chen', 'Kevin Lu', 'A. Rajeswaran', 'Kimin Lee', 'Aditya Grover', 'M. Laskin', 'P. Abbeel', 'A. Srinivas', 'Igor Mordatch']",
         "2021",
         "semantic_scholar",
         null,
         "1736.0",
         "c1ad5f9b32d80f1c65d67894e5b8c2fdf0ae4500"
        ],
        [
         "1528",
         null,
         "Reinforcement learning for demand response: A review of algorithms and modeling techniques",
         "A need is identified to further explore reinforcement learning to coordinate multi-agent systems that can participate in demand response programs under demand-dependent electricity prices, when electricity prices are modelled as demand- dependent variables.",
         "['José R. Vázquez-Canteli', 'Z. Nagy']",
         "2019",
         "semantic_scholar",
         "10.1016/J.APENERGY.2018.11.002",
         "600.0",
         "648ea87fe7f99ca8ea5090cb1ba40242299ef4c4"
        ],
        [
         "1395",
         null,
         "Perceptual Learning Directs Auditory Cortical Map Reorganization through Top-Down Influences",
         "The primary sensory cortex is positioned at a confluence of bottom-up dedicated sensory inputs and top-down inputs related to higher-order sensory features, attentional state, and behavioral reinforcement. We tested whether topographic map plasticity in the adult primary auditory cortex and a secondary auditory area, the suprarhinal auditory field, was controlled by the statistics of bottom-up sensory inputs or by top-down task-dependent influences. Rats were trained to attend to independent parameters, either frequency or intensity, within an identical set of auditory stimuli, allowing us to vary task demands while holding the bottom-up sensory inputs constant. We observed a clear double-dissociation in map plasticity in both cortical fields. Rats trained to attend to frequency cues exhibited an expanded representation of the target frequency range within the tonotopic map but no change in sound intensity encoding compared with controls. Rats trained to attend to intensity cues expressed an increased proportion of nonmonotonic intensity response profiles preferentially tuned to the target intensity range but no change in tonotopic map organization relative to controls. The degree of topographic map plasticity within the task-relevant stimulus dimension was correlated with the degree of perceptual learning for rats in both tasks. These data suggest that enduring receptive field plasticity in the adult auditory cortex may be shaped by task-specific top-down inputs that interact with bottom-up sensory inputs and reinforcement-based neuromodulator release. Top-down inputs might confer the selectivity necessary to modify a single feature representation without affecting other spatially organized feature representations embedded within the same neural circuitry.",
         "['D. Polley', 'Elizabeth E Steinberg', 'M. Merzenich']",
         "2006",
         "semantic_scholar",
         "10.1523/JNEUROSCI.3771-05.2006",
         "567.0",
         "3fdfbf8de976c103ad0ee05cc9112fe8316fa342"
        ],
        [
         "328",
         null,
         "Reinforcement Knowledge Graph Reasoning for Explainable Recommendation",
         "Recent advances in personalized recommendation have sparked great interest in the exploitation of rich structured information provided by knowledge graphs. Unlike most existing approaches that only focus on leveraging knowledge graphs for more accurate recommendation, we aim to conduct explicit reasoning with knowledge for decision making so that the recommendations are generated and supported by an interpretable causal inference procedure. To this end, we propose a method called Policy-Guided Path Reasoning (PGPR), which couples recommendation and interpretability by providing actual paths in a knowledge graph. Our contributions include four aspects. We first highlight the significance of incorporating knowledge graphs into recommendation to formally define and interpret the reasoning process. Second, we propose a reinforcement learning (RL) approach featured by an innovative soft reward strategy, user-conditional action pruning and a multi-hop scoring function. Third, we design a policy-guided graph search algorithm to efficiently and effectively sample reasoning paths for recommendation. Finally, we extensively evaluate our method on several large-scale real-world benchmark datasets, obtaining favorable results compared with state-of-the-art methods.",
         "['Yikun Xian', 'Zuohui Fu', 'S. Muthukrishnan', 'Gerard de Melo', 'Yongfeng Zhang']",
         "2019",
         "semantic_scholar",
         "10.1145/3331184.3331203",
         "480.0",
         "9a14989424b16a4685c43ffc8057b40157631dd2"
        ],
        [
         "1608",
         null,
         "Working-memory capacity protects model-based learning from stress",
         "It is found that stress response attenuates the contribution of model-based, but not model-free, contributions to behavior, suggesting that executive function may be protective against the deleterious effects of acute stress.",
         "['A. R. Otto', 'Candace M. Raio', 'Alice Y. Chiang', 'E. Phelps', 'N. Daw']",
         "2013",
         "semantic_scholar",
         "10.1073/pnas.1312011110",
         "432.0",
         "78b89e68ed84ef344f82922ea702a736a5791d0a"
        ]
       ],
       "shape": {
        "columns": 9,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>arxiv_id</th>\n",
       "      <th>title</th>\n",
       "      <th>summary</th>\n",
       "      <th>authors</th>\n",
       "      <th>year</th>\n",
       "      <th>source</th>\n",
       "      <th>doi</th>\n",
       "      <th>citation_count</th>\n",
       "      <th>paper_id_s2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Decision Transformer: Reinforcement Learning v...</td>\n",
       "      <td>We introduce a framework that abstracts Reinfo...</td>\n",
       "      <td>['Lili Chen', 'Kevin Lu', 'A. Rajeswaran', 'Ki...</td>\n",
       "      <td>2021</td>\n",
       "      <td>semantic_scholar</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1736.0</td>\n",
       "      <td>c1ad5f9b32d80f1c65d67894e5b8c2fdf0ae4500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1528</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Reinforcement learning for demand response: A ...</td>\n",
       "      <td>A need is identified to further explore reinfo...</td>\n",
       "      <td>['José R. Vázquez-Canteli', 'Z. Nagy']</td>\n",
       "      <td>2019</td>\n",
       "      <td>semantic_scholar</td>\n",
       "      <td>10.1016/J.APENERGY.2018.11.002</td>\n",
       "      <td>600.0</td>\n",
       "      <td>648ea87fe7f99ca8ea5090cb1ba40242299ef4c4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1395</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Perceptual Learning Directs Auditory Cortical ...</td>\n",
       "      <td>The primary sensory cortex is positioned at a ...</td>\n",
       "      <td>['D. Polley', 'Elizabeth E Steinberg', 'M. Mer...</td>\n",
       "      <td>2006</td>\n",
       "      <td>semantic_scholar</td>\n",
       "      <td>10.1523/JNEUROSCI.3771-05.2006</td>\n",
       "      <td>567.0</td>\n",
       "      <td>3fdfbf8de976c103ad0ee05cc9112fe8316fa342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Reinforcement Knowledge Graph Reasoning for Ex...</td>\n",
       "      <td>Recent advances in personalized recommendation...</td>\n",
       "      <td>['Yikun Xian', 'Zuohui Fu', 'S. Muthukrishnan'...</td>\n",
       "      <td>2019</td>\n",
       "      <td>semantic_scholar</td>\n",
       "      <td>10.1145/3331184.3331203</td>\n",
       "      <td>480.0</td>\n",
       "      <td>9a14989424b16a4685c43ffc8057b40157631dd2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1608</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Working-memory capacity protects model-based l...</td>\n",
       "      <td>It is found that stress response attenuates th...</td>\n",
       "      <td>['A. R. Otto', 'Candace M. Raio', 'Alice Y. Ch...</td>\n",
       "      <td>2013</td>\n",
       "      <td>semantic_scholar</td>\n",
       "      <td>10.1073/pnas.1312011110</td>\n",
       "      <td>432.0</td>\n",
       "      <td>78b89e68ed84ef344f82922ea702a736a5791d0a</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     arxiv_id                                              title  \\\n",
       "399       NaN  Decision Transformer: Reinforcement Learning v...   \n",
       "1528      NaN  Reinforcement learning for demand response: A ...   \n",
       "1395      NaN  Perceptual Learning Directs Auditory Cortical ...   \n",
       "328       NaN  Reinforcement Knowledge Graph Reasoning for Ex...   \n",
       "1608      NaN  Working-memory capacity protects model-based l...   \n",
       "\n",
       "                                                summary  \\\n",
       "399   We introduce a framework that abstracts Reinfo...   \n",
       "1528  A need is identified to further explore reinfo...   \n",
       "1395  The primary sensory cortex is positioned at a ...   \n",
       "328   Recent advances in personalized recommendation...   \n",
       "1608  It is found that stress response attenuates th...   \n",
       "\n",
       "                                                authors  year  \\\n",
       "399   ['Lili Chen', 'Kevin Lu', 'A. Rajeswaran', 'Ki...  2021   \n",
       "1528             ['José R. Vázquez-Canteli', 'Z. Nagy']  2019   \n",
       "1395  ['D. Polley', 'Elizabeth E Steinberg', 'M. Mer...  2006   \n",
       "328   ['Yikun Xian', 'Zuohui Fu', 'S. Muthukrishnan'...  2019   \n",
       "1608  ['A. R. Otto', 'Candace M. Raio', 'Alice Y. Ch...  2013   \n",
       "\n",
       "                source                             doi  citation_count  \\\n",
       "399   semantic_scholar                             NaN          1736.0   \n",
       "1528  semantic_scholar  10.1016/J.APENERGY.2018.11.002           600.0   \n",
       "1395  semantic_scholar  10.1523/JNEUROSCI.3771-05.2006           567.0   \n",
       "328   semantic_scholar         10.1145/3331184.3331203           480.0   \n",
       "1608  semantic_scholar         10.1073/pnas.1312011110           432.0   \n",
       "\n",
       "                                   paper_id_s2  \n",
       "399   c1ad5f9b32d80f1c65d67894e5b8c2fdf0ae4500  \n",
       "1528  648ea87fe7f99ca8ea5090cb1ba40242299ef4c4  \n",
       "1395  3fdfbf8de976c103ad0ee05cc9112fe8316fa342  \n",
       "328   9a14989424b16a4685c43ffc8057b40157631dd2  \n",
       "1608  78b89e68ed84ef344f82922ea702a736a5791d0a  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine the two standardized dataframes into one\n",
    "combined_df = pd.concat([arxiv_df, ss_df], ignore_index=True)\n",
    "print(f\"\\nTotal records before de-duplication: {len(combined_df)}\")\n",
    "\n",
    "# --- Robust De-duplication ---\n",
    "# Clean up the DOI column by removing any leading/trailing whitespace\n",
    "combined_df['doi'] = combined_df['doi'].str.strip()\n",
    "\n",
    "# Replace empty strings in 'doi' with NaN to handle them consistently\n",
    "#combined_df['doi'].replace('', np.nan, inplace=True)\n",
    "combined_df.loc[combined_df['doi'] == '', 'doi'] = np.nan\n",
    "\n",
    "# Sort by citation count (descending) to keep the more cited version of a paper\n",
    "# and by source to have a predictable order\n",
    "combined_df.sort_values(by=['citation_count', 'source'], ascending=[False, True], inplace=True)\n",
    "\n",
    "# First pass: Drop duplicates based on DOI for all papers that have one\n",
    "final_df = combined_df.drop_duplicates(subset='doi', keep='first').copy()\n",
    "\n",
    "# For entries that had no DOI (NaN), they are not de-duplicated yet.\n",
    "# Second pass: De-duplicate the remaining entries based on a cleaned title.\n",
    "# Create a temporary lowercase title for matching.\n",
    "final_df['title_lower'] = final_df['title'].str.lower().str.strip()\n",
    "final_df.drop_duplicates(subset='title_lower', keep='first', inplace=True)\n",
    "\n",
    "# Clean up the temporary column\n",
    "final_df = final_df.drop(columns=['title_lower'])\n",
    "\n",
    "print(f\"Total unique records after de-duplication: {len(final_df)}\")\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a1d7c80f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Removed 106 records with missing or short summaries.\n",
      "Final corpus size: 1652\n",
      "\n",
      "Master corpus of 1652 unique papers saved to ./data/processed/master_corpus.csv\n",
      "\n",
      "Source Distribution:\n",
      "source\n",
      "semantic_scholar    1652\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# --- Final Data Cleaning ---\n",
    "# Ensure the summary column is a string\n",
    "final_df['summary'] = final_df['summary'].astype(str)\n",
    "\n",
    "# Remove rows where the summary is missing or very short (e.g., less than 20 characters)\n",
    "initial_count = len(final_df)\n",
    "final_df = final_df[final_df['summary'].str.len() > 20]\n",
    "print(f\"\\nRemoved {initial_count - len(final_df)} records with missing or short summaries.\")\n",
    "print(f\"Final corpus size: {len(final_df)}\")\n",
    "\n",
    "# --- Save the Master Corpus ---\n",
    "master_path = './data/processed/master_corpus.csv'\n",
    "final_df.to_csv(master_path, index=False)\n",
    "print(f\"\\nMaster corpus of {len(final_df)} unique papers saved to {master_path}\")\n",
    "\n",
    "# Display the distribution of papers from each source\n",
    "print(\"\\nSource Distribution:\")\n",
    "print(final_df['source'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "54b4480d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1652 entries, 399 to 2125\n",
      "Data columns (total 9 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   arxiv_id        0 non-null      object \n",
      " 1   title           1652 non-null   object \n",
      " 2   summary         1652 non-null   object \n",
      " 3   authors         1652 non-null   object \n",
      " 4   year            1649 non-null   Int64  \n",
      " 5   source          1652 non-null   object \n",
      " 6   doi             1651 non-null   object \n",
      " 7   citation_count  1652 non-null   float64\n",
      " 8   paper_id_s2     1652 non-null   object \n",
      "dtypes: Int64(1), float64(1), object(7)\n",
      "memory usage: 130.7+ KB\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "arxiv_id",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "title",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "summary",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "authors",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "year",
         "rawType": "Int64",
         "type": "integer"
        },
        {
         "name": "source",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "doi",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "citation_count",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "paper_id_s2",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "dffe4505-4da7-4b2f-9bc6-85dd1365c3d9",
       "rows": [
        [
         "399",
         null,
         "Decision Transformer: Reinforcement Learning via Sequence Modeling",
         "We introduce a framework that abstracts Reinforcement Learning (RL) as a sequence modeling problem. This allows us to draw upon the simplicity and scalability of the Transformer architecture, and associated advances in language modeling such as GPT-x and BERT. In particular, we present Decision Transformer, an architecture that casts the problem of RL as conditional sequence modeling. Unlike prior approaches to RL that fit value functions or compute policy gradients, Decision Transformer simply outputs the optimal actions by leveraging a causally masked Transformer. By conditioning an autoregressive model on the desired return (reward), past states, and actions, our Decision Transformer model can generate future actions that achieve the desired return. Despite its simplicity, Decision Transformer matches or exceeds the performance of state-of-the-art model-free offline RL baselines on Atari, OpenAI Gym, and Key-to-Door tasks.",
         "['Lili Chen', 'Kevin Lu', 'A. Rajeswaran', 'Kimin Lee', 'Aditya Grover', 'M. Laskin', 'P. Abbeel', 'A. Srinivas', 'Igor Mordatch']",
         "2021",
         "semantic_scholar",
         null,
         "1736.0",
         "c1ad5f9b32d80f1c65d67894e5b8c2fdf0ae4500"
        ],
        [
         "1528",
         null,
         "Reinforcement learning for demand response: A review of algorithms and modeling techniques",
         "A need is identified to further explore reinforcement learning to coordinate multi-agent systems that can participate in demand response programs under demand-dependent electricity prices, when electricity prices are modelled as demand- dependent variables.",
         "['José R. Vázquez-Canteli', 'Z. Nagy']",
         "2019",
         "semantic_scholar",
         "10.1016/J.APENERGY.2018.11.002",
         "600.0",
         "648ea87fe7f99ca8ea5090cb1ba40242299ef4c4"
        ],
        [
         "1395",
         null,
         "Perceptual Learning Directs Auditory Cortical Map Reorganization through Top-Down Influences",
         "The primary sensory cortex is positioned at a confluence of bottom-up dedicated sensory inputs and top-down inputs related to higher-order sensory features, attentional state, and behavioral reinforcement. We tested whether topographic map plasticity in the adult primary auditory cortex and a secondary auditory area, the suprarhinal auditory field, was controlled by the statistics of bottom-up sensory inputs or by top-down task-dependent influences. Rats were trained to attend to independent parameters, either frequency or intensity, within an identical set of auditory stimuli, allowing us to vary task demands while holding the bottom-up sensory inputs constant. We observed a clear double-dissociation in map plasticity in both cortical fields. Rats trained to attend to frequency cues exhibited an expanded representation of the target frequency range within the tonotopic map but no change in sound intensity encoding compared with controls. Rats trained to attend to intensity cues expressed an increased proportion of nonmonotonic intensity response profiles preferentially tuned to the target intensity range but no change in tonotopic map organization relative to controls. The degree of topographic map plasticity within the task-relevant stimulus dimension was correlated with the degree of perceptual learning for rats in both tasks. These data suggest that enduring receptive field plasticity in the adult auditory cortex may be shaped by task-specific top-down inputs that interact with bottom-up sensory inputs and reinforcement-based neuromodulator release. Top-down inputs might confer the selectivity necessary to modify a single feature representation without affecting other spatially organized feature representations embedded within the same neural circuitry.",
         "['D. Polley', 'Elizabeth E Steinberg', 'M. Merzenich']",
         "2006",
         "semantic_scholar",
         "10.1523/JNEUROSCI.3771-05.2006",
         "567.0",
         "3fdfbf8de976c103ad0ee05cc9112fe8316fa342"
        ],
        [
         "328",
         null,
         "Reinforcement Knowledge Graph Reasoning for Explainable Recommendation",
         "Recent advances in personalized recommendation have sparked great interest in the exploitation of rich structured information provided by knowledge graphs. Unlike most existing approaches that only focus on leveraging knowledge graphs for more accurate recommendation, we aim to conduct explicit reasoning with knowledge for decision making so that the recommendations are generated and supported by an interpretable causal inference procedure. To this end, we propose a method called Policy-Guided Path Reasoning (PGPR), which couples recommendation and interpretability by providing actual paths in a knowledge graph. Our contributions include four aspects. We first highlight the significance of incorporating knowledge graphs into recommendation to formally define and interpret the reasoning process. Second, we propose a reinforcement learning (RL) approach featured by an innovative soft reward strategy, user-conditional action pruning and a multi-hop scoring function. Third, we design a policy-guided graph search algorithm to efficiently and effectively sample reasoning paths for recommendation. Finally, we extensively evaluate our method on several large-scale real-world benchmark datasets, obtaining favorable results compared with state-of-the-art methods.",
         "['Yikun Xian', 'Zuohui Fu', 'S. Muthukrishnan', 'Gerard de Melo', 'Yongfeng Zhang']",
         "2019",
         "semantic_scholar",
         "10.1145/3331184.3331203",
         "480.0",
         "9a14989424b16a4685c43ffc8057b40157631dd2"
        ],
        [
         "1608",
         null,
         "Working-memory capacity protects model-based learning from stress",
         "It is found that stress response attenuates the contribution of model-based, but not model-free, contributions to behavior, suggesting that executive function may be protective against the deleterious effects of acute stress.",
         "['A. R. Otto', 'Candace M. Raio', 'Alice Y. Chiang', 'E. Phelps', 'N. Daw']",
         "2013",
         "semantic_scholar",
         "10.1073/pnas.1312011110",
         "432.0",
         "78b89e68ed84ef344f82922ea702a736a5791d0a"
        ]
       ],
       "shape": {
        "columns": 9,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>arxiv_id</th>\n",
       "      <th>title</th>\n",
       "      <th>summary</th>\n",
       "      <th>authors</th>\n",
       "      <th>year</th>\n",
       "      <th>source</th>\n",
       "      <th>doi</th>\n",
       "      <th>citation_count</th>\n",
       "      <th>paper_id_s2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Decision Transformer: Reinforcement Learning v...</td>\n",
       "      <td>We introduce a framework that abstracts Reinfo...</td>\n",
       "      <td>['Lili Chen', 'Kevin Lu', 'A. Rajeswaran', 'Ki...</td>\n",
       "      <td>2021</td>\n",
       "      <td>semantic_scholar</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1736.0</td>\n",
       "      <td>c1ad5f9b32d80f1c65d67894e5b8c2fdf0ae4500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1528</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Reinforcement learning for demand response: A ...</td>\n",
       "      <td>A need is identified to further explore reinfo...</td>\n",
       "      <td>['José R. Vázquez-Canteli', 'Z. Nagy']</td>\n",
       "      <td>2019</td>\n",
       "      <td>semantic_scholar</td>\n",
       "      <td>10.1016/J.APENERGY.2018.11.002</td>\n",
       "      <td>600.0</td>\n",
       "      <td>648ea87fe7f99ca8ea5090cb1ba40242299ef4c4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1395</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Perceptual Learning Directs Auditory Cortical ...</td>\n",
       "      <td>The primary sensory cortex is positioned at a ...</td>\n",
       "      <td>['D. Polley', 'Elizabeth E Steinberg', 'M. Mer...</td>\n",
       "      <td>2006</td>\n",
       "      <td>semantic_scholar</td>\n",
       "      <td>10.1523/JNEUROSCI.3771-05.2006</td>\n",
       "      <td>567.0</td>\n",
       "      <td>3fdfbf8de976c103ad0ee05cc9112fe8316fa342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Reinforcement Knowledge Graph Reasoning for Ex...</td>\n",
       "      <td>Recent advances in personalized recommendation...</td>\n",
       "      <td>['Yikun Xian', 'Zuohui Fu', 'S. Muthukrishnan'...</td>\n",
       "      <td>2019</td>\n",
       "      <td>semantic_scholar</td>\n",
       "      <td>10.1145/3331184.3331203</td>\n",
       "      <td>480.0</td>\n",
       "      <td>9a14989424b16a4685c43ffc8057b40157631dd2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1608</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Working-memory capacity protects model-based l...</td>\n",
       "      <td>It is found that stress response attenuates th...</td>\n",
       "      <td>['A. R. Otto', 'Candace M. Raio', 'Alice Y. Ch...</td>\n",
       "      <td>2013</td>\n",
       "      <td>semantic_scholar</td>\n",
       "      <td>10.1073/pnas.1312011110</td>\n",
       "      <td>432.0</td>\n",
       "      <td>78b89e68ed84ef344f82922ea702a736a5791d0a</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     arxiv_id                                              title  \\\n",
       "399       NaN  Decision Transformer: Reinforcement Learning v...   \n",
       "1528      NaN  Reinforcement learning for demand response: A ...   \n",
       "1395      NaN  Perceptual Learning Directs Auditory Cortical ...   \n",
       "328       NaN  Reinforcement Knowledge Graph Reasoning for Ex...   \n",
       "1608      NaN  Working-memory capacity protects model-based l...   \n",
       "\n",
       "                                                summary  \\\n",
       "399   We introduce a framework that abstracts Reinfo...   \n",
       "1528  A need is identified to further explore reinfo...   \n",
       "1395  The primary sensory cortex is positioned at a ...   \n",
       "328   Recent advances in personalized recommendation...   \n",
       "1608  It is found that stress response attenuates th...   \n",
       "\n",
       "                                                authors  year  \\\n",
       "399   ['Lili Chen', 'Kevin Lu', 'A. Rajeswaran', 'Ki...  2021   \n",
       "1528             ['José R. Vázquez-Canteli', 'Z. Nagy']  2019   \n",
       "1395  ['D. Polley', 'Elizabeth E Steinberg', 'M. Mer...  2006   \n",
       "328   ['Yikun Xian', 'Zuohui Fu', 'S. Muthukrishnan'...  2019   \n",
       "1608  ['A. R. Otto', 'Candace M. Raio', 'Alice Y. Ch...  2013   \n",
       "\n",
       "                source                             doi  citation_count  \\\n",
       "399   semantic_scholar                             NaN          1736.0   \n",
       "1528  semantic_scholar  10.1016/J.APENERGY.2018.11.002           600.0   \n",
       "1395  semantic_scholar  10.1523/JNEUROSCI.3771-05.2006           567.0   \n",
       "328   semantic_scholar         10.1145/3331184.3331203           480.0   \n",
       "1608  semantic_scholar         10.1073/pnas.1312011110           432.0   \n",
       "\n",
       "                                   paper_id_s2  \n",
       "399   c1ad5f9b32d80f1c65d67894e5b8c2fdf0ae4500  \n",
       "1528  648ea87fe7f99ca8ea5090cb1ba40242299ef4c4  \n",
       "1395  3fdfbf8de976c103ad0ee05cc9112fe8316fa342  \n",
       "328   9a14989424b16a4685c43ffc8057b40157631dd2  \n",
       "1608  78b89e68ed84ef344f82922ea702a736a5791d0a  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.info()\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7705ce7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "causal-rl-ev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
